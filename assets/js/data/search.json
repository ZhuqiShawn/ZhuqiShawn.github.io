[ { "title": "Animal Detection Networks", "url": "/posts/Animal-Detection-Networks/", "categories": "Deep Learning", "tags": "deep learning, cnn, object detection", "date": "2021-10-02 06:00:00 +0200", "snippet": " Abstract: The project aims to provide a demo animal detection model with the long-term goal of developing a phone application. Two different state-of-art algorithms are tested out, one is YOLOv5 and another is Faster R-CNN. These two represents two different families of detection methods, one is one-stage while another is two-stage. YOLO is an end-to-end target detection framework that transforms target detection into regression problems while R-CNN is a target detection framework combining region proposal and CNN classification. We compare the two algorithms in accuracy and speed and found that YOLOv5 performs better in both aspects. At the end of this report, there will be some discussion about the results and methods to further expand the model in the future.IntroductionAs one of the most potent public education organisations, Universeum is a public arena for lifelong learning where children and adults explore the world through science and technology. This project is for building an animal detection model based on deep learning methods. Doing so can help Universeum further develop related mobile applications in the future, thereby making it easier to convey knowledge and information to tourists. In this project, we collected and annotated our own dataset, which contains 19 classes of animals. The method of data augmentation is performed to generalize new images and introduce them to the networks. Then, the transfer learning method would be applied and feed the dataset to two state-of-art pre-trained networks, YOLOv5 and Faster R-CNN.Related WorkObject detection, as of one the most fundamental and challenging problems in computer vision, has received significant attention in recent years. It is an important computer vision task that detects visual objects of certain classes in digital images. In the deep learning era, object detection can be categorized into two groups1: â€œtwo-stage detectionâ€ and â€œone-stage detectionâ€, where the former frames the detection as a â€œcoarse-to-fineâ€ process while the latter frames it as to â€œcomplete in one stepâ€. In this project, two different detectors from each category are tried out, YOLOv5 and Faster R-CNN.You Only Look Once (YOLO)YOLO was primely proposed by R.Joseph et al. in 2015, which is the very first one-stage detector2. Compared to the approach taken by object detection algorithms before YOLO, YOLO proposes using an end-to-end neural network that makes predictions of bounding boxes and class probabilities all at once. YOLO provided a super fast and accurate object detection algorithm that revolutionized computer vision research related to object detection. It applies a single neural network to the full image, which divides the image into regions and predicts bounding boxes and probabilities for each region simultaneously. R. Joseph has made a series of improvements on the first version and proposed version 2 and 3 editions34 but then he left the community last year, so YOLOv4 and later versions are not his official work. However, the legacy continues through new researchers. In this project, we decided to use the latest model of the YOLO family, version 5, which is an open-source project that consists of a family of object detection models and detection methods based on the YOLO model pre-trained on the COCO dataset.How YOLO worksThe YOLO algorithm works by dividing the image into $N$ grids, each having an equal dimensional region of $S \\times S$. Each of these $N$ grids is responsible for the detection and localization of the object it contains. Correspondingly, these grids predict bounding boxes relative to their cell coordinates, along with the object label and probability of the object being present in the cell. This process greatly lowers the computation as both detection and recognition are handled by cells from the image. However, it brings forth a lot of duplicate predictions due to multiple cells predicting the same object with different bounding box predictions. YOLO makes use of Non-Maximal Suppression (NMS) to deal with this issue. In NMS, YOLO suppresses all bounding boxes that have lower probability scores. YOLO achieves this by first looking at the probability scores associated with each decision and taking the largest one. Following this, it suppresses the bounding boxes having the largest IOU with the current high probability bounding box. This step is repeated till the final bounding boxes are obtained.There are two parts to the loss function, bounding box regression loss and classification loss. In terms of b-box regression loss, IOU loss was used in the past. The latest models use deformations based on this loss, such as CIOU Loss and DIOU Loss.YOLO ArchitectureYOLO is designed to create features from input images and feed them through a prediction system to draw boxes around objects and predict their classes. Since YOLOv3, the YOLO network consists of three main pieces. Backbone: A convolutional neural network that aggregates and forms image features at different granularities. Neck: A series of layers to mix and combine image features to pass them forward to prediction. Head: Consumes features from the neck and takes box and class prediction steps.There are abundant approaches that can be used to combine different architectures at each component listed above. The contributions of YOLOv4 and YOLOv5 first integrate breakthroughs in other fields of computer vision and incorporate them into the original YOLO structure to improve performance.Faster R-CNNFaster R-CNN is a member of R-CNN family. R-CNN is published by R. Girsh in 20135. Then some improvements have been made by the author himself, which is Fast R-CNN, published in early 20156. And finally is the model published in the same year, Faster R-CNN7. Figure 1 is the whole structure of Faster R-CNN and the theory will be elaborated below.Figure 1. Structure of Faster R-CNN7Covolutional LayersAs a CNN network target detection method, Faster R-CNN uses basic Covolutional layers like VGG16 or ResNet50 to extract image feature maps. These layers is the cov layers in Figure 1, which is also called base network because the whole structure is built based on it.The obvious advantage of ResNet over VGG is that it is bigger, hence it has more capacity to actually learn what is needed8.The feature maps are shared for the subsequent RPN layer and fully connected layer. The VGG16 and ResNet50 are always pretrained to reduce the difficulty in training.Region Proposal NetworksClassical detection methods are very time-consuming to generate detection frames. For example, sliding windows, which we have learned in class, really needs to iterate so many times; or R-CNN uses SS (Selective Search) method to generate anchor box5. Faster RCNN abandons the traditional sliding window and SS method and directly uses Region Proposal Networks (RPN) to generate the anchor box. This is also a huge advantage of Faster R-CNN, which can greatly improve the generation speed of the detection frame.Before continue talking about RPN, we want to discuss the anchor box first, which is inherited from R-CNN. Traverse the feature maps calculated by Conv layers, and equip each point with these 9 prior anchors given as hyper parameters as the initial detection frame.The RPN does two different type of predictions: the binary classification and the bounding box regression adjustment. The loss of fist prediction is cross entropy loss and the second is L1 loss:\\[Loss = \\sum^{N}_{i}{|t_*^i - W_*^T\\cdot \\phi(A^i)|} + \\lambda ||W_*||.\\]Here $t_*^i$ is the target box and $W_*^T\\cdot \\phi$ is the linear map of anchor box.It should be clear that only when the distance between two boxes is short enough, can the map be viewed as linear.There are some many anchors, so we introduced Non-Maximum Suppression (NMS). NMS discards those proposals that have a score larger than some predefined threshold with a proposal that has a higher score.RoI PoolingThe Region of Interest (RoI) Pooling layer is responsible for collecting proposals, calculating proposal feature maps, and sending them to the subsequent network.For traditional CNN (such as AlexNet and VGG), when the network is trained, the input image size must be a fixed value. Faster R-CNN tries to solve this problem by reusing the existing conv network. This is done by extracting fixed-sized feature maps for each proposal using region of interest pooling. Fixed size feature maps are needed for the R-CNN in order to classify them into a fixed number of classes.ClassificationThe probability vector is obtained by fully connected layer and softmax. Bounding box regression is used to obtain each proposalâ€™s position offset in order to return to a more accurate target detection frame.MethodologyIn this section, we will see how we conduct our project in detail.DatasetAt the very soul of training deep learning networks is the training dataset. The first step of any object detection model is collecting images data and performing annotation. For this project, we went to the museum in person and manually collected 764 images of real scenes. Training datasets are images collected as samples and annotated for training deep neural networks. For object detection, there are many formats for preparing and annotating your dataset for training. The most popular formats for annotating an object detection datasets are Pascal VOC, Micosoft COCO, and YOLO format. Since we intend to use transfer learning, YOLO and Faster R-CNN require different formats of annotations, so two different annotation formats have been prepared, VOC and YOLO formats.Figure 2. Class balance of the collected and annotated datasetThere are in total 19 classes of animals appeare in our collected datset. However, we can see from the Figure 2 that our data set is unbalanced. Some animals have been labeled more than 300 while some have fewer than 50. This factor needs to be considered in the analysis of the final result. All images are then resized into 416 by 416 and the entired dataset is randomly divided into training, validation, and test set according to the ratio of 7:2:1.Image augmentation is a method to increase the generalizability of your modelâ€™s performance by increasing the diversity of learning examples for your model. Specific methods include rotation, flipping, and brightness adjustment. We performed this process on the training set and expanded it by a factor of two.Transfer LearningSince we only have a small dataset, in order to obtain a good model in time, the transfer learning method is adopted. We separately train on pre-trained YOLOv5 model and Faster RCNN model.YOLOYOLOv5 is an open-source project that consists of a family of object detection models. Since the model will be potentially deployed on mobile phones, smaller models are preferred. Therefore, the smallest YOLOv5 architecture, YOLOv5s, is then selected to be trained on. The pre-trained models are available in their github page. 100 epochs of training are preformed.Faster RCNNPretrained models of Faster R-CNN are downloaded online in this page and here. Compared to the original paper, we set smaller prior anchor box to detect small objects. We use two base models, VGG16 and ResNet50 to fulfill the task. Training epoch is 50, of which 30 is trained while base model is frozen while 20 unfrozen. Learning rate is $10^{-4}$ for frozen part and $10^{-5}$ for unfrozen with decaying.ResultsWe calculate the mAP of two algorithms and results are shown in Table 1. Both of these detectors perform well for detecting large animals, but for small animals that look very similar (especially some fish), both models perform relatively poorly but each has its own merits. Note that this result does not truly reflect the true ability of these two detectors, because the dataset is very limited (the test set has only 76 images). However, this result can also be used as a reference to test the performance of these two models on this specific task. Method YOLOv5 Faster R-CNN all 0.82 0.74 Arapaima 0.64 0.81 Orange Revabborre 0.97 0.70 Stingray 0.96 0.73 Toco Toucan 0.76 0.90 Emerald Toucanet 0.99 1.00 Pied Tamarin 0.46 0.40 Sunbittern 1.00 1.00 Goldbelly Damsel 0.78 0.31 Lemon Damsel 0.88 0.66 Red Ibis 0.88 1.00 Thornback Ray 0.64 0.95 Picked dogfish 0.91 0.71 Clownfish 0.89 0.64 Common dragonet 0.92 0.74 Starfish 0.70 0.66 Indian Surgeonfish 0.55 0.79 Yellowfin surgeonfish 1.00 1.00 Silver Moony 0.76 0.12 Blue Fish 0.96 0.81 Table 1.ResultsBesides, we measure the average fps of two algorithms when predicting videos in Table 2. The whole experiment is implemented on a GeForce RTX 2060 SUPER. Method YOLOv5 Faster R-CNN fps 200.00 8.37 Table 2. mAP of two algorithmsConclusionsIn the above table, we just show the best result of Faster R-CNN and now the comparison between VGG16 and ResNet50 is shown in Table 3. Â  Faster R-CNN VGG Faster R-CNN ResNet mAP 0.74 0.72 fps 8.37 3.93 Table 3. mAP and fps of Faster R-CNN with different base networkssThe mAP of VGG is slightly better than ResNet while fps is two times better surprisingly. We think it is because our task is relatively simple. The main difference between VGG and ResNet is that ResNet has bigger network, which means it will definitely run slower, but for a simple task, the advantage of extracting more information is not shown.According to common sense, one-stage algorithm is faster while two-stage algorithm is more accurate. There is no surprise to see YOLOv5 is greatly quicker while a bit more correct since YOLO has fixed its disadvantage in detecting small objects since YOLOv3 and nowadays YOLOv5 also draw lessons from R-CNN. But it is still strange to observe such a bad performance in small objects (Sliver Moony) with Faster R-CNN. So we check it thoroughly and find the problem, that is, the only two pictures of Silver Moony in the test set is so blurry that even a human can not recognize it easily. We tried another picture borrowed from another group and find it work fine. Since it is quite time-consuming to shuffle the train, validation as well as test set and retrain them, we just clarify the fact here.Since the long-term goal of this project is to develop an animal recognition application for Universeum, we also need to consider the scalability of the model. In theory, this should be very easy. Take YOLOv5 as an example, just change the output dimension of the output layer to the required number, and perform a few rounds of training on the expanded dataset (not verified), and you should be able to get a well performed expanded model. Of course, in order to ensure that the model performs good, the quality and data content of the dataset should be better.Reference Z. Zou, Z. Shi, Y. Guo, and J. Ye, â€œObject detection in 20 years: A survey,â€ arXiv preprint arXiv:1905.05055, 2019.Â &amp;#8617; J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, â€œYou only look once: Unified, real-time object detection,â€ in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 779â€“788.Â &amp;#8617; J. Redmon and A. Farhadi, â€œYOLO9000: better, faster, stronger,â€ in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 7263â€“7271.Â &amp;#8617; J. Redmon and A. Farhadi, â€œYolov3: An incremental improvement,â€ arXiv preprint arXiv:1804.02767, 2018.Â &amp;#8617; R. Girshick, J. Donahue, T. Darrell, and J. Malik, â€œRich feature hierarchies for accurate object detection and semantic segmentation,â€ arXiv e-prints, p. earXiv:1311.2524, Nov. 2013.Â &amp;#8617;Â &amp;#8617;2 R. Girshick, â€œFast R-CNN,â€ arXiv e-prints, p. earXiv:1504.08083, Apr. 2015.Â &amp;#8617; S. Ren, K. He, R. Girshick, and J. Sun, â€œFaster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,â€ arXiv e-prints, p. earXiv:1506.01497, Jun. 2015.Â &amp;#8617;Â &amp;#8617;2 K. He, X. Zhang, S. Ren, and J. Sun, â€œDeep Residual Learning for Image Recognition,â€ arXiv e-prints, p. earXiv:1512.03385, Dec. 2015.Â &amp;#8617; " }, { "title": "Count-min Sketch ç®—æ³•", "url": "/posts/Count-Min-Sketch-%E7%AE%97%E6%B3%95/", "categories": "Algorithm", "tags": "algorithm, big data", "date": "2021-05-19 06:00:00 +0200", "snippet": "ç®€ä»‹Count-min Sketchç®—æ³•æ˜¯ä¸€ä¸ªå¯ä»¥ç”¨æ¥è®¡æ•°çš„ç®—æ³•ï¼Œåœ¨æ•°æ®å¤§å°éå¸¸å¤§æ—¶ï¼Œä¸€ç§é«˜æ•ˆçš„è®¡æ•°ç®—æ³•ï¼Œé€šè¿‡ç‰ºç‰²å‡†ç¡®æ€§æé«˜çš„æ•ˆç‡ã€‚ æ˜¯ä¸€ä¸ªæ¦‚ç‡æ•°æ®æœºåˆ¶ ç®—æ³•æ•ˆç‡é«˜ æä¾›è®¡æ•°ä¸Šé™å…¶ä¸­ï¼Œé‡è¦å‚æ•°åŒ…æ‹¬ Hash å“ˆå¸Œå‡½æ•°æ•°é‡ï¼š k è®¡æ•°è¡¨æ ¼åˆ—çš„æ•°é‡ï¼š m å†…å­˜ä¸­ç”¨ç©ºé—´ï¼š $k \\times m \\times \\text{size of counter}$ä¸¾ä¸ªä¾‹å­ğŸŒ°æˆ‘ä»¬è§„å®šä¸€ä¸ª $m = 5$, $k = 3$ çš„Count-min Sketchï¼Œç”¨æ¥è®¡æ•°ï¼Œå…¶ä¸­æ‰€æœ‰hashå‡½æ•°å¦‚ä¸‹\\[k\\left\\{ \\begin{array}{lr} h_1(x)=\\text{ASCII}(x)\\\\ h_2(x) = 2 + \\text{ASCII}(x)\\\\ h_3(x) = 4 \\cdot \\text{ASCII}(x) \\end{array} \\right.\\]æ³¨æ„ï¼Œæ‰€æœ‰hashå‡½æ•°çš„ç»“æœéœ€ $\\mod m$ä¸‹é¢å¼€å§‹å¡«è¡¨ï¼Œé¦–å…ˆåˆå§‹çŠ¶æ€ä¸º01234$h_1$00000$h_2$00000$h_3$00000é¦–å…ˆï¼Œå‘é‡Œé¢æ·»åŠ å­—æ¯Bï¼Œå…¶ASCIIç ä¸º66ï¼Œæ±‚hashå‡½æ•°çš„ç»“æœä¸º\\[\\left\\{ \\begin{array}{lr} h_1(x) = 1\\\\ h_2(x) = 3\\\\ h_3(x) = 4 \\end{array} \\right.\\]å› æ­¤ï¼Œè¡¨æ ¼å˜ä¸º01234$h_1$01000$h_2$00010$h_3$00001æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æŸ¥è¯¢å­—æ¯Aï¼Œå…¶ASCIIç ä¸º65ï¼Œæ±‚hashå‡½æ•°çš„ç»“æœä¸º\\[\\left\\{ \\begin{array}{lr} h_1(x) = 0\\\\ h_2(x) = 2\\\\ h_3(x) = 0 \\end{array} \\right.\\]ç”¨è¿™ä¸ªç»“æœå»è¯»è¡¨ï¼Œå‘ç°å…¶å¯¹åº”ä½ç½®å‡ä¸º0ï¼Œå› æ­¤å­—æ¯Aæœ€å¤šå‡ºç°0æ¬¡ï¼Œè¿™ä¸ªå€¼æ˜¯å‡†ç¡®çš„ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨æŸ¥è¯¢å­—æ¯Gï¼Œå…¶ASCIIç ä¸º71ï¼Œæ±‚hashå‡½æ•°çš„ç»“æœä¸º\\[\\left\\{ \\begin{array}{lr} h_1(x) = 1\\\\ h_2(x) = 3\\\\ h_3(x) = 4 \\end{array} \\right.\\]ç”¨è¿™ä¸ªç»“æœå»è¯»è¡¨ï¼Œå‘ç°å…¶å¯¹åº”ä½ç½®å‡ä¸º1ï¼Œå› æ­¤å­—æ¯Gæœ€å¤šå‡ºç°1æ¬¡ï¼›å‡ºé”™äº†ï¼æˆ‘ä»¬ä»æœªå‘é‡Œé¢æ·»åŠ è¿‡å­—æ¯Gï¼Œè¿™å°±æ˜¯ä¸€æ¬¡collisionã€‚Count-min Sketchçš„ç¡®ä¼šæœ‰è¿™ç§é—®é¢˜ï¼Œå› ä¸ºè¿™ä¸ªæ¨¡å‹æ˜¯ä»Bloom Filterè¡ç”Ÿè¿‡æ¥çš„ã€‚æ‰€ä»¥è¯´Count-min Sketchæ˜¯ä¸€ä¸ªæ¦‚ç‡æ¨¡å‹ï¼Œè¿”å›çš„ç»“æœæ˜¯ä¸€ä¸ªä¸Šé™å€¼ï¼ˆupper-boundï¼‰ã€‚è®¾è®¡æœ€ä¼˜ Count-min Sketchæœ‰äº†ä¸Šé¢çš„é—®é¢˜ï¼Œæˆ‘ä»¬è‡ªç„¶è€Œç„¶å°±ä¼šæƒ³åˆ°å¦‚ä½•è®¾è®¡ä¸€ä¸ªæœ€ä¼˜çš„Count-min Sketchæ¨¡å‹ã€‚é¦–å…ˆï¼Œè§„å®šä¸€äº›å‚æ•°ï¼š æ•°æ®æµå¤§å°ï¼š $n$ å…ƒç´  x çš„çœŸå®è®¡æ•°å€¼ï¼š $c_x$ å…ƒç´  x çš„ä¼°è®¡è®¡æ•°å€¼ï¼š $\\hat{c}_x$ æˆ‘ä»¬å¯ä»¥è‡ªå·±é€‰æ‹©çš„å‚æ•°ï¼š $k$ ï¼ˆhashå‡½æ•°æ•°é‡ï¼‰å’Œ $m$ ï¼ˆè¡¨æ ¼åˆ—çš„æ•°é‡ï¼‰æ³¨ï¼šå¦‚æœæˆ‘ä»¬çš„æ¨¡å‹ $k = 1$, $m = n$ ï¼Œå¦å”¯ä¸€çš„ $h_1(x) = x$ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥å¾—åˆ°å‡†ç¡®çš„è®¡æ•°ç»“æœã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¸Œæœ›è®¾å®šä¸€ä¸ªé”™è¯¯èŒƒå›´ $(c_x\\leq\\hat{c}_x \\leq c_x + \\varepsilon n)$ï¼Œè¿™ä¸ªèŒƒå›´è¡¨ç¤ºä¼°è®¡å€¼çš„å–å€¼èŒƒå›´ï¼Œæˆ‘ä»¬å¸Œæœ›ç»“æœåœ¨è¿™ä¸ªèŒƒå›´çš„æ¦‚ç‡ä¸º\\[P\\left(c_x\\leq\\hat{c}_x \\leq c_x + \\varepsilon n\\right)\\geq 1 - \\delta\\]è¿™é‡Œï¼Œ $(1 - \\delta)$ è¡¨ç¤ºç»“æœåœ¨è¿™ä¸ªèŒƒå›´é‡Œçš„æ¦‚ç‡ã€‚é‚£ä¹ˆè®¾è®¡ä¸€ä¸ªæœ€ä¼˜Count-min Sketchæ¨¡å‹çš„è¿‡ç¨‹ä¸ºï¼š ä¼°è®¡æ•°æ®æµå¤§å° $n$ çš„å¤§å° é€‰æ‹©ä¸€ä¸ªåˆç†çš„ $\\varepsilon$ å€¼ä½¿ $\\hat{c}_x - c_x \\leq \\varepsilon n$ é€‰æ‹©ä¸€ä¸ªåˆç†çš„æ¦‚ç‡å€¼ $(1-\\delta)$ $m$ å’Œ $k$ çš„æœ€ä¼˜å€¼å¯ä»¥é€šè¿‡ä»¥ä¸‹å…¬å¼è·å¾—ï¼š\\[m = \\left\\lceil{\\dfrac{e}{\\varepsilon}}\\right\\rceil , k = \\left\\lceil{\\ln (\\dfrac{1}{\\delta})}\\right\\rceil\\] å¯ä»¥çœ‹å‡ºï¼Œæƒ³è¦é”™è¯¯èŒƒå›´è¶Šå°ï¼Œå°±è¦æ›´å¤§çš„ $m$ ï¼Œä¹Ÿå°±æ˜¯è¡¨æ ¼çš„åˆ—æ•°ï¼›åŒç†ï¼Œæƒ³è¦æ›´é«˜çš„æ¦‚ç‡ï¼ˆæ›´å°çš„ $\\delta$ ï¼‰ï¼Œå°±è¦æ›´å¤§çš„ $k$ ï¼Œä¹Ÿå°±æ˜¯æ›´å¤šçš„hashå‡½æ•°ã€‚ä¸¾ä¸ªä¾‹å­ğŸŒ°å‡è®¾æˆ‘ä»¬ç°åœ¨éœ€è¦ä¸ºå¤§å°ä¸º $10^6$ çš„æ•°æ®è®¡æ•°ï¼Œæˆ‘ä»¬é€‰æ‹© $\\varepsilon n = 2000$ ï¼Œå³ $\\varepsilon = 0.002$ã€‚ç”±æ­¤æˆ‘ä»¬å¯ä»¥å¾—å‡º$m = 1360$ï¼Œå‡å¦‚æˆ‘ä»¬å¸Œæœ› $99\\%$ çš„æ¦‚ç‡è½åœ¨è¿™ä¸ªèŒƒå›´å†…ï¼Œå¯å¾— $\\delta = 0.01$ï¼Œå› æ­¤hashå‡½æ•°æ•°é‡ $k = 5$å‡è®¾æ¯ä¸ªè®¡æ•°å•å…ƒå å†…å­˜å¤§å°ä¸º4 byteï¼Œé‚£ä¹ˆï¼Œè¯¥æ¨¡å‹å°†å ç”¨å†…å­˜\\[m \\times k \\times\\text{size of counter} = 1360 \\times 5 \\times 4 \\text{ bytes} \\approx 28\\text{ kB}\\]å‚è€ƒï¼šAdvanced Data Structures: Count-Min Sketches" }, { "title": "Generate Digit Images using VAE", "url": "/posts/Generate-Digit-Images-using-VAE/", "categories": "Deep Learning", "tags": "gnn, vae, cnn", "date": "2021-04-01 06:00:00 +0200", "snippet": " Abstract: In the last decade, deep learning based generative models have gained more and more attention due to their tremendous progress. Among these models, there are two major families of them that deserve extra attention: Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). As a generative model comparable to GANs, VAEs combine the advantages of the Bayesian method and deep learning. It is built on the basis of elegant mathematics, easy to understand and performs outstandingly. Its ability to extract disentangled latent variables also enables it to have a broader meaning than the general generative model. A VAE can be defined as being an autoencoder whose training is regularised to avoid overfitting and ensure that the latent space has good properties that enable the generative process. In this post, VAE will be used to generate digit images based on the MNIST dataset.MethodIn short, as one of state-of-art generative models, a VAE is an autoencoder whose encodings distribution is regularised during the training process to ensure that its latent space has good performance allowing it to generate new data that is not included in the training dataset. It is worth noting that the term â€œvariationalâ€ comes from the close relationship between regularization in statistics and variational inference methods. Just like a standard autoencoder, a variational autoencoder is composed of both an encoder and a decoder, as shown in Figure 11, and that is trained to minimize the reconstruction error between the decoded output and the initial data. However, in order to introduce some regularisation of the latent space, the encoding-decoding process is slightly modified by encoding the input as a distribution on the latent space instead of encoding it as a single point. In practice, the encoded distributions are chosen to be normal so that the encoder can be trained to return the mean and the variance matrix that describes these Gaussians. The reason for encoding the input as a distribution with a certain variance instead of a single point is that it can express latent spatial regularization very naturally: the distribution returned by the encoder is forced to be close to the standard normal distribution. Figure 1. Overview of VAE architectureTherefore, we can intuitively infer that the loss function of a VAE model consists of two parts: a â€œreconstruction termâ€ that tends to make the encoding-decoding scheme as good as possible and a â€œregularisation termâ€ that tends to regularise the organisation of the latent space by making the distributions returned by the encoder close to a standard normal distribution, which can be written as\\[L = ||x-\\hat{x}||^2+\\mathbb{KL}(\\mathbf{N}(\\mu_x, \\sigma_x),\\mathbf{N}(0, 1))\\]This function is also called evidence lower bound (ELBO) loss, which will be derived in Theoretical Part. As you can see, the regularisation term is expressed as the Kulback-Leibler divergence (KL) between the returned distribution and normal distribution. We can notice that the KL divergence between two Gaussian distributions has a closed form that can be directly expressed in terms of the means and the covariance matrices of the the two distributions. In this report, the mean-squared error (MSE) is applied to calculate the reconstruction loss, which is defined as\\[\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^n||x-\\hat{x}||^2\\]where $n$ is the number of training data, and\\[\\text{KL Loss} = \\sum_{i=1}^n(1+\\log(\\sigma_i^2)-\\mu_i^2-\\sigma_i^2)\\]is applied for calculating the KL divergence.Next, we will discuss the encoder and the decoder. The output of an encoder is a compressed representation which is called latent variable and the decoder takes it as input and tries to recreate the original input data. In practice, these two parts can be implemented using two neural networks. In this report, two convolutional neural networks (CNN) are used with details shown in Table 1 and 2. It is worth noting that the reason why the dimension of the fully connected layer of encoder used here is 40 is that half of it is used as mean, and the other half is used as variance so that the length of latent space is 20. Moreover, transposed convolution layers are applied in decoder to up-sample the latent variables. Layer Input Size Filter Size Stride Output Size conv1 $28^2\\times1$ $3^2\\times32$ 2 $14^2\\times32$ conv2 $14^2\\times32$ $3^2\\times64$ 2 $7^2\\times64$ fc $7^2\\times64$ - - $40$ Table 1. Encoder neural network Layer Input Size Filter Size Stride Output Size tconv1 $1^2\\times20$ $7^2\\times64$ 7 $7^2\\times64$ tconv2 $7^2\\times64$ $3^2\\times64$ 2 $14^2\\times64$ tconv3 $14^2\\times64$ $3^2\\times32$ 2 $28^2\\times32$ tconv4 $28^2\\times1$ $3^2\\times1$ 2 $28^2\\times1$ Table 2. Decoder neural networkAs we discussed before, before minimizing the loss, we need to generate sampled data, which includes the mean and the variance vectors to create the final encoding to be passed to the decoder network. However, we need to use back-propagation later to train our network later so that we cannot do sampling in a random manner. In this case, the trick of reparameterization can be adopted to substitute for random sampling. As shown in Figure 2, it is an example of our model with the length of latent variable equal to three and you can see that the the latent space\\[z_i=\\mu_i+\\exp{(\\sigma_i)}\\times e_i\\]where $e_i\\sim\\mathbf{N}(0,1)$. The general idea is that sampling from $\\mathbf{N}(\\mu_i,\\sigma^2)$ is same with sampling from $\\mu_i+\\exp{(\\sigma_i)}\\times e_i$.Figure 2. VAE with reparameterizationExperiment and EvaluationNow that we have defined everything we need, it is time to get it trained. The training parameters are shown in Table 3 and momentum is used in this report. The training progress plot is shown in Figure 3, the loss stays at around $18.5\\%$ after 50 epochs of training. Epochs Learning Rate Gradient Decay Factor 50 0.001 0.9 Table 3. VAE with reparameterizationFigure 3.Training progress plot of VAE&amp;lt;/center&amp;gt;_Now, letâ€™s evaluate our trained VAE. First of all, randomly select ten images with labels of 0-9 from the test set and pass them through the encoder, and then use the decoder to reconstruct the images using the obtained representation. Shown in Figure 4, in most cases, after the encoding-decoding process, the reconstruction result is quite good, but for some input images whose writing is not in standard style, errors will occur. In addition, by observing these reconstructed images, we can find that the images generated by VAE are usually blurry.Figure 4. Reconstruction images of each digitNow, letâ€™s generate some new images. Randomly generate a number of encodings from normal distribution with the length same with the latent space and pass them through the decoder to reconstruct the images, the results are shown in Figure 5. In addition to the phenomenon mentioned before that the generated image will get blurred, it is not difficult to see that some generated images are meaningless, such as the image in the fourth row and fifth column, since it is hard to determine whether itâ€™s a 4 or 9.Figure 5. Randomly generated samples of digitsIn fact, compared with some models I tried before, the test result of the current model is one of the most satisfactory ones. Previously, I tried to add pooling layers and use shorter hidden variables, but the results were very bad. Some examples are shown in Figure 6. Apparently, using pooling layer to down-sample the image is not a good choice here for encoder and the length of latent variable should not be too short.Figure 6. Image generated by the model in case of failureIn the experiment, the classifier network architecture is shown in Table 4 below and the softmax function is applied on the final output of the network to obtain the classification result. After training on the entire training set of MNIST, the accuracy for testing on the test set can reach 99.21%. Layer Input Size Filter Size Stride Output Size conv1 $28^2\\times1$ $3^2\\times8$ 1 $28^2\\times8$ BN $28^2\\times8$ - - $28^2\\times8$ mp1 $28^2\\times8$ $2^2$ 2 $14^2\\times8$ conv2 $14^2\\times8$ $3^2\\times16$ 1 $14^2\\times16$ BN $14^2\\times16$ - - $14^2\\times16$ mp2 $14^2\\times16$ $2^2$ 2 $7^2\\times16$ conv3 $7^2\\times16$ $3^2\\times32$ 1 $7^2\\times32$ BN $7^2\\times32$ - - $7^2\\times32$ fc $7^2\\times32$ - - $10$ Table 4. Convolutional neural network (CNN) architecture of classiï¬er. BN is an abbreviation for batch normalization layer and there is always a ReLU activation layer follows it, and mp is for max pooling.Following the instruction, using half of the MNIST training set to train the VAE model defined in last section and use it to generate new images. The process of generating new images is as follows: re-input the training set used for training VAE to the encoder and get its encoding, add some Gaussian Noise to a portion of the encoding and then pass it through the decoder to regenerate new images. The generated image can refer to Figure 5, whose result is generally consistent. Use a certain proportion of unused training data in MNIST together with the same amount of newly generated data to train the classifier defined in Table 4, and use the test set in MNIST to test the trained classifier. The accuracy of the tests is shown in Table 5. As you can see, when the amount of training data increases, the accuracy of the test also increases. This is not difficult to explain, but what is interesting is that these test results are not as good as the classifier trained with the entire MNIST training set. I think the reason may be because the MNIST dataset is so easy that adding some interference does not improve the generalization ability of the classifier. However, I think for some more difficult datasets, doing this will reduce the overfitting and improve the classification ability. Percentage (%) 20 50 100 Accuracy (%) 97.50 98.34 98.70 Table 5. The accuracy of the classiï¬er trained with different proportions of the images in the other half of the training set on the MNIST test set together with a same amount of newly generated images.Reference Joseph Rocca: Understanding Variational Autoen-coders (VAEs). LinkÂ &amp;#8617; " }, { "title": "Note for Learning Git", "url": "/posts/Note-for-learning-Git/", "categories": "Git", "tags": "git, github, version control", "date": "2020-10-01 06:00:00 +0200", "snippet": "Basics CMD Note git --version Display current git version git config --global --edit Open git global config file and then you can edit it git config --global user.name &quot;San Zhang&quot; Change global user name git config --global user.email abc@gmail.com Change global user email git status Gives info on the current status of a git repo and its contents git init To create a new git repo git add file1 file2 To stage changes to be committed git add . To stage all changes at once git commit -m &quot;some message&quot; To commit changes from staging area git commit -a -m &quot;some message&quot; or git commit -am &quot;some message&quot; Combine last two cmds git commit --amend To modify the last commit git log --oneline Check different commits and display them briefly one line for each commit Branching CMD Note git branch To view your existing branches git branch -v To view your existing branches with more details (Last commit) git branch &amp;lt;branch-name&amp;gt; To make a new branch based upon the current HEAD git branch -d &amp;lt;branch-name&amp;gt; To delete a branch git branch -D &amp;lt;branch-name&amp;gt; To force delete a branch git branch -m &amp;lt;new-name&amp;gt; To change the current branch to a new name git switch &amp;lt;branch-name&amp;gt; To switch branch git switch -c &amp;lt;branch-name&amp;gt; To create a new branch and switch to it git checkout &amp;lt;branch-name&amp;gt; To switch branch git checkout -b &amp;lt;branch-name&amp;gt; To create a new branch and switch to it git merge &amp;lt;branch-name&amp;gt; Fast-forward merge (master no change), merge the branch to the master branch git merge &amp;lt;branch-name&amp;gt; None-fast-forward merge when some is appended on the master branch after creating the branch. There will be a new commit. Whenever you encounter merge conflicts, follow these steps to resolve them: Open up the file(s) with merge conflicts Edit the file(s) to remove the conflicts. Decide which branchâ€™s content you want to keep in each conflict. Or keep the content from both. Remove the conflict â€œmarkersâ€ in the document Add your changes and then make a commitgit diffTo view changes between commits, branches, files, our working directory, etc.It is used alongside commands like git status and git log, to get a better picture of a repository and how it has changed over time. CMD Note git diff &amp;lt;file&amp;gt; List all the changes in working directory that are not staged for the next commit git diff HEAD &amp;lt;file&amp;gt; List all changes in the working directory tree since your last commit git diff --staged &amp;lt;file&amp;gt; or git diff --cached &amp;lt;file&amp;gt; List the changes between the staging area and our last commit git diff &amp;lt;branch-1&amp;gt; &amp;lt;branch-2&amp;gt;or git diff &amp;lt;branch-1&amp;gt;..&amp;lt;branch-2&amp;gt; List the chanegs between the tips of branch1 and branch2 git diff &amp;lt;commit-hash-1&amp;gt; &amp;lt;commit-hash-2&amp;gt;or git diff &amp;lt;commit-hash-1&amp;gt;..&amp;lt;commit-hash-2&amp;gt; List the changes between two different commits git stashWhen working on a new branch, you made some changes but havenâ€™t make any commits. Now you need to switch back to master/other branches, two case could happen: the changes come with me to the destination branch git wonâ€™t let me switch if it detects potential conflicts (where git stash could be used)Git pit provides an easy way of stashing these uncommitted changes so that we can return to them later, without having to maker unnecessary commits. CMD Note git stash Save changes that you are not yet ready to commit git stash pop Remove the most recently stashed changes in your stash and re-apply them to your working copy git stash apply Apply whatever is stashed away, without removing it from the stash. This can be useful if you want to apply stashed changes to multiple branches git stash list View all stashes git stash apply stash@{stash-id} (git assumes you want to apply the most recent stash when you run git stash apply) You can specify a particular stash-id to apply git stash drop stash@{stash-id} Delete a particular stash git stash clear Clear out all stahses Time Travelinggit checkoutcheckout command can be used to create branches, switch to new branches, restore files, and undo history.Note: HEAD usually refers to a branch NOT a specific commit When we checkout a particular commit, HEAD points at that commit rather than at the branch pointer (DETACHED HEAD). Then you have a couple options: Stay in detached HEAD to examine the contents of the old commit. Leave and go back to wherever you were before: reattach the HEAD Create a new branch and switch to it. You can now make and save changes, since HEAD is no longer detached. git checkout supports a slightly odd syntax for referencing previous commits relative to a particular commit. HEAD~1 refers to the commit before HEAD (parent) HEAD~2 refers to 2 commits before HEAD (grandparent) â€¦ CMD Note git checkout &amp;lt;commit-hash&amp;gt; View a previous commit git switch &amp;lt;branch-name&amp;gt; Re-attach HEAD: simply switch back to whatever branch you were on before git switch - Take you back to where you left off git checkout HEAD &amp;lt;file&amp;gt; or git checkout -- &amp;lt;file&amp;gt; Revert the file back to whatever it looked like in last commit (to the HEAD), discard the chanegs git restoregit restore is a brand new Git command that helps with undoing operations.Because it is so new, most of the existing Git tutorials and books do not mention it, but it is worth knowing!Recall that git checkout does a million different things, which many git users find very confusing. git restore was introduced alongside git switch as alternatives to some of the uses for checkout. CMD Note git restore &amp;lt;file&amp;gt; Restore the file to the contents in the HEAD git restore --source &amp;lt;commit-hash&amp;gt; &amp;lt;file&amp;gt; Restore the file to the contents in a particular commit git restore --staged &amp;lt;file&amp;gt; Remove unwanted file(s) in current staged file(s) git resetSuppose youâ€™ve just made a couple of commits on the master branch, but you actually meant to make them on a separate branch instead. To undo those commits, you can use git reset. CMD Note git reset &amp;lt;commit-hash&amp;gt; Reset the repo back to a specific commit. The commits are gone. git reset --hard &amp;lt;commit-hash&amp;gt; Undo both the commits AND the actual changes in your files git revertgit revert is similar to git reset in that they both â€œundoâ€ changes, but they accomplish it in different ways.git reset actually moves the branch pointer backwards, eliminating commits.git revert instead creates a brand new commit which reverses/undos the changes from a commit. Because it results in a new commit, you will be prompted to enter a commit message.Both git reset and git revert help us reverse changes, but there is a significant difference when it comes to collaboration: If you want to reverse some commits that other people already have on their machines, you should use revert. If you want to reverse commits that you havenâ€™t shared with others, use reset and no one will ever know!Remote Tracking Branchesmaster: A regular branch reference. I can move this around myself.origin/master: This is a â€œRemote Tracking Branchâ€. Itâ€™s a reference to the state of the master branch on the remote. I canâ€™t move this myself. Itâ€™s like a bookmark pointing to the last known commit on the master branch on origin. At the time you last communicated with this remote repository, here is where x branch was pointing.They follow this pattern &amp;lt;remote&amp;gt;/&amp;lt;branch&amp;gt;. origin/master references the state of the master branch on the remote repo named origin. upstream/logoRedesign references the state of the logoRedesign branch on the remote named upstream (a common remote name) CMD Note git branch -r View the remote branches our local repository knows about git checkout origin/master Checkout these remote branch pointers git switch &amp;lt;remote-branch-name&amp;gt;or git checkout --track origin/remote-branch-name Create a new local branch from the remote branch of the same name FetchingFetching allows us to download changes from a remote repository, BUT those changes will not be automatically integrated into our working files.It lets you see what others have been working on, without having to merge those changes into your local repo.Think of it as â€œplease go and get the latest information from Github, but donâ€™t screw up my working directory.â€ CMD Note git fetch &amp;lt;remote&amp;gt; &amp;lt;branch&amp;gt; Fetch branches and history from a specific remote repository. It only updates remote tracking branches git fetch origin Fetch all changes from the origin remote repository Pullinggit pull is another command we can use to retrieve changes from a remote repository. Unlike fetch, pull actually updates our HEAD branch with whatever changes are retrieved from the remote.â€œgo and download data from Github AND immediately update my local repo with those changesâ€ CMD Note git pull &amp;lt;remote&amp;gt; &amp;lt;branch&amp;gt; git fetch + git merge RebasingThere are two main ways to use the git rebase command: as an alternative to merging as a cleanup toolMerge vs. RebaseThe feature branch has a bunch of merge commits. If the master branch is very active, my feature branchâ€™s history is muddiedWe can instead rebase the feature branch onto the master branch. This moves the entire feature branch so that it BEGINS at the tip of the master branch. All of the work is still there, but we have re-written history.Instead of using a merge commit, rebasing rewrites history by creating new commits for each of the original feature branch commits.We can also wait until we are done with a feature and then rebase the feature branch onto the master branch.git switch featuregit rebase masterInteractive RebaseSometimes we want to rewrite, delete, rename, or even reorder commits (before sharing them). We can do this using interactive git rebase.Running git rebase with the -i option will enter the interactive mode, which allows us to edit commits, add files, drop commits, etc. Note that we need to specify how far back we want to rewrite commits. Also, notice that we are not rebasing onto another branch. Instead, we are rebasing a series of commits onto the HEAD they currently are based on.Functions: pick - use the commit reword - use the commit, but edit the commit message edit - use commit, but stop for amending fixup - use commit contents but meld it into previous commit and discard the commit message drop - remove commit CMD Note git rebase -i HEAD~n Interactive rebase, with n commits before the HEAD Interact with remote repo CMD Note git remote -v View any existing remotes for you repository git remote add &amp;lt;name&amp;gt; &amp;lt;url&amp;gt; Add a new remote. Name is typically origin git remote rename &amp;lt;old&amp;gt; &amp;lt;new&amp;gt; Rename a remote git remote remove &amp;lt;name&amp;gt; Remove a remote git push &amp;lt;remote&amp;gt; &amp;lt;local-branch&amp;gt; Make a push to a local branch up to a remote branch of the same name git push &amp;lt;remote&amp;gt; &amp;lt;local-branch&amp;gt;:&amp;lt;remote-branch&amp;gt; Make a push to a local branch up to a remote branch of the different name git push -u The -u option allows us to set the upstream of the branch weâ€™re pushingRunning git push -u origin master sets the upsteam of the local master branch so that it tracks the master branch on the origin repo, then next time we only need to do git push TagsTags are pointers that refer to particular points in Git history. We can mark a particular moment in time with a tag. Tags are most often used to mark version releases in projects (v4.1.0, v4.1.1, etc.)Think of tags as branch references that do NOT CHANGE. Once a tag is created, it always refers to the same commit. Itâ€™s just a label for a commit.There are two types of Git tags we can use: lightweight tags: just a name/label that points to a particular commit annotated tags: store extra meta data including the authorâ€™s name and email, the date, and a tagging message (like a commit message) Semantic Versioning: The semantic versioning spec outlines a standardized versioning system for software releases. It provides a consistent way for developers to give meaning to their software releases (how big of a change is this release ??) Versions consist of three numbers separated by periods. Patch releases: normally do not contain new features or significant changes. They typically signify bug fixes and other changes that do not impact how the code is used Minor releases: signify that new features or functionality have been added, but the project is still backwards compatible. No breaking changes. The new functionality is optional and should not force users to rewrite their own is code. Major releases: signify significant changes that is no longer backwards compatible. Features may be removed or changed substantially. CMD Note git tag Print a list of all the tags in the current repository git tag -l &quot;*beta*&quot; Search for tags that match a particular wildcard pattern. For example, The one on the left will print a list of tags that include â€œbetaâ€ in their name. git checkout &amp;lt;tag&amp;gt; View the state of a repo at a particular tag git diff &amp;lt;tag1&amp;gt; &amp;lt;tag2&amp;gt; Check out the difference between two tages/versions git tag &amp;lt;tagname&amp;gt; To create a lightweight tag, By default, Git will create the tag referring to the commit that HEAD is referencing. git tag -a &amp;lt;tagname&amp;gt; To create a annotated tagSimilar to git commit, we can also use the -m option to pass a message directly and forgo the opening of the text editor git show &amp;lt;tagname&amp;gt; See more information about a particular tag git tag &amp;lt;tagname&amp;gt; &amp;lt;commit&amp;gt; Tag an older commit by providing the cimmit hash git tag -f &amp;lt;tagname&amp;gt; Force to change the exisiting &amp;lt;tagname&amp;gt; to a new commit git tag -d &amp;lt;tagname&amp;gt; Delete a existing tag git push --tags Transfer all of your tags to the remote server that are not already there.By default, the git push command doesnt transfer tags to remote servers. git push &amp;lt;tagname&amp;gt; To push one tag What is in .git?Note: Thereâ€™s more, but this is the juicy stuffconfigThe config file is for configuration. Weâ€™ve seen how to configure global settings like our name and email across all Git repos, but we can also configure things on a per-repo basis.Reference link: git-configrefs Folder Inside of refs, youâ€™ll find a heads directory. refs/heads contains one file per branch in a repository. Each file is named after a branch and contains the hash of the commit at the tip of the branch. E.g. refs /heads/master contains the commit hash of the last commit on the master branch. Refs also contains a refs/tags folder which contains one file for each tag in the repo. Refs also contains a refs/remotes folder which contains different remotes have been set up.HEAD FileHEAD is just a text file that keeps track of where HEAD points. If it contains refs/heads/master, this means that HEAD is pointing to the master branch. In detached HEAD, the HEAD file contains a commit hash instead of a branch reference.objects FolderThe objects directory contains all the repo files. This is where Git stores the backups of files, the commits in a repo, and more. The files are all compressed and encrypted, so they wonâ€™t look like much! Hashing Functions Hashing functions are functions that map input data of some arbitrary size to fixed-size output values. Cryptographic Hash Function One-way function which is infeasible to invert Small change in input yields large change in the output Deterministic: same input yields same output Unlikely to find 2 outputs with same value Git is a key-value data store. We can insert any kind of content into a Git repository, and cit will hand us back a unique key we can later use to retrieve that content. These keys that we get back are SHA-1 checksums.Git uses a hashing function called SHA-1 (though this is set to change eventually).Git uses SHA-1 to hash our files, directories, and commits. SHA-1 always generates 40-digit hexadecimal numbers. The commit hashes weâ€™ve seen a million times are the output of SHA-1Four types of Git objects: Git blobs (binary large object) are the object type Git uses to store the contents of files in a given repository. Blobs donâ€™t even include the filenames of each file or any other data. They just store the contents of a file! CMD Note git hash-object &amp;lt;file&amp;gt; Takes some data, stores in in our .git/objects directory and gives us back the unique SHA-1 hash that refers to that data object.In the simplest form (show on the left), Git simply takes some content and returns the unique key that WOULD be used to store our object. But it does not actually store anything. echo &quot;hello&quot; \\| git hash-object --stdin The --stdin option tells git hash-object to use the content from stdin rather than a file. In the example, it will hash the word â€œhelloâ€ echo &quot;hello&quot; \\| git hash-object --stdin -w Rather than simply outputting the key that git would store our object under, we can use the -w option to tell git to actually write the object to the database. After running this command, check out the contents of .git/objects git cat-file -p &amp;lt;object-hash&amp;gt; Retrieve the stored data in .git/objects database. The -p option tells Git to pretty print the contents of the object based on its type git cat-file -t &amp;lt;object-hash&amp;gt; Tell the type of this hash object Trees are Git objects used to store the contents of al directory. Each tree contains pointers that can refer to blobs and to other trees.Each entry in a tree contains the SHA-1 hash of a blob or tree, as well as the mode, type, and filename. CMD Note git cat-file -p master^{tree} Print out the tree object that is pointed to by the tip of our master branch Commit objects combine a tree object along with information about the context that led to the current tree. Commits store a reference to parent commit(s), the author, the commiter and of course the commit message! Annotated tag " }, { "title": "åŸºäºæ·±åº¦å­¦ä¹ çš„æ¸¸å®¢äººè„¸è¡¨æƒ…çš„è¯†åˆ«", "url": "/posts/facial-expression-detection-using-deep-learning/", "categories": "Deep Learning", "tags": "deep learning, face detection, emotion detection, cnn", "date": "2020-06-30 06:00:00 +0200", "snippet": " æ‘˜è¦ï¼šé¢å¯¹é¢çš„è¿›è¡Œæ²Ÿé€šäº¤æµï¼Œåœ¨æˆ‘ä»¬æ—¥å¸¸äººä¸äººä¹‹é—´çš„äº¤å¾€ä¸­æ‰®æ¼”ç€å¾ˆé‡è¦çš„è§’è‰²ï¼Œä»äº¤è°ˆè€…çš„é¢éƒ¨æˆ‘ä»¬å¯ä»¥è·å–è®¸å¤šæˆ‘ä»¬åœ¨æ–‡å­—äº¤æµä¸­éš¾ä»¥è·å–çš„ä¿¡æ¯ï¼Œä¾‹å¦‚ä¸€ä¸ªäººçš„å¿ƒç†çŠ¶æ€ã€æƒ…ç»ªã€æ„å‘ç­‰ã€‚çš„ç¡®ï¼Œä¸€ä¸ªäººçš„é¢éƒ¨è•´å«ç€è®¸å¤šæ½œåœ¨æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼Œå¯¹å…¶åŠ ä»¥æœ‰æ•ˆåœ°æ”¶é›†å’Œåˆ†æï¼Œå°±å¯ä»¥åˆ›é€ å‡ºä¸€äº›æœ‰åˆ©ç”¨ä»·å€¼çš„æ•°æ®ã€‚è¿‘äº›å¹´ï¼Œå¯¹äºé¢éƒ¨è¡¨æƒ…è¯†åˆ«çš„ç ”ç©¶ï¼Œè¢«å¹¿æ³›åœ°åº”ç”¨äºæ•™è‚²ã€å¿ƒç†ã€åŒ»å­¦ä»¥åŠå•†ä¸šç­‰å¤šä¸ªé¢†åŸŸï¼Œäººè„¸è¡¨æƒ…è¯†åˆ«ä¹Ÿæ˜¯å½“å‰è®¡ç®—æœºè§†è§‰ã€æ¨¡å¼è¯†åˆ«ç­‰å¤šä¸ªäººå·¥æ™ºèƒ½é¢†åŸŸçš„çƒ­é—¨ç ”ç©¶è¯¾é¢˜ï¼Œå®ƒæ˜¯å®ç°æ™ºèƒ½äººæœºäº¤äº’ä¸­é‡è¦çš„ä¸€ä¸ªç¯èŠ‚ã€‚ æ·±åº¦å­¦ä¹ ï¼Œæ˜¯ä¸€ä¸ªè¿‘åå¹´æ¥é£é€Ÿå‘å±•çš„é¢†åŸŸï¼Œå®ƒçš„å‡ºç°æ”¹è¿›äº†è®¸å¤šç°æœ‰çš„æœºå™¨å­¦ä¹ ç®—æ³•ã€‚åœ¨ç‰¹å¾æå–å’Œæ•°å­¦å»ºæ¨¡ä¸Šï¼Œæ·±åº¦å­¦ä¹ éƒ½æœ‰ç€æ˜æ˜¾çš„ä¼˜åŠ¿ï¼Œç”±äºæ‹¥æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå›°æ‰°è¿‡å»äººå·¥æ™ºèƒ½é¢†åŸŸå‘å±•çš„ä¸€äº›éš¾ä»¥è§£å†³çš„é—®é¢˜è¢«è¾ƒå¥½çš„å…‹æœäº†ï¼Œå¹¶ä¸”éšç€è¿‘äº›å¹´GPUæ•°æ®å¤„ç†èƒ½åŠ›çš„é£é€Ÿæå‡ï¼Œæ·±åº¦å­¦ä¹ ä¹Ÿè¢«å¹¿æ³›çš„åº”ç”¨äºç›®æ ‡æ£€æµ‹ã€è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰å¤šä¸ªé¢†åŸŸå¹¶å–å¾—äº†ä¸é”™çš„æˆæ•ˆã€‚ æœ¬è¯¾é¢˜ä¸»è¦é’ˆå¯¹æ—…æ¸¸æ™¯åŒºè¿™ä¸€ç‰¹å®šåº”ç”¨åœºæ™¯ï¼Œå®ç°åˆ©ç”¨å¤šä»»åŠ¡å·ç§¯ç¥ç»ç½‘ç»œï¼ˆMTCNNï¼‰æ¨¡å‹å®ç°å¯¹æ¸¸å®¢äººè„¸çš„å®æ—¶æ£€æµ‹ï¼Œå†åˆ©ç”¨ä¸€ä¸ªç®€åŒ–çš„AlexNetå·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹å®ç°å¯¹æ‰€æ£€æµ‹åˆ°çš„äººè„¸è¿›è¡Œè¡¨æƒ…åˆ†ç±»ã€‚ç»è¿‡å¤šæ¬¡çš„æ¨¡å‹è®­ç»ƒå’Œæ”¹è‰¯ï¼Œæœ€ç»ˆåœ¨CPUç¯å¢ƒä¸‹å¯ä»¥å®ç°åˆ°ä¸€ä¸ªä¸é”™çš„å®æ—¶æ£€æµ‹æ•ˆæœã€‚ Abstract: Face-to-face communication plays an important role in our daily interpersonal interactions. From the face of the interlocutor, we can obtain a lot of information that we cannot obtain in text communication, such as a personâ€™s mental state, emotions, and intentions, etc. Indeed, faces contain a lot of potentially valuable information. An effective collection and analysis of them can create some valuable data. In recent years, research on facial expression recognition has been widely used in education, psychology, medicine, and business. Facial expression recognition is also popular research in the community of artificial intelligence such as computer vision and pattern recognition. It is a critical component in the realization of intelligent human-computer interaction. Deep learning is a rapid development field over the past decade, and its emergence has improved many existing machine learning algorithms. In feature extraction and mathematical modelling, deep learning has obvious advantages. Due to its good generalization ability, some intractable problems that have troubled the development of artificial intelligence in the past have been better overcome. In recent years, with the rapid development of GPU data processing capabilities, deep learning has also been widely used in target detection, computer vision, natural language processing and other fields and has achieved good results. This subject is mainly aimed at the specific application scenario of tourist scenic spots, using the multi-task convolutional neural network (MTCNN) model to realize the real-time detection of touristâ€™s faces, and then a simplified AlexNet convolutional neural network model is used to realize the classification of the detected faces. After many times of model training and improvement, a good real-time detection result can be achieved in the CPU environment.1. ç»ªè®ºæœ¬ç« èŠ‚é‡ç‚¹ä»‹ç»åŸºäºæ·±åº¦å­¦ä¹ çš„æ™¯åŒºæ¸¸å®¢ç¬‘è„¸è¯†åˆ«ä¸å®ç°è¿™ä¸€è¯¾é¢˜çš„ç ”ç©¶èƒŒæ™¯å’Œæ„ä¹‰ï¼Œå›½å†…å¤–ç›®å‰é’ˆå¯¹äººè„¸æ£€æµ‹ã€è¡¨æƒ…è¯†åˆ«ç­‰ç ”ç©¶æ–¹å‘çš„ç ”ç©¶æˆæœä»¥åŠé‡åˆ°çš„ä¸€äº›éš¾é¢˜ã€‚æœ¬ç« èŠ‚è¿˜å°†å¯¹å®ç°æœ¬è¯¾é¢˜çš„å…·ä½“æ­¥éª¤åˆæ­¥è¿›è¡Œç®€è¦æ¦‚è¿°ï¼Œå¹¶å¯¹æœ¬æ–‡çš„ç»“æ„æ¡†æ¶ä½œå‡ºé˜è¿°ã€‚1.1 è¯¾é¢˜çš„èƒŒæ™¯å’Œæ„ä¹‰å¿ƒç†å­¦ç ”ç©¶è¡¨æ˜ï¼Œäººè„¸è¡¨æƒ…æ˜¯äººç±»è¡¨è¾¾æƒ…æ„Ÿçš„é‡è¦è½½ä½“ä¹‹ä¸€ï¼Œå…¶ä¸­è•´æ¶µç€ä¸°å¯Œçš„äººä½“è¡Œä¸ºä¿¡æ¯ï¼Œé€šè¿‡ä¸€ä¸ªäººçš„è¡¨æƒ…ï¼Œå°±å¯ä»¥é¢„æµ‹æ­¤æ—¶æ­¤åˆ»è¿™ä¸ªäººå†…å¿ƒçš„æƒ…æ„Ÿã€åŠ¨æœºã€ä¸ªæ€§ç­‰ä¼—å¤šä¿¡æ¯ã€‚ç”±æ­¤å¯è§ï¼Œå¯¹è¡¨æƒ…çš„ç†è§£æ˜¯æ™ºèƒ½çš„ä½“ç°ï¼Œå¦‚æœè®¡ç®—æœºå¯ä»¥æ™ºèƒ½åœ°å¯¹è¿™äº›äººè„¸è¡¨æƒ…ä¿¡æ¯è¿›è¡Œæ”¶é›†å¹¶åŠ ä»¥åˆ†è¾¨ï¼Œå°†ä»æ ¹æœ¬ä¸Šæ”¹å˜äººæœºäº¤äº’çš„æ–¹å¼ï¼Œä½¿è®¡ç®—æœºæ›´å¥½çš„æœåŠ¡äºäººç±»ã€‚éšç€è¿‘äº›å¹´æ•°æ®é‡çš„çˆ†ç‚¸å¼å¢é•¿ï¼Œå¤§æ•°æ®æŠ€æœ¯ä¹Ÿåœ¨è¿…çŒ›åœ°å‘å±•ï¼Œåœ¨å•†ä¸šã€ç»æµä»¥åŠå…¶ä»–é¢†åŸŸä¸­ï¼Œå†³ç­–å°†æ—¥ç›Šè¶‹å‘äºç”±æ•°æ®å’Œåˆ†æè€Œä½œå‡ºï¼Œè€Œå¹¶éåŸºäºä¼ ç»Ÿä¹ æƒ¯ä¸Šçš„ç»éªŒå’Œç›´è§‰ã€‚å¤§æ•°æ®èƒ½å¤Ÿä¸ºä¼ä¸šæ”¹è¿›è¿è¥æ¨¡å¼æä¾›å¼ºæœ‰åŠ›å‚è€ƒï¼Œä¹Ÿå¯ä¸ºç”¨æˆ·å¸¦æ¥æ›´ä¸ºä¸°å¯Œçš„æœåŠ¡ä½“éªŒï¼Œè€Œå¦‚ä»Šåœ¨é«˜æ•ˆçš„ç®—æ³•å’Œæˆç†Ÿè®¡ç®—èƒ½åŠ›çš„ç¡¬ä»¶çš„åŠ æŒä¸‹ï¼Œæ·±åº¦å­¦ä¹ è¢«åº”ç”¨åœ¨äº†æˆ‘ä»¬ç”Ÿæ´»çš„æ–¹æ–¹é¢é¢ã€‚è€Œäººè„¸ä½œä¸ºä¸€ä¸ªå¯ä»¥ä¼ é€’æ€æƒ³æ„Ÿæƒ…çš„ä¸€ç§é‡è¦æ–¹å¼ï¼Œè¿‘äº›å¹´å¯¹äººè„¸çš„å„ç§ç ”ç©¶ä¹Ÿæ˜¯ä¸æ–­åœ°è¢«å¼€å±•å¹¶ä¸”ç ”ç©¶æˆæœè¢«å¹¿æ³›çš„åº”ç”¨äºå„ç§é¢†åŸŸã€‚ç„¶è€Œç›®å‰åœ¨å›½å†…ï¼Œæœºå™¨å­¦ä¹ æŠ€æœ¯åœ¨æ—…æ¸¸ä¸šçš„åº”ç”¨ä»ç„¶å¤„äºå‘å±•çš„åˆçº§é˜¶æ®µï¼Œå›½å†…å¤§éƒ¨åˆ†æ™ºæ…§æ—…æ¸¸åŠæ—…æ¸¸å¤§æ•°æ®ä¼ä¸šä»ç„¶ä»¥æä¾›ä¿¡æ¯åŒ–åŸºç¡€å»ºè®¾åŠæ—…æ¸¸å¤§æ•°æ®çš„å¯è§†åŒ–ä¸ºä¸»ï¼Œç¼ºä¹çœŸæ­£çš„æœºå™¨å­¦ä¹ ä»¥åŠäººå·¥æ™ºèƒ½çš„æŠ€æœ¯å®ç°ã€‚ç›®å‰ï¼Œå›½å†…ä»…æœ‰å°‘æ•°ä¸ºæ—…æ¸¸è€…å’Œæ—…æ¸¸ç®¡ç†éƒ¨é—¨æä¾›æ™ºèƒ½åŒ–æœåŠ¡çš„ä¼ä¸šï¼Œè¿™äº›ä¼ä¸šç‡å…ˆåº”ç”¨äº†æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œä»è€Œè¿›ä¸€æ­¥æå‡äº†è¡Œä¸šç®¡ç†æ°´å¹³ï¼Œä¼˜åŒ–æ¸¸å®¢æ—…æ¸¸æœåŠ¡ä½“éªŒã€‚åœ¨æ—…æ¸¸ç®¡ç†æ–¹é¢ï¼Œå¯¹äºæœºå™¨å­¦ä¹ æŠ€æœ¯çš„åº”ç”¨å¯ä»¥ä¿ƒè¿›æ—…æ¸¸äº§ä¸šç›‘ç®¡æ£€æµ‹ä»¥åŠå¼ºåŒ–æ™¯åŒºå†…éƒ¨ç®¡ç†åˆ¶åº¦ï¼Œä»è€Œæå‡æ—…æ¸¸ä¸šç®¡ç†äººå‘˜çš„å†³ç­–èƒ½åŠ›ä»¥åŠç®¡ç†æ•ˆç‡ã€‚ç»è¿‡å¤šå¹´ç ”ç©¶çš„å‘å±•ï¼Œå›´ç»•äººè„¸è¡¨æƒ…è¯†åˆ«çš„ç ”ç©¶å·²ç»æ•°ä¸èƒœæ•°ï¼Œæ¥è‡ªå…¨ä¸–ç•Œä¸åŒåœ°æ–¹çš„ç ”ç©¶è€…ä»¬ä¹Ÿå…ˆåæå‡ºäº†å¾ˆå¤šä¼˜ç§€çš„æ–¹æ³•ã€‚è¿‘äº›å¹´ï¼Œç”±äºæ·±åº¦å­¦ä¹ å…·æœ‰æ— ç›‘ç£ç‰¹å¾å­¦ä¹ èƒ½åŠ›çªå‡ºçš„ä¼˜ç‚¹ï¼Œæ‰€ä»¥å¯¹ä¹‹çš„åº”ç”¨ä¹Ÿæ˜¯è¶Šæ¥è¶Šå¤šã€‚æœ¬è¯¾é¢˜å°±é’ˆå¯¹æ—…æ¸¸ä¸šè¿™ä¸€ç‰¹å®šè¡Œä¸šï¼Œåˆ©ç”¨ä¸€äº›ç°æœ‰çš„æŠ€æœ¯æ‰‹æ®µï¼Œå®ç°å¯¹æ—…æ¸¸æ™¯åŒºæ¸¸å®¢äººè„¸çš„æ£€æµ‹ä»¥åŠè¡¨æƒ…è¯†åˆ«ï¼Œä»è€Œå¯ä»¥ååŠ©æ™¯åŒºç®¡ç†è€…æ›´åŠæ—¶çš„è°ƒæ•´è¿è¥æ¨¡å¼ï¼Œä¸ºæ¸¸å®¢æä¾›æ›´å¥½çš„æœåŠ¡å’Œå‡ºæ¸¸ä½“éªŒã€‚1.2 å›½å†…å¤–ç›®å‰å¯¹äºäººè„¸æ£€æµ‹ã€äººè„¸è¡¨æƒ…çš„ç ”ç©¶ç°çŠ¶ä»19ä¸–çºªèµ·ï¼Œäººç±»å°±å¼€å§‹äº†å¯¹äºäººè„¸è¡¨æƒ…çš„ç ”ç©¶ï¼Œä½†æ˜¯æåŠäººè„¸è¡¨æƒ…ï¼Œå°±ä¸å¯é¿å…çš„ä¼šæ¶‰åŠåˆ°äººè„¸æ£€æµ‹çš„é—®é¢˜ï¼Œå› ä¸ºå¯¹äºè®¡ç®—æœºè€Œè¨€ï¼Œéœ€è¦å…ˆä»å¤§ç»´åº¦çš„å›¾åƒæ•°æ®ä¸­å®Œæˆå¯¹äººè„¸ä½ç½®çš„å®šä½å’Œè£åˆ‡ï¼Œæ‰èƒ½è¿›ä¸€æ­¥å®ç°å¯¹äººè„¸è¡¨æƒ…çš„åˆ†ç±»ã€‚æ‰€ä»¥ï¼Œæœ¬èŠ‚å°†é’ˆå¯¹äººè„¸æ£€æµ‹ã€äººè„¸è¡¨æƒ…ç°å¦‚ä»Šå›½å†…å¤–çš„ä¸»è¦ç ”ç©¶ç°çŠ¶ä½œå‡ºæ¦‚æ‹¬å¹¶æ€»ç»“ç ”ç©¶ä¸­é‡åˆ°çš„ä¸€äº›ä¸»è¦é—®é¢˜ã€‚1.2.1 äººè„¸æ£€æµ‹äººè„¸æ£€æµ‹æ˜¯è®¡ç®—æœºè§†è§‰ä¸­ä¸€ä¸ªè¢«å¹¿æ³›ç ”ç©¶çš„è¯¾é¢˜ï¼Œå¦‚ä»Šå¤§å¤šæ•°äººè„¸æ£€æµ‹å™¨éƒ½å¯ä»¥è½»æ¾çš„å®ç°æ­£é¢äººè„¸çš„æ£€æµ‹ä»»åŠ¡ï¼Œå¯¹äºè¯¥é¢†åŸŸçš„æœ€æ–°ç ”ç©¶æ›´å¤šåœ°é›†ä¸­åœ¨ä¸å—æ§åˆ¶çš„é¢éƒ¨æ£€æµ‹é—®é¢˜ä¸Š1ï¼Œä¾‹å¦‚å§¿åŠ¿å˜åŒ–ï¼Œå¤¸å¼ çš„è¡¨æƒ…å’Œæç«¯ç…§æ˜ç­‰è®¸å¤šå¯èƒ½å¯¼è‡´é¢éƒ¨å¤–è§‚å‡ºç°è¾ƒå¤§çš„è§†è§‰å˜åŒ–çš„å› ç´ ï¼Œè¿™äº›å› ç´ å¯èƒ½ä¼šä¸¥é‡åœ°é™ä½é¢éƒ¨æ£€æµ‹å™¨çš„è€ç”¨æ€§ã€‚äººè„¸æ£€æµ‹ä»»åŠ¡çš„å›°éš¾ä¸»è¦æºè‡ªä¸¤ä¸ªæ–¹é¢ï¼š1ï¼‰åœ¨æ‚ä¹±çš„èƒŒæ™¯ä¸­ï¼Œäººè„¸çš„è§†è§‰å˜åŒ–ä¼šå¾ˆå¤§ï¼› 2ï¼‰åœ¨å¤§èŒƒå›´å†…æœç´¢å„ç§å„æ ·çš„äººè„¸å°ºå¯¸å’Œå§¿åŠ¿ã€‚å‰ä¸€ç§è¦æ±‚äººè„¸æ£€æµ‹å™¨èƒ½å‡†ç¡®åœ°è§£å†³äºŒåˆ†ç±»é—®é¢˜ï¼Œè€Œåä¸€ç§åˆ™è¿›ä¸€æ­¥è¦æ±‚æ£€æµ‹å™¨éœ€è¦æé«˜æ£€æµ‹æ•ˆç‡ã€‚æœ€å¼€å§‹Viola-Jonesæ˜¯ä½¿ç”¨Haarç‰¹å¾æ¥å®ç°é¢éƒ¨æ£€æµ‹å™¨çš„2ï¼Œä½¿ç”¨è¿™ä¸ªç‰¹å¾å¯ä»¥å¯¹ç›¸å¯¹æ­£å‘çš„é¢éƒ¨å®Œæˆè¿…é€Ÿè¯„ä¼°å¹¶è¯†åˆ«ã€‚ç„¶è€Œï¼Œç”±äºHaarç‰¹å¾çš„ç®€å•ç‰¹è´¨ï¼Œåœ¨ä¸å—æ§çš„ç¯å¢ƒä¸­ä½¿ç”¨å®ƒè¿›è¡Œäººè„¸æ£€æµ‹ç›¸å¯¹è¾ƒå¼±ï¼Œä¾‹å¦‚å„ç§å„æ ·çš„äººè„¸çš„å§¿åŠ¿æˆ–ä¸ç†æƒ³çš„å…‰ç…§æ¡ä»¶ã€‚åœ¨è¿‡å»çš„åå¹´ä¸­ï¼Œç ”ç©¶è€…ä»¬æå‡ºäº†å¾ˆå¤šåŸºäºViola-Jonesçš„äººè„¸æ£€æµ‹å™¨çš„æ”¹è¿›3ï¼Œè¿™äº›æ”¹è¿›å¤§éƒ¨åˆ†æ˜¯åŸºäºæ›´å…ˆè¿›çš„ç‰¹å¾çš„çº§è”å¼æ¡†æ¶ï¼Œå¾—åŠ›äºæ›´ä¼˜ç§€çš„ç‰¹å¾ï¼Œè¿™äº›æ”¹è¿›å¯ä»¥å®ç°å‡†ç¡®åº¦æ›´é«˜çš„äºŒåˆ†ç±»ä»»åŠ¡ï¼Œè€Œä¸”æ€»ä½“ä¸Šå¯¹äºè®¡ç®—é‡çš„éœ€æ±‚å¹¶æ²¡æœ‰æé«˜ã€‚ä½†è¿‘äº›å¹´éšç€ç¡¬ä»¶æ¡ä»¶çš„ä¸æ–­è¿›æ­¥ï¼Œå·ç§¯ç¥ç»ç½‘ç»œåœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­å–å¾—äº†è´¨çš„é£è·ƒï¼Œä¹Ÿå¾ˆå¿«è¢«åº”ç”¨äºäººè„¸æ£€æµ‹çš„ä»»åŠ¡ä¸­ï¼Œå…¶æ€§èƒ½å’Œç²¾åº¦éƒ½è¶…è¿‡äº†ä¹‹å‰åŸºäºç‰¹å¾çš„æ£€æµ‹æ–¹æ³•ã€‚åœ¨ç¬¬äºŒç« ä¸­ï¼Œå°†å¯¹ä¸€äº›åŸºäºæ·±åº¦å­¦ä¹ æ–¹æ³•çš„äººè„¸æ£€æµ‹ç®—æ³•è¿›è¡Œé˜è¿°å’Œæ€»ç»“ã€‚1.2.2 äººè„¸è¡¨æƒ…äººé€šè¿‡å˜åŒ–è‡ªèº«çš„è„¸éƒ¨è‚Œè‚‰ä»¥åŠçœ¼å’Œå£éƒ¨çš„è‚Œè‚‰ï¼Œä»è€Œå½¢æˆå„ç§å„æ ·çš„è¡¨æƒ…ï¼Œè¿™äº›è¡¨æƒ…å¯ä»¥è¢«ç”¨æ¥ä¼ è¾¾ä¸åŒçš„æƒ…ç»ªçŠ¶æ€ï¼Œä¸å£°éŸ³ã€è¯­è¨€å’Œèº«ä½“è¯­è¨€ç­‰ç»„æˆäº†äººçš„äº¤æµç³»ç»Ÿä¸­æ²Ÿé€šç³»ç»Ÿã€‚äººè„¸é¢éƒ¨è¡¨æƒ…è¯†åˆ«æ˜¯ä¸€ä¸ªæ¨ªè·¨ä¼—å¤šç§‘ç ”é¢†åŸŸçš„è¯¾é¢˜ï¼Œä¾‹å¦‚ç¥ç»å­¦ã€äººå·¥æ™ºèƒ½é¢†åŸŸç­‰ï¼Œå½“ç„¶å¯¹äºè¯¥é¡¹è¯¾é¢˜çš„ç ”ç©¶æˆæœä¹Ÿè¢«åº”ç”¨äºå¹¿æ³›çš„é¢†åŸŸï¼Œä¾‹å¦‚å¯¹ç—…äººè¿›è¡Œå¿ƒç†æµ‹è¯„ã€æ½œåœ¨ç”¨æˆ·è´­ä¹°è¡Œä¸ºçš„é¢„æµ‹ç­‰4ã€‚å¯¹äºäººè„¸è¡¨æƒ…çš„ç ”ç©¶çš„å…ˆé”‹è€…æ˜¯Darwin5ï¼Œä¹‹åEkmanå’ŒFriesenæå‡ºäº†é¢éƒ¨è¿åŠ¨ç¼–ç ç³»ç»Ÿï¼ˆFACSï¼‰6ï¼ŒFACSæ˜¯ä¸€ç§åŸºäºäººç±»è§‚å¯Ÿè€…çš„ç³»ç»Ÿï¼Œæ—¨åœ¨ä¿ƒè¿›å®¢è§‚æµ‹é‡ç”±é¢éƒ¨è‚Œè‚‰æ”¶ç¼©å¼•èµ·çš„é¢éƒ¨å¤–è§‚çš„ç»†å¾®å˜åŒ–ã€‚é€šè¿‡å¯¹44ä¸ªé¢éƒ¨åŠ¨ä½œå•å…ƒçš„ç›‘æ§ï¼ŒFACSå¯ä»¥å¯¹æ‰€æœ‰æ˜æ˜¾å¯è¾¨åˆ«çš„è¡¨è¾¾æ–¹å¼è¿›è¡Œè¯­è¨€æè¿°ã€‚è¿™ä¸ªç³»ç»Ÿè¿˜å®šä¹‰äº†å…­ç§æ™®éå…¬è®¤çš„åŸºæœ¬çš„äººè„¸è¡¨æƒ…ï¼Œåˆ†åˆ«æ˜¯ï¼šå¼€å¿ƒã€ä¼¤å¿ƒã€æƒŠå–œã€å®³æ€•ã€ç”Ÿæ°”å’ŒåŒæ¶ã€‚å°½ç®¡è¿™å…­ç§è¡¨æƒ…æ˜¯å¦æ˜¯é€šç”¨çš„äººè„¸è¡¨æƒ…å—åˆ°äº†å¾ˆå¤šäººçš„è´¨ç–‘ï¼Œä½†æ˜¯ä¹‹åå¤§å¤šæ•°åŸºäºè§†è§‰çš„é¢éƒ¨ç ”ç©¶éƒ½æ˜¯ä¾èµ–äºEkmançš„å®šä¹‰çš„è¡¨æƒ…åˆ†ç±»å¼€å±•çš„ï¼Œæ‰€ä»¥è¯´FACSç³»ç»Ÿçš„æå‡ºæ˜¯å…·æœ‰é‡Œç¨‹ç¢‘æ„ä¹‰çš„ã€‚ä½¿ç”¨è®¡ç®—æœºå¯¹äººè„¸è¡¨æƒ…ä¿¡æ¯è¿›è¡Œç‰¹å¾æå–ï¼Œå¹¶ä¸”æŒ‰ç…§äººç±»çš„æ€ç»´æ–¹å¼å¯¹å…¶åŠ ä»¥ç†è§£å’Œå½’ç±»ï¼Œå†ä½¿ç”¨äººç±»å¯¹è¡¨æƒ…å·²æœ‰çš„è®¤çŸ¥ä½¿è®¡ç®—æœºå¯¹æ£€æµ‹åˆ°äººè„¸è¡¨æƒ…åŠ ä»¥è”æƒ³ï¼Œå°±æ˜¯æˆ‘ä»¬æ‰€è¯´çš„äººè„¸è¡¨æƒ…è‡ªåŠ¨è¯†åˆ«ç³»ç»Ÿã€‚äººè„¸è¡¨æƒ…è‡ªåŠ¨è¯†åˆ«ç³»ç»Ÿå¯ä»¥è¢«åº”ç”¨äºäººæœºäº¤äº’ã€å‹åŠ›ç›‘æµ‹ã€äººç±»è¡Œä¸ºåˆ†æç­‰ä¼—å¤šé¢†åŸŸï¼Œæ‰€ä»¥å¯¹äºå¼€å‘ä¸€å¥—å®Œæ•´çš„äººè„¸è¡¨æƒ…è¯†åˆ«ç³»ç»Ÿçš„ç ”ç©¶å¸å¼•äº†æ¥è‡ªä¸åŒé¢†åŸŸçš„ç ”ç©¶è€…ã€‚æ€»ä½“æ¥è¯´ï¼Œå¯¹äºäººè„¸è¡¨æƒ…è¯†åˆ«çš„æ–¹æ³•å¯ä»¥è¢«ç²—ç•¥çš„åˆ†ä¸ºä¸¤ç§ï¼šåŸºäºå‡ ä½•ç‰¹å¾çš„æ–¹æ³•å’ŒåŸºäºå¤–è§‚çš„æ–¹æ³•7ã€‚åŸºäºå‡ ä½•ç‰¹å¾çš„æ–¹æ³•ä¾èµ–äºäººè„¸ç‰¹å¾çš„å‡ ä½•åˆ†å¸ƒï¼Œä¾‹å¦‚çœ‰æ¯›ã€çœ¼ç›ã€çœ¼è§’ã€é¼»å­ã€å˜´å·´ç­‰è„¸éƒ¨å…ƒç´ çš„ä½ç½®å’Œå½¢çŠ¶ï¼Œä½†æ˜¯è¯•éªŒç»“æœè¡¨æ˜ï¼Œç”±äºå›¾åƒçš„è´¨é‡ã€ç…§æ˜çš„æƒ…å†µç­‰ä¸€äº›ä¸å¯æ§çš„å› ç´ ï¼Œç”¨åŸºäºäººè„¸å‡ ä½•ç‰¹å¾åˆ†å¸ƒçš„æ–¹æ³•é¢„æµ‹äººè„¸è¡¨æƒ…å¹¶ä¸æ˜¯å¾ˆå¯é ï¼›è‡³äºç¬¬äºŒç§æ–¹æ³•ï¼ŒåŸºäºå¤–è§‚çš„æ–¹æ³•ï¼Œæ˜¯é‡‡ç”¨å…‰æµæ³•æˆ–æŸäº›ç‰¹å®šçš„æ»¤æ³¢å™¨å¯¹æ•´ä¸ªäººè„¸æˆ–äººè„¸çš„å±€éƒ¨è¿›è¡Œåˆ†æã€‚äººè„¸è¡¨æƒ…è¯†åˆ«å¤§è‡´å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼šå›¾åƒçš„é‡‡é›†ï¼Œå›¾åƒæ•°æ®çš„é¢„å¤„ç†ï¼Œäººè„¸æ£€æµ‹ï¼Œäººè„¸ç‰¹å¾æå–ï¼Œæœ€ç»ˆè·å¾—è¡¨æƒ…æ‰€å±çš„åˆ†ç±»ã€‚å›¾1ç»™å‡ºæ¥è¡¨æƒ…è¯†åˆ«çš„å…·ä½“è¿‡ç¨‹ã€‚å›¾1 è¡¨æƒ…è¯†åˆ«è¿‡ç¨‹1.3 æœ¬è¯¾é¢˜çš„å†…å®¹é˜è¿°æœ¬è¯¾é¢˜ä¸»è¦æ¨¡æ‹Ÿæ—…æ¸¸æ™¯åŒºè¿™ä¸€ç‰¹å®šåº”ç”¨åœºæ™¯ï¼Œå…ˆååˆ†åˆ«è®­ç»ƒå‡ºä¸€å¥—å¯ç”¨çš„äººè„¸æ£€æµ‹ç¥ç»ç½‘ç»œæ¨¡å‹å’Œä¸€å¥—è´Ÿè´£å®Œæˆè¡¨æƒ…åˆ†ç±»çš„ç½‘ç»œæ¨¡å‹ï¼Œå°†ä¸¤ä¸ªæ¨¡å‹è¿›è¡Œèåˆã€‚ä¹‹åå®Œæˆå¯¹åŒ…å«äººè„¸çš„å›¾åƒä»¥åŠè§†é¢‘æ•°æ®çš„é‡‡é›†ï¼Œå®Œæˆå¯¹é‡‡é›†çš„å›¾åƒä»¥åŠè§†é¢‘æ•°æ®è¿›è¡Œé¢„å¤„ç†åï¼Œè¾“é€è¿›è®­ç»ƒå¥½å¹¶èåˆåœ¨ä¸€èµ·çš„ç¥ç»ç½‘ç»œï¼Œä½¿ç”¨æ¨¡å‹å®ç°å¯¹æ•°æ®ä¸­äººè„¸çš„å®šä½ä»¥åŠé¢„æµ‹è¡¨æƒ…çš„æ‰€å±åˆ†ç±»ã€‚å®ç°æœ¬è¯¾é¢˜å…·ä½“æµç¨‹å¦‚ä¸‹ï¼š(1) é€‰å–è®­ç»ƒæ•°æ®é›†å®ç°è®­ç»ƒæœ¬è¯¾é¢˜æ‰€ç”¨å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œä¸€å…±è¦ç”¨åˆ°ä¸‰å¥—æ•°æ®é›†ï¼Œåˆ†åˆ«ç”¨æ¥è®­ç»ƒäººè„¸æ£€æµ‹æ¨¡å‹ã€äººè„¸å…³é”®ç‚¹å›å½’ä»¥åŠè¾¹æ¡†å›å½’ã€å’Œè¡¨æƒ…åˆ†ç±»çš„æ¨¡å‹ã€‚(2) ç¥ç»ç½‘ç»œçš„æ­å»ºå¦‚ä¸Šè¿°æ‰€ç¤ºï¼Œä¸€å…±éœ€è¦ä¸¤ä¸ªä¸åŒçš„ç½‘ç»œæ¨¡å‹ã€‚ç”¨äºå®ç°äººè„¸æ£€æµ‹çš„æ¨¡å‹æˆ‘é‡‡ç”¨äº†å¤šä»»åŠ¡å·ç§¯ç¥ç»ç½‘ç»œ(MTCNN)ï¼Œè¯¥ç½‘ç»œå¯ä»¥åŒæ—¶å®ç°äººè„¸çš„æ£€æµ‹ä¸å¯¹é½ï¼ŒåŒæ—¶å®Œæˆäººè„¸å…³é”®ç‚¹å®šä½ã€‚ç”¨äºäººè„¸è¡¨æƒ…åˆ†ç±»çš„ç½‘ç»œï¼Œæˆ‘é‡‡ç”¨äº†ç»å…¸çš„AlexNetç»“æ„8ï¼Œä½†å°†å…¶ç‰¹å¾ç»´åº¦è¿›è¡Œäº†è°ƒæ•´ï¼Œä»åŸå§‹çš„åƒåˆ†ç±»ç½‘ç»œé™æˆ7åˆ†ç±»ç½‘ç»œã€‚(3) ç½‘ç»œè®­ç»ƒä½¿ç”¨çš„æ·±åº¦å­¦ä¹ æ¡†æ¶Caffeï¼Œå®Œæˆå¯¹ç¥ç»ç½‘ç»œçš„æ­å»ºå’Œè®­ç»ƒï¼ŒCaffeæ¡†æ¶å¯¹äºç®€å•æ¨¡å‹çš„å»ºç«‹å’Œè®­ç»ƒï¼Œå¯¹äºåˆšåˆšæ¥è§¦æ·±åº¦å­¦ä¹ çš„äººæ›´åŠ å‹å¥½ä¸”æ›´åŠ ä¾¿æ·ã€‚å¹¶ä¸”Caffeæ‹¥æœ‰pythonå’Œmatlabçš„æ¥å£ï¼Œå¯ä»¥æ–¹ä¾¿çš„æ­é…åˆ©ç”¨opencvå¯¹å›¾åƒè¿›è¡Œå¤„ç†ã€‚(4) æµ‹è¯•ä»¥åŠæ¨¡å‹å®Œå–„åœ¨ç½‘ç»œè®­ç»ƒç»“æŸåï¼Œå°±å¯ä»¥ç”¨å»ºç«‹å¥½çš„ç½‘ç»œæ¨¡å‹ï¼Œåœ¨å®é™…çš„åº”ç”¨åœºæ™¯ä¸‹è¿›è¡Œæµ‹è¯•ï¼Œæ£€æµ‹ç†è®ºæ˜¯å¦ç¬¦åˆå®é™…ï¼ŒåŠæ—¶å‘ç°é—®é¢˜å¹¶å°½é‡å®Œæˆå¯¹ç½‘ç»œæ¨¡å‹çš„ä¿®æ­£ã€‚1.4 æœ¬æ–‡çš„ç»„ç»‡ç»“æ„ç¬¬ä¸€ç«  ç»ªè®ºã€‚ä¸»è¦å¯¹å¼€å±•æœ¬è¯¾é¢˜çš„ç ”ç©¶èƒŒæ™¯è¿›è¡Œä»‹ç»ï¼Œè¯´æ˜ç ”ç©¶æ­¤è¯¾é¢˜çš„ç›®çš„å’Œæ„ä¹‰ï¼Œç®€å•æ€»ç»“å½“å‰å›½å†…å¤–é’ˆå¯¹äººè„¸æ£€æµ‹å’Œäººè„¸è¡¨æƒ…æ£€æµ‹ç ”ç©¶çš„ä¸»æ”»æ–¹å‘å’Œæ–¹æ³•ï¼Œå¹¶ç®€å•ä»‹ç»å¯¹äºæœ¬è¯¾é¢˜çš„ä¸»è¦å†…å®¹å’Œå®ç°æ­¥éª¤ä»¥åŠä½¿ç”¨æ–¹æ³•ï¼Œæ¦‚æ‹¬æœ¬æ–‡çš„ç« èŠ‚å®‰æ’ã€‚ç¬¬äºŒç«  æ·±åº¦å­¦ä¹ çš„åŸºæœ¬åŸç†ã€‚æœ¬æ–‡é‡ç‚¹ä½¿ç”¨çš„æ–¹æ³•éƒ½æ˜¯åŸºäºå·ç§¯ç¥ç»ç½‘ç»œæ¥å®Œæˆå¯¹æ•°æ®ç‰¹å¾çš„æå–çš„ï¼Œä»è€Œå¯ä»¥è¿›ä¸€æ­¥å®ç°åˆ†ç±»ä»»åŠ¡ã€‚æœ¬ç« å°†ç®€è¦å¯¹å·ç§¯ç¥ç»ç½‘ç»œçš„åŸç†å’Œç»“æ„è¿›è¡Œé˜è¿°ã€‚ä¸ºäº†èƒ½å¤Ÿå‡†ç¡®ä¸”å®æ—¶çš„ä»ç»™å®šå›¾åƒæˆ–è§†é¢‘æ•°æ®ä¸­å®Œæˆå¯¹äººè„¸çš„æ£€æµ‹å’Œè¡¨æƒ…çš„è¯†åˆ«ï¼Œäººè„¸æ£€æµ‹çš„ä»»åŠ¡æ˜¾å¾—å°¤ä¸ºé‡è¦ï¼Œæ‰€ä»¥ä¸€ä¸ªå¥½çš„äººè„¸æ£€æµ‹æ¨¡å‹æ˜¯ä¸å¯æˆ–ç¼ºåœ°ã€‚æœ¬ç« è¿›è€Œå°†é‡ç‚¹ä»‹ç»å¤šä»»åŠ¡å·ç§¯ç¥ç»ç½‘ç»œç†è®ºï¼Œä»¥åŠåœ¨å…·ä½“å®ç°äººè„¸æ£€æµ‹ä»»åŠ¡æ—¶æ¶‰åŠåˆ°çš„æ–¹æ³•æˆ–ç®—æ³•ã€‚ç¬¬ä¸‰ç«  äººè„¸æ£€æµ‹åŠäººè„¸è¡¨æƒ…è¯†åˆ«ç½‘ç»œç»“æ„ã€‚æœ¬ç« ä¸»è¦å¯¹äººè„¸æ£€æµ‹ã€äººè„¸è¡¨æƒ…è¯†åˆ«çš„å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œè®¨è®ºï¼Œå°†è¯¦ç»†é˜è¿°æ‰€ç”¨ç½‘ç»œæ¶æ„å’Œè®­ç»ƒæ–¹æ³•ã€‚ç¬¬å››ç«  å®éªŒä¸æµ‹è¯•ã€‚æ ¹æ®ç ”ç©¶çš„è¯¾é¢˜ï¼Œå®éªŒæ­¥éª¤å¯è¢«åˆ†ä¸ºå››æ­¥ï¼šï¼ˆ1ï¼‰è®­ç»ƒæ•°æ®é›†çš„é€‰å–ï¼›ï¼ˆ2ï¼‰ç¥ç»ç½‘ç»œæ¨¡å‹çš„æ­å»ºåŠè®­ç»ƒï¼›ï¼ˆ3ï¼‰å®é™…ä»»åŠ¡ä¸‹çš„æ£€æµ‹å’Œå¯¹æ¨¡å‹çš„ä¿®æ­£ã€‚æœ¬ç« ä¸»è¦è®°å½•ç”¨å·ç§¯ç¥ç»ç½‘ç»œå®ç°äººè„¸è¡¨æƒ…è¯†åˆ«ç³»ç»Ÿçš„è¿‡ç¨‹å¹¶å®é™…æµ‹è¯•ä½¿ç”¨æ•ˆæœã€‚2. æ·±åº¦å­¦ä¹ çš„åŸºæœ¬åŸç†2.1 å¼•è¨€æœºå™¨å­¦ä¹ æŠ€æœ¯ä¸ºç°ä»£ç¤¾ä¼šçš„å„ä¸ªæ–¹é¢æä¾›äº†å¼ºå¤§çš„åŠ¨åŠ›ï¼šä»ç½‘ç»œæœç´¢åˆ°ç¤¾äº¤ç½‘ç»œä¸Šçš„å†…å®¹è¿‡æ»¤å†åˆ°ç”µå­å•†åŠ¡ç½‘ç«™ä¸Šçš„æ¨èæœºåˆ¶ï¼Œå¹¶ä¸”å®ƒåœ¨è¯¸å¦‚ç›¸æœºå’Œæ™ºèƒ½æ‰‹æœºä¹‹ç±»çš„æ¶ˆè´¹äº§å“ä¸­è¶Šæ¥è¶Šé¢‘ç¹åœ°è¢«åˆ©ç”¨ã€‚ç„¶è€Œå¸¸è§„çš„æœºå™¨å­¦ä¹ æŠ€æœ¯åœ¨å¤„ç†åŸå§‹æ ¼å¼çš„è‡ªç„¶æ•°æ®æ–¹é¢å—åˆ°äº†é™åˆ¶ã€‚å‡ åå¹´æ¥ï¼Œæƒ³è¦æ„å»ºä¸€ä¸ªæ¨¡å¼è¯†åˆ«æˆ–è€…æœºå™¨å­¦ä¹ ç³»ç»Ÿï¼Œéœ€è¦éå¸¸ä»”ç»†çš„å·¥ç¨‹è®¾è®¡å’Œç›¸å½“å¤šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼Œæ‰èƒ½è®¾è®¡å‡ºä¸€ä¸ªå¯ä»¥ä»è¾“å…¥ä¸­æ£€æµ‹æˆ–åˆ†ç±»å…¶æ¨¡å¼çš„å­å­¦ä¹ ç³»ç»Ÿã€‚è¡¨å¾å­¦ä¹ æ˜¯ä¸€ç»„å…è®¸å‘æœºå™¨æä¾›åŸå§‹æ•°æ®å¹¶å¯ä»¥è‡ªåŠ¨å‘ç°éœ€è¦è¢«æ£€æµ‹æˆ–åˆ†ç±»çš„è¡¨å¾çš„æ–¹æ³•ã€‚æ·±åº¦å­¦ä¹ æ–¹æ³•æ˜¯å…·æœ‰å¤šå±‚è¡¨å¾çš„è¡¨å¾å­¦ä¹ æ–¹æ³•ï¼Œè¿™äº›å±‚çº§æ˜¯é€šè¿‡ç»„åˆç®€å•ä½†éçº¿æ€§çš„æ¨¡å—è€Œè·å¾—çš„ï¼Œæ¯ä¸ªæ¨¡å—å°†ä¸€ä¸ªå±‚çº§ï¼ˆä»åŸå§‹è¾“å…¥å¼€å§‹ï¼‰çš„è¡¨å¾è½¬æ¢ä¸ºæ›´é«˜ä¸”ç¨å¾®æ›´åŠ æŠ½è±¡çš„å±‚çº§çš„è¡¨å¾ã€‚æœ‰äº†è¶³å¤Ÿå¤šçš„æ­¤ç±»è½¬æ¢ï¼Œéå¸¸å¤æ‚çš„å‡½æ•°æ¨¡å‹ä¹Ÿå¯ä»¥è¢«å­¦ä¹ äº†ã€‚äº‹å®è¯æ˜ï¼Œæ·±åº¦å­¦ä¹ æ–¹æ³•éå¸¸å–„äºå‘ç°é«˜ç»´æ•°æ®ä¸­çš„å¤æ‚ç»“æ„ï¼Œå› æ­¤åœ¨ç§‘ç ”ï¼Œå•†ä¸šå’Œæ”¿åºœç­‰ä¼—å¤šé¢†åŸŸéƒ½æ˜¯é€‚ç”¨çš„ã€‚æ·±å±‚ç¥ç»ç½‘ç»œæ˜¯ç›®å‰ä¸»è¦çš„æ·±åº¦å­¦ä¹ å½¢å¼ï¼Œå·ç§¯ç¥ç»ç½‘ç»œå°±æ˜¯å…¶ä¸­ä¸€ç§ç»å…¸çš„ç»“æ„ã€‚å·ç§¯ç¥ç»ç½‘ç»œæ˜¯è¢«è®¾è®¡ç”¨æ¥å¤„ç†å¤šç»´æ•°ç»„æ•°æ®çš„ï¼Œæ¯”å¦‚ä¸€å¼ åŒ…å«ä¸‰ä¸ªç”±ä¸‰é¢œè‰²é€šé“ç»„æˆçš„äºŒç»´åƒç´ å€¼çš„å½©è‰²å›¾åƒã€‚å·ç§¯ç¥ç»ç½‘ç»œä½¿ç”¨4ä¸ªå…³é”®çš„æ€æƒ³æ¥åˆ©ç”¨è‡ªç„¶ä¿¡å·çš„å±æ€§ï¼šå±€éƒ¨è¿æ¥ã€æƒå€¼å…±äº«ã€æ± åŒ–ä»¥åŠå¤šç½‘ç»œå±‚çš„ä½¿ç”¨9ã€‚æœ¬ç« å°†ç®€è¦æ¦‚æ‹¬å·ç§¯ç¥ç»ç½‘ç»œçš„ç»“æ„å’ŒåŸºæœ¬åŸç†ï¼Œå¹¶é‡ç‚¹ä»‹ç»ä¸€ä¸‹æœ¬æ–‡äººè„¸æ£€æµ‹æ‰€ç”¨çš„ç®—æ³• â€“ å¤šä»»åŠ¡å·ç§¯ç¥ç»ç½‘ç»œï¼ˆMTCNNï¼‰ã€‚2.2 å·ç§¯ç¥ç»ç½‘ç»œä¸€ä¸ªå…¸å‹çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„ç»“æ„å¯ä»¥è¢«åˆ†ä¸ºå‡ ä¸ªéƒ¨åˆ†ï¼Œå¦‚å›¾2æ‰€ç¤ºã€‚æœ€å‰é¢çš„éƒ¨åˆ†æ˜¯ç”±å·ç§¯å±‚å’Œæ± åŒ–å±‚ä¸¤ç§å±‚çº§ç»“æ„æ„æˆã€‚å·ç§¯å±‚å«æœ‰å¤šä¸ªç‰¹å¾å›¾ï¼ˆFeature Mapï¼‰ï¼Œè¿™äº›ç‰¹å¾å›¾åˆ™æ˜¯ç”±ç¥ç»å…ƒç»„æˆï¼Œæ¯ä¸€ä¸ªå•ç‹¬çš„ç¥ç»å…ƒé€šè¿‡å·ç§¯æ ¸ä¸å½“å‰ç‰¹å¾å›¾ä¹‹å‰çš„ç‰¹å¾å›¾çš„å±€éƒ¨ç›¸è¿æ¥ï¼Œè¿™é‡Œçš„å·ç§¯æ ¸æ˜¯ä¸€ç»„ç”±æƒå€¼ç»„æˆçš„çŸ©é˜µï¼Œè€Œè¿™ä¸ªé“¾æ¥çš„å±€éƒ¨å¯ä»¥è¢«ç§°ä¸ºå±€éƒ¨æ„Ÿå—é‡ã€‚è¿™é‡Œå¾—åˆ°çš„å±€éƒ¨åŠ æƒå’Œå°†è¢«é€šè¿‡ä¸€æ¬¡éçº¿æ€§å‡½æ•°çš„è¿ç®—ï¼Œä¾‹å¦‚ä¿®æ­£çº¿æ€§å•å…ƒï¼ˆReLUå‡½æ•°ï¼‰ï¼Œä¹‹åå°†å¾—åˆ°è¯¥å·ç§¯å±‚ä¸­æ‰€æœ‰ç¥ç»å…ƒçš„è¾“å‡ºå€¼ã€‚åœ¨åŒä¸€ç‰¹å¾å›¾ä¸­ï¼Œæ‰€æœ‰çš„ç¥ç»å…ƒå…±äº«ç›¸åŒçš„å·ç§¯æ ¸ï¼Œè¯¥ç‰¹æ€§ä¹Ÿè¢«ç§°ä¸ºæƒå€¼å…±äº«ï¼Œä½†åœ¨ä¸åŒçš„ç‰¹å¾å›¾ä¸­ï¼Œæ‰€ç”¨çš„å·ç§¯æ ¸æ˜¯ä¸åŒçš„ã€‚é€šè¿‡æƒå€¼å…±äº«æ“ä½œï¼Œå¯ä»¥æœ‰æ•ˆçš„é™ä½æ¨¡å‹å¤æ‚åº¦ï¼Œå¹¶å¯ä»¥æ›´å¿«æ·åœ°è®­ç»ƒç½‘ç»œã€‚ä½¿ç”¨è¿™ç§ç»“æ„çš„åŸå› æœ‰ä¸¤ä¸ªï¼šç¬¬ä¸€ï¼Œå¯¹äºä¸€ä¸ªé˜µåˆ—æ•°æ®ï¼Œä¾‹å¦‚å›¾åƒï¼Œå±€éƒ¨çš„ä¸€äº›æ•°æ®æ˜¯å…·æœ‰é«˜åº¦ç›¸å…³æ€§çš„ï¼Œä¸€ä¸ªå¯è¾¨åˆ«çš„å›¾æ¡ˆæ˜¯ç”±ä¸€ä¸ªåŒºåŸŸå†…çš„æ‰€æœ‰æ•°æ®å½¢æˆçš„è€Œå¹¶éæŸä¸€ç‚¹çš„ä¸€ä¸ªæ•°æ®ï¼›ç¬¬äºŒï¼Œåœ¨ä¾‹å¦‚å›¾åƒè¿™ç§æ•°æ®ä¸­ï¼Œå±€éƒ¨çš„æ•°æ®æ˜¯å’Œå…¶ä½ç½®æ— å…³çš„ï¼Œä¾‹å¦‚äººè„¸å¯èƒ½ä¼šå‡ºç°åœ¨ä¸€å¼ å›¾åƒä¸­çš„ä»»æ„ä½ç½®ã€‚åœ¨æ•°å­¦æ„ä¹‰ä¸Šï¼Œè¿™é‡Œçš„å·ç§¯æ ¸æ“ä½œå°±æ˜¯ä¸€ä¸ªç¦»æ•£çš„å·ç§¯æ“ä½œï¼Œå·ç§¯ç¥ç»ç½‘ç»œä¹Ÿå› æ­¤è€Œå¾—åã€‚åœ¨å·ç§¯å±‚ä¹‹åï¼Œç´§æ¥ç€çš„æ˜¯æ± åŒ–å±‚ï¼Œå’Œå·ç§¯å±‚ç»“æ„ç±»ä¼¼ï¼Œç”±æ± åŒ–æ“ä½œå¾—åˆ°çš„æ¯ä¸€ä¸ªç‰¹å¾å›¾éƒ½å”¯ä¸€å¯¹åº”å…¶ä¹‹å‰å±‚çš„æŸä¸€ä¸ªå±€éƒ¨æ„Ÿå—é‡ã€‚å¦‚æœè¯´å·ç§¯å±‚çš„ä½œç”¨æ˜¯ç”¨æ¥æ£€æµ‹ä¸Šä¸€å±‚ä¸­å±€éƒ¨ç‰¹å¾çš„ï¼Œé‚£ä¹ˆæ± åŒ–å±‚æ‰®æ¼”çš„è§’è‰²åˆ™æ˜¯èåˆå‰é¢æ£€æµ‹åˆ°çš„ç›¸ä¼¼çš„ç‰¹å¾ã€‚ç”±äºç»„æˆæŸä¸€å›¾æ¡ˆçš„ç‰¹å¾æ˜¯å¯å˜çš„ï¼Œæ‰€ä»¥ä¸ºäº†æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œå¯¹ä¹‹å‰æå–åˆ°çš„ç‰¹å¾å›¾è¿›è¡Œå‹ç¼©å¤„ç†æ˜¯å¾ˆæœ‰å¿…è¦çš„ï¼Œè¿™ä¹ˆåšåŒæ—¶è¿˜å¯ä»¥é™ä½æ¨¡å‹çš„å¤æ‚åº¦ï¼Œæé«˜è®­ç»ƒé€Ÿåº¦ã€‚å¸¸ç”¨çš„æ± åŒ–æ“ä½œåŒ…æ‹¬æœ€å¤§æ± åŒ–ã€å‡å€¼æ± åŒ–ç­‰ï¼Œåˆ†åˆ«å¯¹å…¶å¯¹åº”çš„å±€éƒ¨æ„Ÿå—é‡è¿›è¡Œæ±‚æœ€å¤§å€¼æˆ–å‡å€¼ï¼Œè‡³äºåœ¨ä¸åŒåº”ç”¨åœºæ™¯ä¸‹åº”è¯¥åº”ç”¨å“ªäº›æ± åŒ–æ“ä½œï¼ŒBoureauç­‰äºº10åšå‡ºäº†è¯¦å°½çš„ç†è®ºåˆ†æã€‚ç»è¿‡å‡ ä¸ªå·ç§¯å±‚å’Œæ± åŒ–å±‚çš„åå¤æ“ä½œåï¼Œåœ¨CNNç»“æ„ä¸­ä¸€èˆ¬è¿˜ä¼šæœ‰1ä¸ªæˆ–ä»¥ä¸Šçš„å…¨è¿æ¥å±‚ã€‚ä¸ä¼ ç»Ÿçš„ç¥ç»ç½‘è·¯ç±»ä¼¼ï¼ŒCNNä¸­çš„å…¨è¿æ¥å±‚ä¸­çš„å•ä¸ªç¥ç»å…ƒä¼šä¸å…¶å‰ä¸€å±‚çš„æ‰€æœ‰ç¥ç»å…ƒä¸€ä¸€å¯¹åº”ç›¸è¿ï¼Œé€šè¿‡å…¨è¿æ¥å±‚åï¼Œä¹‹å‰ç»è¿‡å·ç§¯å±‚å’Œæ± åŒ–å±‚åå¾—åˆ°çš„åŒºåˆ†æ€§å±€éƒ¨ä¿¡æ¯å°†è¢«å¾—åˆ°æ•´åˆã€‚å¦‚æœæƒ³è¦æå‡æ•´ä¸ªç½‘ç»œç»“æ„çš„æ€§èƒ½ï¼Œå¯ä»¥åœ¨å…¨è¿æ¥å±‚ä¸­çš„æ¯ä¸ªç¥ç»å…ƒååŠ ä¸€ä¸ªéçº¿æ€§å‡½æ•°ï¼ˆä¾‹å¦‚ReLUå‡½æ•°ï¼‰ä»¥æé«˜å…¶éçº¿æ€§åˆ†ç±»èƒ½åŠ›ã€‚é€šè¿‡æœ€åä¸€å±‚å…¨è¿æ¥å±‚åï¼Œå°†å¾—åˆ°è¾“å‡ºå€¼ï¼Œè¿™äº›å€¼å½¢æˆè¾“å‡ºå±‚ï¼Œè¯¥å±‚å¯ä»¥é€šè¿‡é‡‡ç”¨ä¾‹å¦‚softmaxé€»è¾‘å›å½’çš„æ–¹æ³•è¿›è¡Œåˆ†ç±»ä»»åŠ¡ã€‚æœ€åé€šè¿‡è®¡ç®—ä¸€ä¸ªæŸå¤±å‡½æ•°ï¼Œå°±å¯ä»¥å¼€å§‹ä½¿ç”¨åå‘ä¼ æ’­ç®—æ³•è¿›è¡Œç‰¹å¾å€¼çš„è¿­ä»£è®­ç»ƒäº†ã€‚å½“ç„¶ï¼Œæœ¬æ–‡åªæ˜¯ç®€å•å¯¹å·ç§¯ç¥ç»ç½‘ç»œçš„åŸç†å’Œç»“æ„è¿›è¡Œäº†æ¦‚è¿°ï¼Œå¾ˆæœ‰å¾ˆå¤šç»†èŠ‚ä¾‹å¦‚ç‰¹å¾å›¾ç»´åº¦å’Œæ•°é‡çš„é€‰å–ã€æŸå¤±å‡½æ•°çš„é€‰å–ã€æ­£åˆ™åŒ–æ–¹æ³•ç­‰éƒ½æ²¡æœ‰è¿›è¡Œæ€»ç»“ï¼Œç”±äºå…¶å†…å®¹è¿‡äºå†—é•¿ï¼Œæ‰€ä»¥ä»…å¯¹ä¸€äº›å¿…è¦çš„å†…å®¹è¿›è¡Œäº†é˜è¿°ã€‚å›¾2 å·ç§¯ç¥ç»ç½‘ç»œçš„åŸºæœ¬ç»“æ„2.3 äººè„¸æ£€æµ‹ç®—æ³•2.3.1 äººè„¸æ£€æµ‹ç®—æ³•æ¦‚è¿°äººè„¸æ£€æµ‹çš„ç›®æ ‡æ˜¯æ‰¾å‡ºè¾“å…¥å›¾åƒä¸­æ˜¯å¦æœ‰äººè„¸ï¼Œå¦‚æœæœ‰ï¼Œéœ€è¦è¾“å‡ºæ‰€æœ‰åŒ…å«äººè„¸çš„å¯¹åº”ä½ç½®ï¼Œç®—æ³•çš„è¾“å‡ºæ˜¯äººè„¸å¤–æ¥çŸ©å½¢åœ¨å›¾åƒä¸­çš„åæ ‡ï¼Œå¯èƒ½è¿˜åŒ…æ‹¬å§¿æ€å¦‚å€¾æ–œè§’åº¦ã€äº”å®˜ä½ç½®ç­‰ä¿¡æ¯ã€‚åœ¨ç›®æ ‡æ£€æµ‹æ‰€æœ‰çš„å­ç ”ç©¶é¢†åŸŸä¸­ï¼Œäººè„¸æ£€æµ‹æ˜¯ç›®å‰è¢«ç ”ç©¶çš„æœ€å……åˆ†çš„é—®é¢˜ä¹‹ä¸€ï¼Œå®ƒåœ¨å®‰é˜²ç›‘æ§ã€äººæœºäº¤äº’ã€ç¤¾äº¤å¨±ä¹ç­‰æ–¹é¢æœ‰å¾ˆé«˜çš„åº”ç”¨ä»·å€¼ï¼Œä¹Ÿæ˜¯æ•´ä¸ªäººè„¸è¯†åˆ«ã€è¡¨æƒ…è¯†åˆ«ç­‰ç®—æ³•çš„ç¬¬ä¸€æ­¥ã€‚è™½ç„¶äººè„¸è¿‘ä¼¼äºä¸€ä¸ªåˆšä½“ï¼Œéƒ½æ˜¯ç”±äº”å®˜æ„æˆï¼Œä½†ç”±äºè¡¨æƒ…ã€å§¿æ€ã€è§’åº¦ã€å…‰ç…§ç­‰åŸå› ï¼Œå‡†ç¡®çš„å®šä½å›¾åƒä¸­çš„äººè„¸æ˜¯æœ‰éš¾åº¦çš„ã€‚ä¸€ä¸ªäººè„¸æ£€æµ‹ç®—æ³•éœ€è¦è€ƒè™‘å¦‚ä¸‹å‡ ä¸ªé—®é¢˜ï¼šäººè„¸ä¼šå‡ºç°åœ¨å›¾åƒä¸­çš„ä»»æ„ä½ç½®ï¼›äººè„¸åœ¨å›¾åƒä¸­çš„å°ºå¯¸æ˜¯å¯å˜çš„ï¼›å›¾åƒä¸­çš„äººè„¸å¯èƒ½æ˜¯ä»»æ„è§’åº¦å’Œå§¿æ€çš„ï¼›å›¾åƒä¸­çš„äººè„¸æœ‰å¯èƒ½è¢«é®æŒ¡ã€‚æ­£ç¡®æ£€æµ‹ç‡å’Œè¯¯æ£€ç‡ä¸¤ä¸ªæŒ‡æ ‡é€šå¸¸è¢«ç”¨æ¥è¯„ä»·ä¸€ä¸ªäººè„¸æ£€æµ‹æ¨¡å‹çš„å¥½åï¼Œæ­£ç¡®æ£€æµ‹ç‡è¢«å®šä¹‰ä¸º\\[\\text{æ­£ç¡®æ£€æµ‹ç‡}=\\frac{\\text{æ£€æµ‹åˆ°çš„äººè„¸æ•°é‡}}{\\text{å›¾åƒä¸­æ‰€æœ‰çš„äººè„¸æ•°é‡}}\\]è¯¯æ£€ç‡è¢«å®šä¹‰ä¸º\\[\\text{è¯¯æ£€ç‡}=\\frac{\\text{è¯¯æŠ¥ä¸ªæ•°}}{\\text{å›¾åƒä¸­æ‰€æœ‰éäººè„¸æ‰«æçª—å£æ•°é‡}}\\]ä¸€ä¸ªä¼˜ç§€çš„äººè„¸æ£€æµ‹ç®—æ³•ï¼Œå°†ä¼šåœ¨æ­£ç¡®æ£€æµ‹ç‡å’Œè¯¯æ£€ç‡ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œä»è€Œè·å¾—å°½å¯èƒ½é«˜çš„æ­£ç¡®æ£€æµ‹ç‡å’Œä½çš„è¯¯æ£€ç‡ã€‚ä¸€ä¸ªå…¸å‹çš„äººè„¸åˆ†ç±»å™¨æ¨¡å‹éœ€è¦ç”¨å¤§é‡çš„äººè„¸å’Œéäººè„¸å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œè®­ç»ƒåå¯ä»¥å¾—åˆ°ä¸€ä¸ªèƒ½å¤Ÿè§£å†³äºŒåˆ†ç±»é—®é¢˜çš„åˆ†ç±»å™¨æ¨¡å‹ï¼Œè¯¥åˆ†ç±»å™¨æ¨¡å‹ä¹Ÿè¢«ç§°ä¸ºäººè„¸æ£€æµ‹æ¨¡ç‰ˆã€‚è¯¥åˆ†ç±»å™¨æ¨¡å‹å¯ä»¥å¯¹è¾“å…¥å›¾åƒè¿›è¡Œé¢„æµ‹ï¼Œä»è€Œè·å¾—å›¾ä¸­æ˜¯å¦ä¸ºäººè„¸ï¼Œä½†é€šå¸¸è¯¥åˆ†ç±»å™¨ä»…å¯ä»¥å¯¹å›ºå®šå¤§å°çš„è¾“å…¥è¿›è¡Œå¤„ç†ï¼Œæ‰€ä»¥å¯¹ä¸€èˆ¬è¾“å…¥ï¼ˆéè®­ç»ƒæ—¶è®¾å®šçš„å°ºå¯¸ï¼‰çš„æ•°æ®ï¼Œéœ€è¦è¿›è¡Œå¦‚å›¾3æ‰€ç¤ºçš„å¤„ç†ã€‚ç”±äºäººè„¸çš„å°ºå¯¸å¯ä»¥æ˜¯ä»»æ„çš„ï¼Œæ‰€ä»¥ç¬¬ä¸€æ­¥éœ€è¦å¯¹åŸå§‹è¾“å…¥å›¾åƒè¿›è¡Œå°ºå¯¸çš„æ”¾å¤§å’Œç¼©å°ï¼Œä»è€Œæ„å»ºå›¾åƒé‡‘å­—å¡”ï¼›æ¥ä¸‹æ¥ä¸ºäº†æ£€æµ‹æ‰€æœ‰å›¾åƒé‡‘å­—å¡”å†…å›¾åƒçš„ä»»æ„ä½ç½®çš„äººè„¸ï¼Œéœ€è¦å¯¹æ‰€æœ‰å›¾åƒè¿›è¡Œæ»‘åŠ¨çª—å£ï¼ˆSliding Windowï¼‰æ£€æµ‹ï¼Œç®€å•æ¥è¯´å°±æ˜¯ç”¨ä¸€ä¸ªå›ºå®šå°ºå¯¸çš„çª—å£åœ¨æ‰€æœ‰å›¾åƒå†…è¿›è¡Œä»ä¸Šåˆ°ä¸‹ã€ä»å·¦åˆ°å³çš„æ»‘åŠ¨æ£€æµ‹ã€‚è¿™ä¸ªè¿‡ç¨‹éœ€è¦å¯¹å¤§é‡ä¸åŒå°ºåº¦çš„å›¾åƒè¿›è¡Œåå¤æ‰«æï¼Œä»è€Œè·å¾—é¢„é€‰äººè„¸çª—å£ï¼Œæ‰€ä»¥è¿™ä¸ªè¿‡ç¨‹ä¼šæ¶ˆè€—å¤§é‡æ—¶é—´ã€‚å›¾3 å¤šå°ºåº¦æ»‘åŠ¨çª—å£å®ç°äººè„¸æ£€æµ‹å¦‚å›¾3æ‰€ç¤ºï¼Œæ»‘åŠ¨çª—å£åœ¨å®Œæˆæ£€æµ‹åï¼Œè¿˜ä¼šè¿›è¡Œä¸€æ­¥éæå¤§å€¼æŠ‘åˆ¶ï¼ˆNMS, Non-maximum Suppressionï¼‰æ“ä½œã€‚é¡¾åæ€ä¹‰ï¼Œè¯¥æ“ä½œå°±æ˜¯æŠ‘åˆ¶å±€éƒ¨éæå¤§å€¼çš„é¢„æµ‹ï¼Œä»è€Œä¿ç•™æ¦‚ç‡æœ€é«˜çš„å±€éƒ¨æœ€å¤§å€¼ã€‚è¯¥ç®—æ³•ä¸ä»…é€‚ç”¨äºäººè„¸æ£€æµ‹ï¼Œåœ¨å‡ ä¹æ‰€æœ‰çš„ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œéƒ½ä¼šç”¨NMSæ¥å¯¹æ‰€æœ‰çš„é¢„æµ‹çª—å£è¿›è¡Œç­›é€‰ã€‚å¦‚å›¾4ï¼Œå‡è®¾å½“å‰å®Œæˆæ»‘åŠ¨çª—å£æ£€æµ‹åï¼Œæœ‰å¤šä¸ªäººè„¸çš„é¢„é€‰çª—å£å­˜åœ¨ï¼Œå¯ä»¥çœ‹å‡ºè¿™äº›é¢„é€‰çª—å£æ˜¯é«˜åº¦é‡åˆçš„ï¼Œæˆ‘ä»¬çš„ä»»åŠ¡æ˜¯å°†è¿™äº›é«˜åº¦é‡åˆçš„é¢„é€‰çª—å£è¿›è¡Œæ’åºï¼Œè¿­ä»£æ’é™¤æ‰æ‰€æœ‰æ¦‚ç‡ç›¸å¯¹è¾ƒä½ä¸”ä¸æ¦‚ç‡ç›¸å¯¹è¾ƒé«˜çš„é¢„é€‰çª—å£é«˜åº¦é‡åˆçš„é¢„é€‰çª—å£ï¼Œä»è€Œä¿ç•™IOUç›¸å¯¹è¾ƒä½çš„æ‰€æœ‰é«˜æ¦‚ç‡é¢„é€‰çª—å£ã€‚ç»è¿‡NMSæ“ä½œåï¼Œå¤§é‡çš„é«˜åº¦é‡åˆçš„æ— ç”¨é¢„æµ‹çª—å£å°†ä¼šè¢«ç­›é€‰æ‰ã€‚å›¾4 NMSæ“ä½œä»¥ä¸Šæ˜¯å¯¹æ•´ä¸ªäººè„¸æ£€æµ‹æµç¨‹çš„ç®€å•æ¦‚è¿°ï¼ŒåŸºæœ¬è¦†ç›–äº†å…¸å‹äººè„¸æ£€æµ‹ç®—æ³•çš„æ‰€æœ‰æµç¨‹ã€‚ä¸åŒçš„äººè„¸æ£€æµ‹ç®—æ³•ï¼Œå¤§å¤šæ˜¯åŸºäºè¯¥æµç¨‹åŸºç¡€ä¸Šï¼Œå¯¹äººè„¸åˆ†ç±»å™¨è¿›è¡Œæ”¹è‰¯ï¼Œä¾‹å¦‚çº§è”CNNç®—æ³•1ï¼ˆCascade CNNï¼‰ï¼Œè¯¥ç®—æ³•çš„äººè„¸åˆ†ç±»å™¨é‡‡ç”¨çš„æ˜¯çº§è”å¼çš„æ¡†æ¶ã€‚åœ¨æ„å»ºå®Œå›¾åƒé‡‘å­—å¡”åï¼Œåœ¨ç¬¬ä¸€å±‚ç½‘ç»œä¸Šï¼ˆ12-netï¼‰å°†å¯¹è¿™äº›å›¾åƒè¿›è¡Œæ»‘åŠ¨çª—å£æ£€æµ‹å¹¶è¿‡æ»¤æ‰è¶…è¿‡90%çš„é¢„é€‰çª—å£ï¼Œå‰©ä¸‹çš„çª—å£å°†è¢«é€å…¥ä¸€çº§æ ¡å‡†ç½‘ç»œï¼ˆ12-calibration-netï¼‰è¿›è¡Œçª—å£æ ¡å‡†ï¼Œå¹¶é‡‡ç”¨NMSåˆå¹¶é«˜åº¦é‡å çš„æ£€æµ‹çª—å£ï¼Œåœ¨åé¢æ›´é«˜å±‚çº§çš„ç½‘ç»œä¸­ï¼Œå°†è¿›è¡Œäººè„¸æ£€æµ‹æ¡†ä½ç½®è¿›ä¸€æ­¥çš„çŸ«æ­£å’Œè¿›ä¸€æ­¥çš„ç­›é€‰ï¼Œè¯¦ç»†ç½‘ç»œç»“æ„å¯ä»¥å‚è€ƒæ–‡çŒ®[1]ã€‚è¯¥ç®—æ³•åœ¨ä¸€å®šç¨‹åº¦ä¸Šé™ä½äº†ä¼ ç»Ÿç®—æ³•åœ¨å¼€æ”¾ç¯å¢ƒä¸­å¯¹äºå…‰ç…§ã€ä½ç½®ã€è§’åº¦ç­‰çš„æ•æ„Ÿåº¦ï¼Œç›¸è¾ƒäºä¹‹å‰åŸºäºä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œçš„ä¸€äº›ç®—æ³•æ˜¯æœ‰å¾ˆå¤§çªç ´çš„ï¼Œä½†æ˜¯ç”±äºæ¡†æ¶çš„ç¬¬ä¸€çº§ç½‘ç»œä»ç„¶æ˜¯é‡‡ç”¨æ»‘åŠ¨çª—å£çš„å½¢å¼è¿›äººè„¸ä½ç½®çš„åˆæ­¥æ£€æµ‹ï¼Œæ‰€ä»¥åœ¨å¯¹æ•ˆç‡è¦æ±‚è¾ƒé«˜çš„åº”ç”¨åœºæ™¯ä¸‹ï¼Œè¯¥ç®—æ³•è¿˜æ˜¯è¢«é™åˆ¶äº†ï¼Œè€Œä¸”çº§è”CNNæ¨¡å‹å¯¹äºå°ç›®æ ‡äººè„¸æ£€æµ‹ä»ä¸æ˜¯å¾ˆå¥½ã€‚ä»ç„¶é‡‡ç”¨ç±»ä¼¼äºçº§è”CNNç®—æ³•çš„çº§è”å¼æ¡†æ¶ï¼ŒZhangç­‰äºº11åœ¨2016å¹´æå‡ºå¦ä¸€ç§çº§è”å¼çš„äººè„¸æ£€æµ‹æ¨¡å‹â€“å¤šä»»åŠ¡å·ç§¯ç¥ç»ç½‘ç»œï¼Œæœ¬æ–‡åç»­åœ¨ç¨‹åºå®ç°æ—¶ä¹Ÿæ˜¯é‡‡ç”¨çš„è¿™ç§äººè„¸æ£€æµ‹ç®—æ³•ï¼Œåœ¨ä¸‹ä¸€å°èŠ‚å°†å¯¹è¿™ç§ç®—æ³•è¿›è¡Œè¯¦ç»†é˜è¿°ã€‚2.3.2 å¤šä»»åŠ¡å·ç§¯ç¥ç»ç½‘ç»œMTCNNï¼ˆMulti-task Convolutional Neural Networkï¼‰é¡¾åæ€ä¹‰æ˜¯ä¸€ç§é›†äººè„¸åˆ†ç±»ã€äººè„¸åŒºåŸŸå›å½’å’Œäººè„¸å…³é”®ç‚¹å›å½’ä¸‰ä¸ªä»»åŠ¡äºä¸€èº«çš„å¤šä»»åŠ¡åŒæ—¶æ‰§è¡Œçš„ç®—æ³•ï¼Œç»“æ„é‡‡ç”¨çš„æ˜¯ç±»ä¼¼äºçº§è”CNNç®—æ³•çš„çº§è”å¼ç»“æ„ï¼Œä¸åŒäºçº§è”CNNç®—æ³•çš„æ˜¯åœ¨åˆçº§çš„æ£€æµ‹ä¸­æ²¡æœ‰ä½¿ç”¨æ»‘åŠ¨çª—å£çš„æ–¹æ³•æ¥å¯¹è¾“å…¥å›¾åƒè¿›è¡Œåˆæ­¥äººè„¸æ¡†é¢„æµ‹ï¼Œè¿™ç§ç®—æ³•åŒæ—¶å…¼é¡¾äº†è®¡ç®—æ•ˆç‡å’Œå‡†ç¡®ç‡ï¼Œå¯¹ä¸Šè¿°ä¸¤ç§è¯„æµ‹æŒ‡æ ‡è¿›è¡Œäº†è‰¯å¥½çš„å¹³è¡¡ã€‚åœ¨å¼€å§‹ä»‹ç»MTCNNçš„ç½‘ç»œç»“æ„å’Œå·¥ä½œåŸç†ä¹‹å‰ï¼Œéœ€è¦å…ˆäº†è§£è¾¹æ¡†å›å½’ï¼ˆBounding-Box regressionï¼‰çš„å·¥ä½œåŸç†ï¼Œè¿™ä¹Ÿæ˜¯åœ¨MTCNNç½‘ç»œä¸­åœ¨æ‰§è¡Œäººè„¸è¾¹æ¡†é¢„æµ‹ã€äººè„¸å…³é”®ç‚¹å›å½’ä»»åŠ¡æ—¶è‡³å…³é‡è¦çš„ä¸€ä¸ªæ­¥éª¤ã€‚ä¹‹æ‰€ä»¥è¦æ‰§è¡Œè¾¹æ¡†å›å½’çš„æ“ä½œï¼Œæ˜¯ä¸ºäº†ä½¿é¢„æµ‹çš„äººè„¸è¾¹æ¡†ä½ç½®æˆ–è€…å¯¹äºäººè„¸å…³é”®ç‚¹çš„å®šä½æ›´åŠ å‡†ç¡®ï¼Œå¦‚å›¾5(a)æ‰€ç¤ºï¼Œåœ¨è¾“å…¥å›¾åƒç»è¿‡ç½‘ç»œåå¾—åˆ°çš„é¢„æµ‹äººè„¸çª—å£ä¸ºçº¢è‰²æ¡†è¦†ç›–åŒºåŸŸï¼Œè€Œå‡†ç¡®çš„äººè„¸ä½ç½®åº”è¯¥æ˜¯ç»¿è‰²æ¡†æ‰€è¦†ç›–çš„åŒºåŸŸï¼Œè¿™æ—¶çº¢è‰²æ¡†çš„å®šä½å¹¶ä¸å‡†ç¡®ï¼ˆä¾‹å¦‚ $IOU&amp;lt;0.7$)ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å¯¹é¢„æµ‹çª—å£è¿›è¡Œä¿®æ­£ã€‚å¯¹äºçª—å£ï¼Œä¸€èˆ¬ä½¿ç”¨å››ç»´å‘é‡ $(x,y,w,h)$ åˆ†åˆ«è¡¨ç¤ºçª—å£çš„ä¸­å¿ƒç‚¹åæ ‡ã€é«˜åº¦ã€å’Œå®½åº¦ï¼Œå¦‚å›¾5(b)ç»¿è‰²æ¡† $G$ ä»£è¡¨å‡†ç¡®ä½ç½®ï¼Œçº¢è‰²æ¡† $P$ ä»£è¡¨åŸå§‹çš„é¢„æµ‹ä½ç½®ï¼Œè€Œè“è‰²æ¡† $Gâ€™$ åˆ™ä»£è¡¨ä¸€ä¸ªç»è¿‡æ ¡å‡†åæ›´åŠ æ¥è¿‘äºçœŸå®çª—å£ $G$ çš„çª—å£åŒºåŸŸã€‚æ‰€ä»¥è¾¹æ¡†å›å½’çš„ç›®çš„å°±æ˜¯å½“ç»™å®š $(P_x,P_y,P_w,P_h)$ æ—¶ï¼Œå¯»æ±‚ä¸€ç§æ˜ å°„ $f$ ï¼Œä»è€Œä½¿å¾—\\[f(P_x,P_y,P_w,P_h)=(G&#39;_x,G&#39;_y,G&#39;_w,G&#39;_h)\\tag{2.1}\\]å¹¶ä¸”\\[(G&#39;_x,G&#39;_y,G&#39;_w,G&#39;_h) \\approx (G_x,G_y,G_w,G_h)\\tag{2.2}\\]è‡³äºå¦‚ä½•ä¿®æ”¹æŸå¤±å‡½æ•°ä»è€Œä½¿çª—å£ $P$ æ ¡æ­£æˆæ›´æ¥è¿‘å‡†ç¡®çª—å£çš„çª—å£ $Gâ€™$ ï¼Œæœ¬æ–‡ä¸åšè¯¦ç»†èµ˜è¿°ï¼Œå¯ä»¥å‚è€ƒæ–‡çŒ®12ã€‚å›¾5 è¾¹æ¡†å›å½’åŸç†MTCNNé‡‡ç”¨äº†ä¸‰ä¸ªçº§è”å·ç§¯ç¥ç»ç½‘ç»œçº§è”çš„ç»“æ„ï¼Œå¦‚å›¾6æ‰€ç¤ºï¼Œåˆ†åˆ«ä¸ºP-Netï¼ŒR-Netï¼ŒO-Netï¼Œä¸‰ä¸ªç½‘ç»œç»“æ„ä»ç®€å•åˆ°å¤æ‚ï¼Œé‡‡å–å€™é€‰æ¡†ä»¥åŠåˆ†ç±»å™¨çš„æ€æƒ³ï¼Œä»è€Œå¯ä»¥éå¸¸é«˜æ•ˆçš„è¿›è¡Œäººè„¸æ£€æµ‹ä»»åŠ¡ã€‚å›¾6 MTCNNæ•´ä½“ç»“æ„å›¾åƒé‡‘å­—å¡”ä¸­å›¾åƒé¦–å…ˆä¼šè¢«é€å…¥P-Netï¼ˆProposal Networkï¼‰ï¼Œé¡¾åæ€ä¹‰è¿™æ˜¯ä¸€ä¸ªäººè„¸åŒºåŸŸå»ºè®®ç½‘ç»œã€‚è¯¥ç½‘ç»œæ˜¯ä¸€ä¸ªå…¨å·ç§¯ç½‘ç»œ13ï¼Œç›¸è¾ƒäºä¸Šæ–‡æåˆ°çš„çº§è”CNNç½‘ç»œç¬¬ä¸€çº§çš„12-netï¼ŒP-Netä½¿ç”¨çš„å…¨å·ç§¯ç½‘ç»œçš„ä¼˜åŠ¿åœ¨äºç”±äºåœ¨ç½‘ç»œä¸­æ²¡æœ‰å…¨è¿æ¥å±‚ï¼ˆå½“ç„¶æœ‰äº›ç½‘ç»œåœ¨æœ‰å…¨è¿æ¥å±‚æ—¶ä¹Ÿå¯ä»¥æ¥å—ä»»æ„å°ºå¯¸çš„å›¾åƒè¾“å…¥ï¼Œä¾‹å¦‚ç©ºé—´é‡‘å­—å¡”æ± åŒ–æ³•14ï¼‰ï¼Œæ‰€ä»¥å®ƒå¯ä»¥æ¥å—ä»»æ„å°ºå¯¸çš„å›¾åƒè¾“å…¥ï¼Œä¸”ç›¸è¾ƒäºæ»‘åŠ¨çª—å£çš„æ–¹æ³•ï¼Œå·ç§¯è¿ç®—å¯ä»¥èŠ‚çœå¤§é‡çš„è®¡ç®—æ—¶é—´ã€‚P-Netæ˜¯ä¸€ä¸ªç›¸å¯¹è¾ƒæµ…çš„ç½‘ç»œï¼Œè€Œä¸”è¾“å…¥ç‰¹å¾ä»…ä¸º$12\\times12\\times3$å¤§å°ï¼Œæ‰€ä»¥è¯¥ç½‘ç»œå¯ä»¥å¿«é€Ÿåœ°åˆæ­¥å¯¹äººè„¸æ¡†ä½ç½®ä»¥åŠäººè„¸å…³é”®ç‚¹è¿›è¡Œå›å½’ã€‚ç”±äºè¿™ä¸ªç½‘ç»œä»…æ˜¯ç”±ä¸‰ä¸ªå·ç§¯å±‚å’Œä¸€ä¸ªæœ€å¤§æ± åŒ–å±‚æ„æˆçš„ç¥ç»ç½‘ç»œï¼Œè€Œä¸”æ¯å±‚ç»“æ„å¹¶ä¸å¤æ‚ï¼Œæ‰€ä»¥ç‰¹å¾ç»è¿‡è¿™ä¸€çº§ç½‘ç»œåå¾—åˆ°çš„é¢„æµ‹ä»…ä»…æ˜¯æœ‰ä¸€å®šå¯ä¿¡åº¦çš„ï¼Œå³ä½¿ä¹‹åä½¿ç”¨NMSå’Œè¾¹æ¡†å›å½’ç®—æ³•åˆ†åˆ«å¯¹å€™é€‰æ¡†è¿›è¡Œäº†ç­›é€‰å’Œæ ¡æ­£ï¼Œä½†æ˜¯æ­¤æ—¶çš„é¢„æµ‹ç»“æœä»ç„¶ä¸å‡†ç¡®ã€‚ç¬¬äºŒçº§ç½‘ç»œä¸ºR-Netï¼ˆRefine Networkï¼‰ï¼Œé¡¾åæ€ä¹‰è¿™ä¸€çº§ç½‘ç»œæ˜¯ç”¨æ¥æ”¹è¿›å‰ä¸€çº§ç½‘ç»œå¯¹äººè„¸ä½ç½®å’Œå…³é”®ç‚¹çš„å›å½’çš„ã€‚ç›¸è¾ƒäºP-Netï¼ŒR-Netå¤šäº†ä¸€å±‚å…¨è¿æ¥å±‚ï¼Œä¸”è¾“å…¥ç‰¹å¾ä¹Ÿæ˜¯å¢å¤§åˆ°äº†$24\\times24\\times3$ï¼Œå› æ­¤è¿™ä¸€å±‚ç½‘ç»œä¼šå¯¹è¾“å…¥æ•°æ®è¿›è¡Œæ›´åŠ ä»”ç»†ä¸¥æ ¼çš„ç­›é€‰ï¼Œå¤§éƒ¨åˆ†çš„é”™è¯¯é¢„æµ‹ä¼šè¢«è¯¥çº§ç½‘ç»œæ’é™¤æ‰ï¼Œä¹‹åä¼šæœ‰åŒæ ·çš„NMSå’Œè¾¹æ¡†å›å½’ç®—æ³•æ“ä½œã€‚ä¹‹æ‰€ä»¥R-Netå¯ä»¥å®ç°æ›´åŠ ç²¾ç¡®çš„é¢„æµ‹ï¼Œè¿™å¾—åŠ›äºR-Netæœ€åä¸€å±‚128å¤§å°çš„å…¨è¿æ¥å±‚ï¼Œè¿™ä¸ªå…¨è¿æ¥å±‚å¯ä»¥ä¿ç•™æ›´å¤šçš„å›¾åƒç‰¹å¾ï¼Œä»è€Œå®ç°æ›´é«˜çš„å‡†ç¡®æ€§ã€‚æœ€åä¸€çº§ç½‘ç»œè¢«ç§°ä¸ºO-Netï¼ˆOutput Networkï¼‰ï¼Œè¿™å±‚ç½‘ç»œæ˜¯ç”¨æ¥è¿›è¡Œæœ€åä¸€çº§ç­›é€‰å’Œæ ¡å‡†å¹¶è¾“å‡ºé¢„æµ‹å’Œå›å½’ç»“æœçš„ã€‚O-Netæ˜¯ä¸‰çº§ç½‘ç»œä¸­ç»“æ„æœ€ä¸ºå¤æ‚çš„ä¸€ä¸ªï¼Œè¾“å…¥ç‰¹å¾ä¸º$48\\times48\\times3$ï¼Œç›¸è¾ƒäºR-Netï¼ŒO-Netå¤šäº†ä¸€ä¸ªå·ç§¯å±‚ï¼Œå¹¶ä¸”å…¨è¿æ¥å±‚çš„ç»´åº¦æ‰©å¤§åˆ°äº†å…¶ä¸¤å€ï¼Œè¿™æ„å‘³ç€å®ƒä¼šä¿ç•™æ›´å¤šçš„å›¾åƒç‰¹å¾ç»†èŠ‚ï¼Œå®ç°æ›´åŠ ç²¾å‡†çš„äººè„¸åˆ¤åˆ«ã€äººè„¸åŒºåŸŸå›å½’ã€ä»¥åŠäººè„¸ç‰¹å¾å®šä½ã€‚æ€»ç»“æ¥è¯´ï¼ŒMTCNNåˆ©ç”¨ä»ç®€æ˜“åˆ°å¤æ‚çš„çº§è”å¼ç¥ç»ç½‘ç»œç»“æ„ï¼Œå…¼é¡¾äº†è®¡ç®—æ•ˆç‡å’Œå‡†ç¡®ç‡ï¼Œç¬¬ä¸€çº§ç½‘ç»œä½¿ç”¨äº†å…¨å·ç§¯ç½‘ç»œä»è€Œé¿å…äº†æ»‘åŠ¨çª—å£é‡‡æ ·æ“ä½œå¸¦æ¥çš„å·¨å¤§çš„è®¡ç®—æ—¶é—´æ¶ˆè€—ï¼Œå¤§å¹…åº¦æå‡äº†äººè„¸æ¡†é¢„æµ‹å’Œå…³é”®ç‚¹å›å½’çš„æ•ˆç‡ã€‚ç”±äºå…¶ç®€æ˜“çš„ç½‘ç»œç»“æ„å’Œç›¸å¯¹ä¸é”™çš„æ•ˆç‡å’Œå‡†ç¡®åº¦ï¼ŒMTCNNåœ¨å·¥ä¸šçº§åœºæ™¯ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚3. äººè„¸æ£€æµ‹åŠäººè„¸è¡¨æƒ…è¯†åˆ«ç½‘ç»œç»“æ„3.1 CNNç»“æ„å¦‚ä¸Šä¸€ç« èŠ‚æ‰€è¿°ï¼ŒMTCNNå…±åŒ…å«3ä¸ªå­ç½‘ç»œï¼ˆP-Netï¼ŒR-Netï¼Œå’ŒO-Netï¼‰ï¼Œ3ä¸ªå­ç½‘ç»œé€šè¿‡çº§è”çš„æ–¹å¼å®ç°å¯¹äººè„¸çš„æ£€æµ‹ã€è¾¹æ¡†å›å½’ã€ä»¥åŠäººè„¸å…³é”®ç‚¹çš„å›å½’ï¼Œ3ä¸ªç½‘ç»œçš„å…·ä½“å‚æ•°å’Œæœºæ„å¦‚å›¾7æ‰€ç¤ºã€‚(a) P-Ne(b) R-Net(c) O-Netå›¾7 MTCNN3ä¸ªå­ç½‘ç»œæ¶æ„ï¼ˆConvä»£è¡¨å·ç§¯å±‚ï¼Œæ­¥é•¿ä¸º1ï¼›Max-Poolä»£è¡¨æœ€å¤§æ± åŒ–å±‚ï¼Œæ­¥é•¿ä¸º2ï¼‰ç”¨äºäººè„¸è¡¨æƒ…åˆ†ç±»çš„ç½‘ç»œï¼Œæœ¬æ–‡ä½¿ç”¨çš„æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆçš„AlexNetï¼ŒåŸå§‹çš„AlexNetæ˜¯ç”¨äº1000ç±»åˆ†ç±»çš„ç»´åº¦åºå¤§çš„ç½‘ç»œï¼Œç”±äºè¡¨æƒ…å…±åˆ†ä¸º7ä¸ªä¸åŒç±»åˆ«ï¼Œæ‰€ä»¥ç®€åŒ–çš„ç½‘ç»œå³ä¸ºä¸€ä¸ªä¸ƒåˆ†ç±»ç½‘ç»œï¼Œç»“æ„å¦‚å›¾8æ‰€ç¤ºã€‚å›¾8 äººè„¸è¡¨æƒ…åˆ†ç±»ç½‘ç»œæ¶æ„ï¼ˆConvä»£è¡¨å·ç§¯å±‚ï¼Œæ­¥é•¿ä¸º1ï¼›Max-Poolä»£è¡¨æœ€å¤§æ± åŒ–å±‚ï¼Œæ­¥é•¿ä¸º2ï¼‰3.2 ç½‘ç»œè®­ç»ƒæˆ‘ä»¬åœ¨æ•´ä¸ªå®ç°ç®—æ³•ä¸­ï¼Œä¸€å…±ç”¨åˆ°äº†ä¸¤ä¸ªæ¨¡å‹ã€‚åœ¨MTCNNç½‘ç»œä¸­å…±æœ‰ä¸‰ä¸ªä»»åŠ¡ï¼Œåˆ†åˆ«æ˜¯äººè„¸äºŒå…ƒåˆ†ç±»ä»»åŠ¡ã€è¾¹æ¡†å›å½’ã€ä»¥åŠäººè„¸å…³é”®ç‚¹å®šä½ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ç”¨åˆ°ä¸‰ä¸ªä¸åŒçš„æŸå¤±å‡½æ•°å¾—åˆ°ç†Ÿç»ƒï¼Œåˆ†åˆ«æ˜¯ äººè„¸äºŒå…ƒåˆ†ç±»ä»»åŠ¡æŸå¤±å‡½æ•°ä½¿ç”¨çš„æ˜¯åˆ†ç±»ä»»åŠ¡å¸¸ç”¨çš„äº¤å‰ç†µå‡½æ•°ï¼Œå¯¹äºæ¯ä¸€ä¸ªæ ·æœ¬$x_i$ï¼Œ\\[L_i^{bin}=-(y_i^{bin}log(p_i)+(1-y_i^{bin})(1-log(p_i)))\\tag{3.1}\\]å…¶ä¸­$p_i$æ˜¯è¾“å…¥ç»è¿‡ç½‘ç»œåå¾—åˆ°æ˜¯äººè„¸çš„æ¦‚ç‡å€¼ï¼Œ$y_i^{bin}\\in{0,1}$è¡¨ç¤ºå·²æ ‡æ³¨çš„çœŸå®æ ‡ç­¾ã€‚ è¾¹æ¡†å›å½’ï¼ˆBounding Box Regressionï¼‰ä»»åŠ¡çš„æŸå¤±å‡½æ•°æ˜¯è®¡ç®—é¢„æµ‹è¾¹æ¡†ä¸æœ€è¿‘çœŸå®è¾¹æ¡†çš„æ¬§å¼è·ç¦»ï¼Œå¯¹äºæ¯ä¸ªæ ·æœ¬$x_i$ï¼Œ\\[L_i^{bbr}=\\left \\|\\hat y_i^{bbr}-y_i^{bbr}\\right \\|_2^2\\tag{3.2}\\]å…¶ä¸­$\\hat y_i^{bbr}$æ˜¯å›å½’é¢„æµ‹å¾—åˆ°çš„è¾¹æ¡†ä½ç½®ï¼Œ$ y_i^{bbr}$æ˜¯æ ‡æ³¨æ¡†çš„çœŸå®ä½ç½®ï¼Œ$ y_i^{bbr}\\in\\mathbb{R}^{4}$ã€‚ äººè„¸å…³é”®ç‚¹å›å½’ä»»åŠ¡ä¸è¾¹æ¡†å›å½’ä»»åŠ¡ç±»ä¼¼ï¼ŒåŒæ ·è®¡ç®—é¢„æµ‹ç‚¹ä¸çœŸå®æ ‡æ³¨ç‚¹çš„æ¬§å¼è·ç¦»ï¼Œå¯¹äºæ¯ä¸ªæ ·æœ¬$x_i$ï¼Œ\\[L_i^{lm}=\\left \\|\\hat y_i^{lm}-y_i^{lm}\\right \\|_2^2\\tag{3.3}\\]å…¶ä¸­$\\hat y_i^{lm}$æ˜¯é€šè¿‡ç½‘ç»œå¾—åˆ°çš„é¢„æµ‹å€¼ï¼Œ$y_i^{lm}$æ˜¯çœŸå®çš„æ ‡æ³¨å€¼ï¼Œ$ y_i^{lm}\\in\\mathbb{R}^{10}$ã€‚ç”±äºæˆ‘ä»¬åœ¨MTCNNçš„ä¸‰ä¸ªä¸åŒCNNä¸­éœ€è¦å®Œæˆä¸åŒçš„ä»»åŠ¡ï¼Œæ‰€ä»¥åœ¨è®­ç»ƒæ—¶ï¼Œè¾“å…¥çš„è®­ç»ƒæ•°æ®ä¹Ÿæ˜¯ä¸åŒçš„ï¼Œå› æ­¤åœ¨è®­ç»ƒä¸åŒçš„CNNæ—¶è®¡ç®—çš„æŸå¤±å‡½æ•°ä¹Ÿéœ€è¦åŒºåˆ«å¯¹å¾…ã€‚æ€»çš„è®­ç»ƒæŸå¤±å‡½æ•°å¯ä»¥è¡¨ç¤ºä¸º\\[\\sum_{i=1}^N \\sum_{j \\in \\left\\{bin,bbr,lm \\right\\} }{\\alpha_j \\beta_i^j L_i^j}\\tag{3.4}\\]å…¶ä¸­$N$è¡¨ç¤ºè®­ç»ƒæ ·æœ¬æ€»æ•°ï¼Œ$\\alpha_j$è¡¨ç¤ºä»»åŠ¡æƒé‡ï¼Œåœ¨P-Netå’ŒR-Netä¸­ï¼Œ$\\alpha_{bin}=1,\\alpha_{bbr}=0.5,\\alpha_{lm}=0.5$ï¼Œåœ¨O-Netä¸­$\\alpha_{bin}=1,\\alpha_{bbr}=0.5,\\alpha_{lm}=1$ï¼Œ$\\beta_i^j\\in{0,1}$è¡¨ç¤ºæ ·æœ¬ç±»å‹æƒé‡ã€‚å¯¹äºäººè„¸è¡¨æƒ…è¯†åˆ«ç½‘ç»œï¼Œå®ƒæ˜¯ä¸€ä¸ªå¤šåˆ†ç±»ç½‘ç»œï¼Œæ‰€ä»¥æŸå¤±å‡½æ•°ä½¿ç”¨çš„åŒæ ·æ˜¯äº¤å‰ç†µå‡½æ•°ï¼Œ\\[L_i^{facialExp}=-(y_i^{facialExp}log(p_i)+(1-y_i^{facialExp})(1-log(p_i)))\\tag{3.5}\\]å…¶ä¸­$p_i$æ˜¯è¾“å…¥ç»è¿‡ç½‘ç»œåå¾—åˆ°é¢„æµ‹è¡¨æƒ…çš„æ¦‚ç‡å€¼ï¼Œ$y_i^{facialExp}\\in{0,1}$è¡¨ç¤ºå·²æ ‡æ³¨çš„çœŸå®æ ‡ç­¾ã€‚3.3 å°ç»“æœ¬ç« èŠ‚ä¸»è¦ä»‹ç»äº†å®ç°æœ¬è¯¾é¢˜å…·ä½“ä½¿ç”¨çš„æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹åŠå…¶è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡ä¸€äº›ç°æœ‰çš„æ·±åº¦å­¦ä¹ æ¡†æ¶å®Œæˆå¯¹ä¸Šè¿°ç†è®ºæ¨¡å‹çš„æ­å»ºï¼Œä¸€ä¸ªç®€å•çš„äººè„¸æ£€æµ‹åŠäººè„¸è¡¨æƒ…è¯†åˆ«çš„æ·±åº¦å­¦ä¹ æ¨¡å‹å°±å¯ä»¥è¢«æˆåŠŸå»ºç«‹äº†ã€‚åœ¨ä¸‹ä¸€ç« ï¼Œå°†å…·ä½“ä»‹ç»å®ç°æœ¬è¯¾é¢˜æ‰€ç”¨çš„ä¸€äº›å®éªŒæ¡ä»¶å’Œæ–¹æ³•ã€‚4. å®éªŒä¸æµ‹è¯•4.1 æ•°æ®é›†å¦‚ä¸Šæ–‡æåˆ°çš„ï¼Œå®ç°äººè„¸è¡¨æƒ…è¯†åˆ«å…±éœ€è®­ç»ƒä¸¤ä¸ªç½‘ç»œæ¨¡å‹ï¼ŒMTCNNæ¨¡å‹ç”¨æ¥åšäººè„¸æ£€æµ‹å’Œäººè„¸å…³é”®ç‚¹å›å½’ï¼Œå¦ä¸€ä¸ªAlexNetæ¨¡å‹ç”¨æ¥åšäººè„¸è¡¨æƒ…åˆ†ç±»é¢„æµ‹ã€‚å¯¹äºè®­ç»ƒMTCNNï¼Œéœ€è¦ä½¿ç”¨ä¸¤ä¸ªæ•°æ®é›†æ¥è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼ŒWider Faceæ•°æ®é›†ç”¨æ¥è®­ç»ƒäººè„¸äºŒåˆ†ç±»ï¼ŒCelebAæ•°æ®é›†ç”¨æ¥è®­ç»ƒäººè„¸è¾¹æ¡†å›å½’ï¼ˆBounding-Box Regressionï¼‰ä»¥åŠäººè„¸å…³é”®ç‚¹å›å½’ã€‚å¯¹äºè®­ç»ƒAlexNetï¼Œç”¨äºè®­ç»ƒè¡¨æƒ…åˆ†ç±»çš„æ•°æ®é›†æ˜¯Kaggleè®ºå›ä¸Šå‘å¸ƒçš„fer2013äººè„¸è¡¨æƒ…æ•°æ®é›†ã€‚WIDER FACEæ•°æ®é›†æ˜¯ä¸€ä¸ªäººè„¸é¢éƒ¨æ£€æµ‹åŸºå‡†æ•°æ®é›†ï¼Œå…±åŒ…å«32,203å¼ å›¾åƒï¼Œå…¶ä¸­å…±æ ‡è®°äº†393,703å¼ äººè„¸ï¼Œè¿™äº›äººè„¸åœ¨å°ºåº¦ï¼Œå§¿æ€ï¼Œé®æŒ¡æ–¹é¢éƒ½æœ‰å¾ˆå¤§çš„å˜åŒ–èŒƒå›´ã€‚Wider Faceæ•°æ®é›†çš„åˆ¶ä½œè€…æ¥è‡ªäºé¦™æ¸¯ä¸­æ–‡å¤§å­¦ï¼Œå…¶å›¾åƒä¸»è¦æºè‡ªäºå…¬å¼€æ•°æ®é›†WIDERã€‚CelebAæ•°æ®é›†æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡äººè„¸å±æ€§æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«è¶…è¿‡äºŒåä¸‡ä¸ªåäººå›¾åƒï¼Œæ¯ä¸ªå›¾åƒéƒ½æœ‰40ä¸ªå±æ€§æ³¨é‡Šï¼Œå…¶ä¸­çš„äººè„¸äº”ç‚¹æ ‡è®°å’Œäººè„¸è¾¹æ¡†æ ‡è®°éœ€è¦è¢«åˆ©ç”¨åˆ°ã€‚æ­¤æ•°æ®é›†ä¸­çš„å›¾åƒæ¶µç›–äº†è¾ƒå¤§çš„å§¿åŠ¿å˜åŒ–å’Œæ‚ä¹±èƒŒæ™¯ã€‚CelebAå…·æœ‰æ ·å¼ä¼—å¤šï¼Œæ•°é‡ä¼—å¤šä¸”æ³¨é‡Šä¸°å¯Œçš„ç‰¹ç‚¹ã€‚CelebAæ•°æ®é›†çš„åˆ¶ä½œè€…ä¹Ÿæ¥è‡ªäºé¦™æ¸¯ä¸­æ–‡å¤§å­¦ã€‚Fer2013æ•°æ®é›†å‘å¸ƒäºKaggleå¹³å°ï¼Œæ˜¯ç›®å‰ç›¸å¯¹è¾ƒå¤§çš„äººè„¸è¡¨æƒ…è¯†åˆ«å…¬å¼€æ•°æ®é›†ã€‚å…±åŒ…å«35886å¼ äººè„¸è¡¨æƒ…å›¾ç‰‡ï¼Œæµ‹è¯•é›†å›¾åƒæœ‰28708å¼ ï¼Œå…¬å…±éªŒè¯é›†å›¾åƒå’Œç§æœ‰éªŒè¯é›†å›¾åƒå„3589å¼ ï¼Œæ‰€æœ‰å›¾ç‰‡éƒ½æ˜¯ç”±å›ºå®šå°ºå¯¸ä¸º48Ã—48çš„ç°åº¦å›¾åƒï¼Œå…±åŒ…å«7ç§è¡¨æƒ…ï¼ˆä¸ä¸Šæ–‡æ‰€è¿°ç›¸ç¬¦ï¼ŒåŒ…å«äº†ä¸­æ€§ï¼‰ï¼Œå¹¶ä¸”æ‰€å±ç±»åˆ«å·²è¢«æ ‡æ³¨ã€‚å®˜æ–¹å‘å¸ƒçš„æ•°æ®é›†çš„æ ¼å¼ä¸ºcsvæ ¼å¼ï¼Œæ‰€ä»¥éœ€å…ˆå¯¹å…¶è¿›è¡Œæ ¼å¼è½¬æ¢(æ­¤å®éªŒä¸­ä½¿ç”¨çš„pythonä¸­pandasåº“å¯¹csvæ–‡ä»¶è¿›è¡Œçš„å¤„ç†)ï¼Œè½¬æ¢æˆä¸ºå¯ç”¨çš„å›¾åƒæ ¼å¼ã€‚4.2 ç½‘ç»œæ­å»ºåŠè®­ç»ƒ4.2.1 å®éªŒç¯å¢ƒ ç³»ç»Ÿï¼šUbuntu 16.04LTSï¼› GPUï¼šTITAN Xï¼› è¯­è¨€ç¯å¢ƒï¼šPython2.7ï¼› æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼šCaffeï¼› æ‰€ç”¨ä¾èµ–åº“åŒ…æ‹¬ï¼špycaffeã€opencvã€numpyã€pandasã€cPickleç­‰ã€‚4.2.2 ç½‘ç»œæ­å»ºåŠè®­ç»ƒåˆ©ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶Caffeå®Œæˆå¯¹æ‰€éœ€ç½‘ç»œçš„æ­å»ºå’Œè®­ç»ƒï¼Œæ­å»ºä¸Šè¿°ç½‘ç»œï¼Œæ­å»ºåcaffeæ·±åº¦å­¦ä¹ ç½‘ç»œå…·ä½“å‚æ•°å¯å‚è§é™„å½•ï¼Œä½¿ç”¨Caffeçš„åŸå§‹æ˜¯å› ä¸ºå…¶å¯¹äºç®€å•ç½‘ç»œçš„æ­å»ºå¾ˆæ–¹ä¾¿ï¼Œä¸”è¿ç®—æ•ˆç‡ä¹Ÿå¾ˆé«˜ã€‚å¯¹äºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œåœ¨MTCNNç½‘ç»œä¸­ï¼Œäººè„¸è¾¹æ¡†å›å½’ä»»åŠ¡å¯¹äºè®­ç»ƒé›†å’ŒéªŒè¯é›†åˆ†åˆ«å¯ä»¥è¾¾åˆ°92.3%ä»¥åŠ85%çš„å›å½’ç‡ï¼Œå¯¹äºäººè„¸å…³é”®ç‚¹å®šä½è¾¾åˆ°91.3%å’Œ79%çš„å‡†ç¡®ç‡ã€‚ç„¶è€Œå¯¹äºäººè„¸è¡¨æƒ…åˆ†ç±»æ¨¡å—ï¼Œå¯¹äºéªŒè¯é›†çš„å‡†ç¡®ç‡ä»…ä»…è¾¾åˆ°äº†63.5%è¯†åˆ«ç‡ã€‚å¯¹è¡¨æƒ…è¯†åˆ«çš„è¯†åˆ«ç‡è¿‡ä½çš„åŸå› è¿›è¡Œåˆ†æï¼Œä½œè€…æœ¬äººè®¤ä¸ºæ˜¯å› ä¸ºäººç±»è¡¨æƒ…è¿‡äºå¤æ‚ï¼Œå¯¹äºä¸€äº›å¾®å¾®çš„è‚Œè‚‰å½¢å˜ï¼Œè®¡ç®—æœºå¾ˆéš¾å¯¹å…¶å¯Ÿè§‰ï¼Œäººç±»è‡ªå·±æœ‰æ—¶ä¹Ÿå¾ˆéš¾å¯¹è¡¨æƒ…ä½œå‡ºæ­£ç¡®åˆ¤æ–­ã€‚åœ¨å‘ç½‘ç»œè¾“å…¥å›¾åƒæ—¶ï¼ˆæ­¤å¤„ä¸å•ç‹¬è®¨è®ºè§†é¢‘ä»»åŠ¡ï¼Œè§†é¢‘åªæ˜¯å°†å¤šå¼ å›¾ç‰‡ç»„åˆè€Œæˆçš„å›¾åƒæµï¼‰ï¼Œéœ€è¦å¯¹è¾“å…¥å›¾åƒè¿›è¡Œé¢„å¤„ç†ï¼ŒåŒ…æ‹¬å›¾åƒå½’ä¸€åŒ–å¤„ç†ä»¥ä¿è¯æ¨¡å‹æ”¶æ•›çš„æ›´å¿«ã€å›¾åƒå°ºå¯¸å¤„ç†ï¼ˆæœ€é•¿è¾¹é™åˆ¶åœ¨1000åƒç´ ä»¥å†…ï¼‰ä»¥ç¡®ä¿è®¡ç®—æ•ˆç‡ã€ã€‚4.3 ç®—æ³•æµ‹è¯•å¦‚å›¾8å’Œå›¾9æ‰€ç¤ºï¼Œåˆ†åˆ«å¯¹åº”è¯¥ç½‘ç»œæ¨¡å‹åˆ†åˆ«å¯¹é™æ€å›¾åƒå’Œè§†é¢‘æµä»»åŠ¡çš„æµ‹è¯•ç»“æœã€‚å¯¹äºå…·æœ‰ç›¸å¯¹å®Œæ•´äººè„¸çš„å›¾åƒï¼ˆå¦‚å›¾8ä¸Šé¢ä¸¤ä¸ªæµ‹è¯•ç»“æœï¼‰ï¼Œè¯¥æ¨¡å‹å¯ä»¥å¾ˆå¥½çš„å¯¹å…¶è¿›è¡Œè¯†åˆ«ï¼Œä½†æ˜¯å¯¹äºå›¾åƒä¸­åŒ…å«ä¸å®Œæ•´äººè„¸ã€è‚¤è‰²è¾ƒæ·±ã€å§¿åŠ¿è¿‡åçš„ï¼Œè¯¥æ¨¡å‹è¿˜æ˜¯ä¸èƒ½å¾ˆå¥½çš„å¯¹å…¶è¿›è¡Œæ£€æµ‹ï¼Œè¿™ä¹Ÿåº”è¯äº†ç¬¬ä¸€ç« æ‰€è¯´çš„åœ¨ä¸å—æ§çš„æ¡ä»¶ä¸‹åœ¨äººè„¸æ£€æµ‹ä»»åŠ¡ä¸­çš„éš¾ç‚¹æ‰€åœ¨ã€‚åœ¨GPUç¯å¢ƒä¸‹ï¼Œå¯¹äºè§†é¢‘æ–‡ä»¶ï¼ˆå¦‚å›¾9ä¸º240p,30fpsè§†é¢‘æ–‡ä»¶ï¼‰çš„æµ‹è¯•ï¼Œå¯ä»¥è¾¾åˆ°15fpså·¦å³ï¼Œå¯ä»¥çœ‹å‡ºæœ¬ç®—æ³•è¿˜ä¸å®Œå–„ï¼Œè¿˜ä¸èƒ½è¾¾åˆ°å®æ—¶çš„æ•ˆæœï¼Œåœ¨MTCNNåŸä½œè®ºæ–‡ä¸­ï¼Œä½œè€…çš„äººè„¸æ£€æµ‹å’Œå…³é”®ç‚¹å›å½’æ¨¡å‹å¯ä»¥è¾¾åˆ°99fpsã€‚å›¾9 å¯¹äºå›¾åƒçš„æµ‹è¯•ç»“æœå›¾10 å¯¹äºè§†é¢‘çš„æµ‹è¯•ç»“æœ4.4 å°ç»“æœ¬ç« èŠ‚ä»‹ç»äº†å¯¹äºæœ¬æ–‡æ‰€ç”¨äººè„¸æ£€æµ‹ä»¥åŠäººè„¸è¡¨æƒ…è¯†åˆ«æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹çš„å®ç°å’Œæµ‹è¯•ç»“æœï¼Œä¸éš¾çœ‹å‡ºï¼Œå¯¹äºäººè„¸è¡¨æƒ…è¯†åˆ«ç½‘ç»œï¼Œè™½ç„¶åœ¨è®­ç»ƒæ—¶å¯¹äºéªŒè¯é›†æµ‹è¯•çš„å‡†ç¡®ç‡ä»…ä»…è¾¾åˆ°63.5%ï¼Œä½†æ˜¯åœ¨å®é™…æµ‹è¯•æ—¶ï¼Œå¯¹äºè¾ƒæ˜æ˜¾äººè„¸è¡¨æƒ…æµ‹è¯•çš„å‡†ç¡®ç‡è¿˜æ˜¯ä¸é”™çš„ã€‚æœ¬æ¨¡å‹ç›®å‰è¾ƒå¤§çš„é—®é¢˜ä»åœ¨è¿ç®—æ•ˆç‡ä¸Šï¼Œä»ä¸èƒ½æ»¡è¶³å·¥ä¸šçº§å®æ—¶æ€§çš„è¦æ±‚ã€‚å‚è€ƒæ–‡çŒ® Li H , Lin Z , Shen X , et al. A convolutional neural network cascade for face detection[C]// 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2015.Â &amp;#8617;Â &amp;#8617;2 Viola P A , Jones M J . Rapid Object Detection using a Boosted Cascade of Simple Features[C]// IEEE Computer Society Conference on Computer Vision &amp;amp; Pattern Recognition. IEEE, 2001.Â &amp;#8617; C. Zhang and Z. Zhang. A survey of recent advances in face detection. Technical Report MSR-TR-2010-66, 2010.Â &amp;#8617; å¾ç³ç³, å¼ æ ‘ç¾, èµµä¿Šè‰. åŸºäºå›¾åƒçš„é¢éƒ¨è¡¨æƒ…è¯†åˆ«æ–¹æ³•ç»¼è¿°[J]. è®¡ç®—æœºåº”ç”¨, 2017(12):171-178+208.Â &amp;#8617; D, C, P,ç­‰. The Expression of Emotions in Man and Animals[J]. The American Journal of Psychology, 1981.ç”¨,2017,37(12):3509-3516+3546.Â &amp;#8617; Ekman P , Friesen W V , Ellsworth P C . Emotion in the Human Face[M]. Cambridge University Press ;, 1982.Â &amp;#8617; Y. l. Tian, T. Kanade, and J. F. Cohn, â€œEvaluation of Gabor-Wavelet-Based Facial Action Unit Recognition in Image Sequences of Increasing Complexity,â€ in Proceedings of the Fifth IEEE International Conference on Automatic Face and Gesture Recognition, pp. 229-234, 2002.Â &amp;#8617; Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105.Â &amp;#8617; Lecun Y , Bengio Y , Hinton G . Deep learning[J]. nature, 2015, 521(7553):436.Â &amp;#8617; Y-Lan Boureau, Jean Ponce, Yann LeCun. A Theoretical Analysis of Feature Pooling in Visual Recognition[C]// International Conference on Machine Learning. DBLP, 2010.Â &amp;#8617; Zhang K , Zhang Z , Li Z , et al. Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks[J]. IEEE Signal Processing Letters, 2016, 23(10):1499-1503.Â &amp;#8617; Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object detection and semantic segmentation. In: CVPR (2014)Â &amp;#8617; Shelhamer E , Long J , Darrell T . Fully Convolutional Networks for Semantic Segmentation[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39(4):640-651.Â &amp;#8617; He K , Zhang X , Ren S , et al. Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition[J]. IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2014, 37(9):1904-16.Â &amp;#8617; " } ]
