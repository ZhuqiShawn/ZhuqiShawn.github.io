[ { "title": "Animal Detection Networks", "url": "/posts/Animal-Detection-Networks/", "categories": "Deep Learning", "tags": "deep learning, cnn, object detection", "date": "2021-10-02 06:00:00 +0200", "snippet": " Abstract: The project aims to provide a demo animal detection model with the long-term goal of developing a phone application. Two different state-of-art algorithms are tested out, one is YOLOv5 and another is Faster R-CNN. These two represents two different families of detection methods, one is one-stage while another is two-stage. YOLO is an end-to-end target detection framework that transforms target detection into regression problems while R-CNN is a target detection framework combining region proposal and CNN classification. We compare the two algorithms in accuracy and speed and found that YOLOv5 performs better in both aspects. At the end of this report, there will be some discussion about the results and methods to further expand the model in the future.IntroductionAs one of the most potent public education organisations, Universeum is a public arena for lifelong learning where children and adults explore the world through science and technology. This project is for building an animal detection model based on deep learning methods. Doing so can help Universeum further develop related mobile applications in the future, thereby making it easier to convey knowledge and information to tourists. In this project, we collected and annotated our own dataset, which contains 19 classes of animals. The method of data augmentation is performed to generalize new images and introduce them to the networks. Then, the transfer learning method would be applied and feed the dataset to two state-of-art pre-trained networks, YOLOv5 and Faster R-CNN.Related WorkObject detection, as of one the most fundamental and challenging problems in computer vision, has received significant attention in recent years. It is an important computer vision task that detects visual objects of certain classes in digital images. In the deep learning era, object detection can be categorized into two groups1: “two-stage detection” and “one-stage detection”, where the former frames the detection as a “coarse-to-fine” process while the latter frames it as to “complete in one step”. In this project, two different detectors from each category are tried out, YOLOv5 and Faster R-CNN.You Only Look Once (YOLO)YOLO was primely proposed by R.Joseph et al. in 2015, which is the very first one-stage detector2. Compared to the approach taken by object detection algorithms before YOLO, YOLO proposes using an end-to-end neural network that makes predictions of bounding boxes and class probabilities all at once. YOLO provided a super fast and accurate object detection algorithm that revolutionized computer vision research related to object detection. It applies a single neural network to the full image, which divides the image into regions and predicts bounding boxes and probabilities for each region simultaneously. R. Joseph has made a series of improvements on the first version and proposed version 2 and 3 editions34 but then he left the community last year, so YOLOv4 and later versions are not his official work. However, the legacy continues through new researchers. In this project, we decided to use the latest model of the YOLO family, version 5, which is an open-source project that consists of a family of object detection models and detection methods based on the YOLO model pre-trained on the COCO dataset.How YOLO worksThe YOLO algorithm works by dividing the image into $N$ grids, each having an equal dimensional region of $S \\times S$. Each of these $N$ grids is responsible for the detection and localization of the object it contains. Correspondingly, these grids predict bounding boxes relative to their cell coordinates, along with the object label and probability of the object being present in the cell. This process greatly lowers the computation as both detection and recognition are handled by cells from the image. However, it brings forth a lot of duplicate predictions due to multiple cells predicting the same object with different bounding box predictions. YOLO makes use of Non-Maximal Suppression (NMS) to deal with this issue. In NMS, YOLO suppresses all bounding boxes that have lower probability scores. YOLO achieves this by first looking at the probability scores associated with each decision and taking the largest one. Following this, it suppresses the bounding boxes having the largest IOU with the current high probability bounding box. This step is repeated till the final bounding boxes are obtained.There are two parts to the loss function, bounding box regression loss and classification loss. In terms of b-box regression loss, IOU loss was used in the past. The latest models use deformations based on this loss, such as CIOU Loss and DIOU Loss.YOLO ArchitectureYOLO is designed to create features from input images and feed them through a prediction system to draw boxes around objects and predict their classes. Since YOLOv3, the YOLO network consists of three main pieces. Backbone: A convolutional neural network that aggregates and forms image features at different granularities. Neck: A series of layers to mix and combine image features to pass them forward to prediction. Head: Consumes features from the neck and takes box and class prediction steps.There are abundant approaches that can be used to combine different architectures at each component listed above. The contributions of YOLOv4 and YOLOv5 first integrate breakthroughs in other fields of computer vision and incorporate them into the original YOLO structure to improve performance.Faster R-CNNFaster R-CNN is a member of R-CNN family. R-CNN is published by R. Girsh in 20135. Then some improvements have been made by the author himself, which is Fast R-CNN, published in early 20156. And finally is the model published in the same year, Faster R-CNN7. Figure 1 is the whole structure of Faster R-CNN and the theory will be elaborated below.Figure 1. Structure of Faster R-CNN7Covolutional LayersAs a CNN network target detection method, Faster R-CNN uses basic Covolutional layers like VGG16 or ResNet50 to extract image feature maps. These layers is the cov layers in Figure 1, which is also called base network because the whole structure is built based on it.The obvious advantage of ResNet over VGG is that it is bigger, hence it has more capacity to actually learn what is needed8.The feature maps are shared for the subsequent RPN layer and fully connected layer. The VGG16 and ResNet50 are always pretrained to reduce the difficulty in training.Region Proposal NetworksClassical detection methods are very time-consuming to generate detection frames. For example, sliding windows, which we have learned in class, really needs to iterate so many times; or R-CNN uses SS (Selective Search) method to generate anchor box5. Faster RCNN abandons the traditional sliding window and SS method and directly uses Region Proposal Networks (RPN) to generate the anchor box. This is also a huge advantage of Faster R-CNN, which can greatly improve the generation speed of the detection frame.Before continue talking about RPN, we want to discuss the anchor box first, which is inherited from R-CNN. Traverse the feature maps calculated by Conv layers, and equip each point with these 9 prior anchors given as hyper parameters as the initial detection frame.The RPN does two different type of predictions: the binary classification and the bounding box regression adjustment. The loss of fist prediction is cross entropy loss and the second is L1 loss:\\[Loss = \\sum^{N}_{i}{|t_*^i - W_*^T\\cdot \\phi(A^i)|} + \\lambda ||W_*||.\\]Here $t_*^i$ is the target box and $W_*^T\\cdot \\phi$ is the linear map of anchor box.It should be clear that only when the distance between two boxes is short enough, can the map be viewed as linear.There are some many anchors, so we introduced Non-Maximum Suppression (NMS). NMS discards those proposals that have a score larger than some predefined threshold with a proposal that has a higher score.RoI PoolingThe Region of Interest (RoI) Pooling layer is responsible for collecting proposals, calculating proposal feature maps, and sending them to the subsequent network.For traditional CNN (such as AlexNet and VGG), when the network is trained, the input image size must be a fixed value. Faster R-CNN tries to solve this problem by reusing the existing conv network. This is done by extracting fixed-sized feature maps for each proposal using region of interest pooling. Fixed size feature maps are needed for the R-CNN in order to classify them into a fixed number of classes.ClassificationThe probability vector is obtained by fully connected layer and softmax. Bounding box regression is used to obtain each proposal’s position offset in order to return to a more accurate target detection frame.MethodologyIn this section, we will see how we conduct our project in detail.DatasetAt the very soul of training deep learning networks is the training dataset. The first step of any object detection model is collecting images data and performing annotation. For this project, we went to the museum in person and manually collected 764 images of real scenes. Training datasets are images collected as samples and annotated for training deep neural networks. For object detection, there are many formats for preparing and annotating your dataset for training. The most popular formats for annotating an object detection datasets are Pascal VOC, Micosoft COCO, and YOLO format. Since we intend to use transfer learning, YOLO and Faster R-CNN require different formats of annotations, so two different annotation formats have been prepared, VOC and YOLO formats.Figure 2. Class balance of the collected and annotated datasetThere are in total 19 classes of animals appeare in our collected datset. However, we can see from the Figure 2 that our data set is unbalanced. Some animals have been labeled more than 300 while some have fewer than 50. This factor needs to be considered in the analysis of the final result. All images are then resized into 416 by 416 and the entired dataset is randomly divided into training, validation, and test set according to the ratio of 7:2:1.Image augmentation is a method to increase the generalizability of your model’s performance by increasing the diversity of learning examples for your model. Specific methods include rotation, flipping, and brightness adjustment. We performed this process on the training set and expanded it by a factor of two.Transfer LearningSince we only have a small dataset, in order to obtain a good model in time, the transfer learning method is adopted. We separately train on pre-trained YOLOv5 model and Faster RCNN model.YOLOYOLOv5 is an open-source project that consists of a family of object detection models. Since the model will be potentially deployed on mobile phones, smaller models are preferred. Therefore, the smallest YOLOv5 architecture, YOLOv5s, is then selected to be trained on. The pre-trained models are available in their github page. 100 epochs of training are preformed.Faster RCNNPretrained models of Faster R-CNN are downloaded online in this page and here. Compared to the original paper, we set smaller prior anchor box to detect small objects. We use two base models, VGG16 and ResNet50 to fulfill the task. Training epoch is 50, of which 30 is trained while base model is frozen while 20 unfrozen. Learning rate is $10^{-4}$ for frozen part and $10^{-5}$ for unfrozen with decaying.ResultsWe calculate the mAP of two algorithms and results are shown in Table 1. Both of these detectors perform well for detecting large animals, but for small animals that look very similar (especially some fish), both models perform relatively poorly but each has its own merits. Note that this result does not truly reflect the true ability of these two detectors, because the dataset is very limited (the test set has only 76 images). However, this result can also be used as a reference to test the performance of these two models on this specific task. Method YOLOv5 Faster R-CNN all 0.82 0.74 Arapaima 0.64 0.81 Orange Revabborre 0.97 0.70 Stingray 0.96 0.73 Toco Toucan 0.76 0.90 Emerald Toucanet 0.99 1.00 Pied Tamarin 0.46 0.40 Sunbittern 1.00 1.00 Goldbelly Damsel 0.78 0.31 Lemon Damsel 0.88 0.66 Red Ibis 0.88 1.00 Thornback Ray 0.64 0.95 Picked dogfish 0.91 0.71 Clownfish 0.89 0.64 Common dragonet 0.92 0.74 Starfish 0.70 0.66 Indian Surgeonfish 0.55 0.79 Yellowfin surgeonfish 1.00 1.00 Silver Moony 0.76 0.12 Blue Fish 0.96 0.81 Table 1.ResultsBesides, we measure the average fps of two algorithms when predicting videos in Table 2. The whole experiment is implemented on a GeForce RTX 2060 SUPER. Method YOLOv5 Faster R-CNN fps 200.00 8.37 Table 2. mAP of two algorithmsConclusionsIn the above table, we just show the best result of Faster R-CNN and now the comparison between VGG16 and ResNet50 is shown in Table 3.   Faster R-CNN VGG Faster R-CNN ResNet mAP 0.74 0.72 fps 8.37 3.93 Table 3. mAP and fps of Faster R-CNN with different base networkssThe mAP of VGG is slightly better than ResNet while fps is two times better surprisingly. We think it is because our task is relatively simple. The main difference between VGG and ResNet is that ResNet has bigger network, which means it will definitely run slower, but for a simple task, the advantage of extracting more information is not shown.According to common sense, one-stage algorithm is faster while two-stage algorithm is more accurate. There is no surprise to see YOLOv5 is greatly quicker while a bit more correct since YOLO has fixed its disadvantage in detecting small objects since YOLOv3 and nowadays YOLOv5 also draw lessons from R-CNN. But it is still strange to observe such a bad performance in small objects (Sliver Moony) with Faster R-CNN. So we check it thoroughly and find the problem, that is, the only two pictures of Silver Moony in the test set is so blurry that even a human can not recognize it easily. We tried another picture borrowed from another group and find it work fine. Since it is quite time-consuming to shuffle the train, validation as well as test set and retrain them, we just clarify the fact here.Since the long-term goal of this project is to develop an animal recognition application for Universeum, we also need to consider the scalability of the model. In theory, this should be very easy. Take YOLOv5 as an example, just change the output dimension of the output layer to the required number, and perform a few rounds of training on the expanded dataset (not verified), and you should be able to get a well performed expanded model. Of course, in order to ensure that the model performs good, the quality and data content of the dataset should be better.Reference Z. Zou, Z. Shi, Y. Guo, and J. Ye, “Object detection in 20 years: A survey,” arXiv preprint arXiv:1905.05055, 2019. &amp;#8617; J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once: Unified, real-time object detection,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 779–788. &amp;#8617; J. Redmon and A. Farhadi, “YOLO9000: better, faster, stronger,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 7263–7271. &amp;#8617; J. Redmon and A. Farhadi, “Yolov3: An incremental improvement,” arXiv preprint arXiv:1804.02767, 2018. &amp;#8617; R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies for accurate object detection and semantic segmentation,” arXiv e-prints, p. earXiv:1311.2524, Nov. 2013. &amp;#8617; &amp;#8617;2 R. Girshick, “Fast R-CNN,” arXiv e-prints, p. earXiv:1504.08083, Apr. 2015. &amp;#8617; S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,” arXiv e-prints, p. earXiv:1506.01497, Jun. 2015. &amp;#8617; &amp;#8617;2 K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image Recognition,” arXiv e-prints, p. earXiv:1512.03385, Dec. 2015. &amp;#8617; " }, { "title": "Count-min Sketch 算法", "url": "/posts/Count-Min-Sketch-%E7%AE%97%E6%B3%95/", "categories": "Algorithm", "tags": "algorithm, big data", "date": "2021-05-19 06:00:00 +0200", "snippet": "简介Count-min Sketch算法是一个可以用来计数的算法，在数据大小非常大时，一种高效的计数算法，通过牺牲准确性提高的效率。 是一个概率数据机制 算法效率高 提供计数上限其中，重要参数包括 Hash 哈希函数数量： k 计数表格列的数量： m 内存中用空间： $k \\times m \\times \\text{size of counter}$举个例子🌰我们规定一个 $m = 5$, $k = 3$ 的Count-min Sketch，用来计数，其中所有hash函数如下\\[k\\left\\{ \\begin{array}{lr} h_1(x)=\\text{ASCII}(x)\\\\ h_2(x) = 2 + \\text{ASCII}(x)\\\\ h_3(x) = 4 \\cdot \\text{ASCII}(x) \\end{array} \\right.\\]注意，所有hash函数的结果需 $\\mod m$下面开始填表，首先初始状态为01234$h_1$00000$h_2$00000$h_3$00000首先，向里面添加字母B，其ASCII码为66，求hash函数的结果为\\[\\left\\{ \\begin{array}{lr} h_1(x) = 1\\\\ h_2(x) = 3\\\\ h_3(x) = 4 \\end{array} \\right.\\]因此，表格变为01234$h_1$01000$h_2$00010$h_3$00001接下来，我们查询字母A，其ASCII码为65，求hash函数的结果为\\[\\left\\{ \\begin{array}{lr} h_1(x) = 0\\\\ h_2(x) = 2\\\\ h_3(x) = 0 \\end{array} \\right.\\]用这个结果去读表，发现其对应位置均为0，因此字母A最多出现0次，这个值是准确的。然后，我们在查询字母G，其ASCII码为71，求hash函数的结果为\\[\\left\\{ \\begin{array}{lr} h_1(x) = 1\\\\ h_2(x) = 3\\\\ h_3(x) = 4 \\end{array} \\right.\\]用这个结果去读表，发现其对应位置均为1，因此字母G最多出现1次；出错了！我们从未向里面添加过字母G，这就是一次collision。Count-min Sketch的确会有这种问题，因为这个模型是从Bloom Filter衍生过来的。所以说Count-min Sketch是一个概率模型，返回的结果是一个上限值（upper-bound）。设计最优 Count-min Sketch有了上面的问题，我们自然而然就会想到如何设计一个最优的Count-min Sketch模型。首先，规定一些参数： 数据流大小： $n$ 元素 x 的真实计数值： $c_x$ 元素 x 的估计计数值： $\\hat{c}_x$ 我们可以自己选择的参数： $k$ （hash函数数量）和 $m$ （表格列的数量）注：如果我们的模型 $k = 1$, $m = n$ ，另唯一的 $h_1(x) = x$，那么我们可以得到准确的计数结果。现在，我们希望设定一个错误范围 $(c_x\\leq\\hat{c}_x \\leq c_x + \\varepsilon n)$，这个范围表示估计值的取值范围，我们希望结果在这个范围的概率为\\[P\\left(c_x\\leq\\hat{c}_x \\leq c_x + \\varepsilon n\\right)\\geq 1 - \\delta\\]这里， $(1 - \\delta)$ 表示结果在这个范围里的概率。那么设计一个最优Count-min Sketch模型的过程为： 估计数据流大小 $n$ 的大小 选择一个合理的 $\\varepsilon$ 值使 $\\hat{c}_x - c_x \\leq \\varepsilon n$ 选择一个合理的概率值 $(1-\\delta)$ $m$ 和 $k$ 的最优值可以通过以下公式获得：\\[m = \\left\\lceil{\\dfrac{e}{\\varepsilon}}\\right\\rceil , k = \\left\\lceil{\\ln (\\dfrac{1}{\\delta})}\\right\\rceil\\] 可以看出，想要错误范围越小，就要更大的 $m$ ，也就是表格的列数；同理，想要更高的概率（更小的 $\\delta$ ），就要更大的 $k$ ，也就是更多的hash函数。举个例子🌰假设我们现在需要为大小为 $10^6$ 的数据计数，我们选择 $\\varepsilon n = 2000$ ，即 $\\varepsilon = 0.002$。由此我们可以得出$m = 1360$，假如我们希望 $99\\%$ 的概率落在这个范围内，可得 $\\delta = 0.01$，因此hash函数数量 $k = 5$假设每个计数单元占内存大小为4 byte，那么，该模型将占用内存\\[m \\times k \\times\\text{size of counter} = 1360 \\times 5 \\times 4 \\text{ bytes} \\approx 28\\text{ kB}\\]参考：Advanced Data Structures: Count-Min Sketches" }, { "title": "Generate Digit Images using VAE", "url": "/posts/Generate-Digit-Images-using-VAE/", "categories": "Deep Learning", "tags": "gnn, vae, cnn", "date": "2021-04-01 06:00:00 +0200", "snippet": " Abstract: In the last decade, deep learning based generative models have gained more and more attention due to their tremendous progress. Among these models, there are two major families of them that deserve extra attention: Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). As a generative model comparable to GANs, VAEs combine the advantages of the Bayesian method and deep learning. It is built on the basis of elegant mathematics, easy to understand and performs outstandingly. Its ability to extract disentangled latent variables also enables it to have a broader meaning than the general generative model. A VAE can be defined as being an autoencoder whose training is regularised to avoid overfitting and ensure that the latent space has good properties that enable the generative process. In this post, VAE will be used to generate digit images based on the MNIST dataset.MethodIn short, as one of state-of-art generative models, a VAE is an autoencoder whose encodings distribution is regularised during the training process to ensure that its latent space has good performance allowing it to generate new data that is not included in the training dataset. It is worth noting that the term “variational” comes from the close relationship between regularization in statistics and variational inference methods. Just like a standard autoencoder, a variational autoencoder is composed of both an encoder and a decoder, as shown in Figure 11, and that is trained to minimize the reconstruction error between the decoded output and the initial data. However, in order to introduce some regularisation of the latent space, the encoding-decoding process is slightly modified by encoding the input as a distribution on the latent space instead of encoding it as a single point. In practice, the encoded distributions are chosen to be normal so that the encoder can be trained to return the mean and the variance matrix that describes these Gaussians. The reason for encoding the input as a distribution with a certain variance instead of a single point is that it can express latent spatial regularization very naturally: the distribution returned by the encoder is forced to be close to the standard normal distribution. Figure 1. Overview of VAE architectureTherefore, we can intuitively infer that the loss function of a VAE model consists of two parts: a “reconstruction term” that tends to make the encoding-decoding scheme as good as possible and a “regularisation term” that tends to regularise the organisation of the latent space by making the distributions returned by the encoder close to a standard normal distribution, which can be written as\\[L = ||x-\\hat{x}||^2+\\mathbb{KL}(\\mathbf{N}(\\mu_x, \\sigma_x),\\mathbf{N}(0, 1))\\]This function is also called evidence lower bound (ELBO) loss, which will be derived in Theoretical Part. As you can see, the regularisation term is expressed as the Kulback-Leibler divergence (KL) between the returned distribution and normal distribution. We can notice that the KL divergence between two Gaussian distributions has a closed form that can be directly expressed in terms of the means and the covariance matrices of the the two distributions. In this report, the mean-squared error (MSE) is applied to calculate the reconstruction loss, which is defined as\\[\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^n||x-\\hat{x}||^2\\]where $n$ is the number of training data, and\\[\\text{KL Loss} = \\sum_{i=1}^n(1+\\log(\\sigma_i^2)-\\mu_i^2-\\sigma_i^2)\\]is applied for calculating the KL divergence.Next, we will discuss the encoder and the decoder. The output of an encoder is a compressed representation which is called latent variable and the decoder takes it as input and tries to recreate the original input data. In practice, these two parts can be implemented using two neural networks. In this report, two convolutional neural networks (CNN) are used with details shown in Table 1 and 2. It is worth noting that the reason why the dimension of the fully connected layer of encoder used here is 40 is that half of it is used as mean, and the other half is used as variance so that the length of latent space is 20. Moreover, transposed convolution layers are applied in decoder to up-sample the latent variables. Layer Input Size Filter Size Stride Output Size conv1 $28^2\\times1$ $3^2\\times32$ 2 $14^2\\times32$ conv2 $14^2\\times32$ $3^2\\times64$ 2 $7^2\\times64$ fc $7^2\\times64$ - - $40$ Table 1. Encoder neural network Layer Input Size Filter Size Stride Output Size tconv1 $1^2\\times20$ $7^2\\times64$ 7 $7^2\\times64$ tconv2 $7^2\\times64$ $3^2\\times64$ 2 $14^2\\times64$ tconv3 $14^2\\times64$ $3^2\\times32$ 2 $28^2\\times32$ tconv4 $28^2\\times1$ $3^2\\times1$ 2 $28^2\\times1$ Table 2. Decoder neural networkAs we discussed before, before minimizing the loss, we need to generate sampled data, which includes the mean and the variance vectors to create the final encoding to be passed to the decoder network. However, we need to use back-propagation later to train our network later so that we cannot do sampling in a random manner. In this case, the trick of reparameterization can be adopted to substitute for random sampling. As shown in Figure 2, it is an example of our model with the length of latent variable equal to three and you can see that the the latent space\\[z_i=\\mu_i+\\exp{(\\sigma_i)}\\times e_i\\]where $e_i\\sim\\mathbf{N}(0,1)$. The general idea is that sampling from $\\mathbf{N}(\\mu_i,\\sigma^2)$ is same with sampling from $\\mu_i+\\exp{(\\sigma_i)}\\times e_i$.Figure 2. VAE with reparameterizationExperiment and EvaluationNow that we have defined everything we need, it is time to get it trained. The training parameters are shown in Table 3 and momentum is used in this report. The training progress plot is shown in Figure 3, the loss stays at around $18.5\\%$ after 50 epochs of training. Epochs Learning Rate Gradient Decay Factor 50 0.001 0.9 Table 3. VAE with reparameterizationFigure 3.Training progress plot of VAE&amp;lt;/center&amp;gt;_Now, let’s evaluate our trained VAE. First of all, randomly select ten images with labels of 0-9 from the test set and pass them through the encoder, and then use the decoder to reconstruct the images using the obtained representation. Shown in Figure 4, in most cases, after the encoding-decoding process, the reconstruction result is quite good, but for some input images whose writing is not in standard style, errors will occur. In addition, by observing these reconstructed images, we can find that the images generated by VAE are usually blurry.Figure 4. Reconstruction images of each digitNow, let’s generate some new images. Randomly generate a number of encodings from normal distribution with the length same with the latent space and pass them through the decoder to reconstruct the images, the results are shown in Figure 5. In addition to the phenomenon mentioned before that the generated image will get blurred, it is not difficult to see that some generated images are meaningless, such as the image in the fourth row and fifth column, since it is hard to determine whether it’s a 4 or 9.Figure 5. Randomly generated samples of digitsIn fact, compared with some models I tried before, the test result of the current model is one of the most satisfactory ones. Previously, I tried to add pooling layers and use shorter hidden variables, but the results were very bad. Some examples are shown in Figure 6. Apparently, using pooling layer to down-sample the image is not a good choice here for encoder and the length of latent variable should not be too short.Figure 6. Image generated by the model in case of failureIn the experiment, the classifier network architecture is shown in Table 4 below and the softmax function is applied on the final output of the network to obtain the classification result. After training on the entire training set of MNIST, the accuracy for testing on the test set can reach 99.21%. Layer Input Size Filter Size Stride Output Size conv1 $28^2\\times1$ $3^2\\times8$ 1 $28^2\\times8$ BN $28^2\\times8$ - - $28^2\\times8$ mp1 $28^2\\times8$ $2^2$ 2 $14^2\\times8$ conv2 $14^2\\times8$ $3^2\\times16$ 1 $14^2\\times16$ BN $14^2\\times16$ - - $14^2\\times16$ mp2 $14^2\\times16$ $2^2$ 2 $7^2\\times16$ conv3 $7^2\\times16$ $3^2\\times32$ 1 $7^2\\times32$ BN $7^2\\times32$ - - $7^2\\times32$ fc $7^2\\times32$ - - $10$ Table 4. Convolutional neural network (CNN) architecture of classiﬁer. BN is an abbreviation for batch normalization layer and there is always a ReLU activation layer follows it, and mp is for max pooling.Following the instruction, using half of the MNIST training set to train the VAE model defined in last section and use it to generate new images. The process of generating new images is as follows: re-input the training set used for training VAE to the encoder and get its encoding, add some Gaussian Noise to a portion of the encoding and then pass it through the decoder to regenerate new images. The generated image can refer to Figure 5, whose result is generally consistent. Use a certain proportion of unused training data in MNIST together with the same amount of newly generated data to train the classifier defined in Table 4, and use the test set in MNIST to test the trained classifier. The accuracy of the tests is shown in Table 5. As you can see, when the amount of training data increases, the accuracy of the test also increases. This is not difficult to explain, but what is interesting is that these test results are not as good as the classifier trained with the entire MNIST training set. I think the reason may be because the MNIST dataset is so easy that adding some interference does not improve the generalization ability of the classifier. However, I think for some more difficult datasets, doing this will reduce the overfitting and improve the classification ability. Percentage (%) 20 50 100 Accuracy (%) 97.50 98.34 98.70 Table 5. The accuracy of the classiﬁer trained with different proportions of the images in the other half of the training set on the MNIST test set together with a same amount of newly generated images.Reference Joseph Rocca: Understanding Variational Autoen-coders (VAEs). Link &amp;#8617; " }, { "title": "Note for Learning Git", "url": "/posts/Note-for-learning-Git/", "categories": "Git", "tags": "git, github, version control", "date": "2020-10-01 06:00:00 +0200", "snippet": "Basics CMD Note git --version Display current git version git config --global --edit Open git global config file and then you can edit it git config --global user.name &quot;San Zhang&quot; Change global user name git config --global user.email abc@gmail.com Change global user email git status Gives info on the current status of a git repo and its contents git init To create a new git repo git add file1 file2 To stage changes to be committed git add . To stage all changes at once git commit -m &quot;some message&quot; To commit changes from staging area git commit -a -m &quot;some message&quot; or git commit -am &quot;some message&quot; Combine last two cmds git commit --amend To modify the last commit git log --oneline Check different commits and display them briefly one line for each commit Branching CMD Note git branch To view your existing branches git branch -v To view your existing branches with more details (Last commit) git branch &amp;lt;branch-name&amp;gt; To make a new branch based upon the current HEAD git branch -d &amp;lt;branch-name&amp;gt; To delete a branch git branch -D &amp;lt;branch-name&amp;gt; To force delete a branch git branch -m &amp;lt;new-name&amp;gt; To change the current branch to a new name git switch &amp;lt;branch-name&amp;gt; To switch branch git switch -c &amp;lt;branch-name&amp;gt; To create a new branch and switch to it git checkout &amp;lt;branch-name&amp;gt; To switch branch git checkout -b &amp;lt;branch-name&amp;gt; To create a new branch and switch to it git merge &amp;lt;branch-name&amp;gt; Fast-forward merge (master no change), merge the branch to the master branch git merge &amp;lt;branch-name&amp;gt; None-fast-forward merge when some is appended on the master branch after creating the branch. There will be a new commit. Whenever you encounter merge conflicts, follow these steps to resolve them: Open up the file(s) with merge conflicts Edit the file(s) to remove the conflicts. Decide which branch’s content you want to keep in each conflict. Or keep the content from both. Remove the conflict “markers” in the document Add your changes and then make a commitgit diffTo view changes between commits, branches, files, our working directory, etc.It is used alongside commands like git status and git log, to get a better picture of a repository and how it has changed over time. CMD Note git diff &amp;lt;file&amp;gt; List all the changes in working directory that are not staged for the next commit git diff HEAD &amp;lt;file&amp;gt; List all changes in the working directory tree since your last commit git diff --staged &amp;lt;file&amp;gt; or git diff --cached &amp;lt;file&amp;gt; List the changes between the staging area and our last commit git diff &amp;lt;branch-1&amp;gt; &amp;lt;branch-2&amp;gt;or git diff &amp;lt;branch-1&amp;gt;..&amp;lt;branch-2&amp;gt; List the chanegs between the tips of branch1 and branch2 git diff &amp;lt;commit-hash-1&amp;gt; &amp;lt;commit-hash-2&amp;gt;or git diff &amp;lt;commit-hash-1&amp;gt;..&amp;lt;commit-hash-2&amp;gt; List the changes between two different commits git stashWhen working on a new branch, you made some changes but haven’t make any commits. Now you need to switch back to master/other branches, two case could happen: the changes come with me to the destination branch git won’t let me switch if it detects potential conflicts (where git stash could be used)Git pit provides an easy way of stashing these uncommitted changes so that we can return to them later, without having to maker unnecessary commits. CMD Note git stash Save changes that you are not yet ready to commit git stash pop Remove the most recently stashed changes in your stash and re-apply them to your working copy git stash apply Apply whatever is stashed away, without removing it from the stash. This can be useful if you want to apply stashed changes to multiple branches git stash list View all stashes git stash apply stash@{stash-id} (git assumes you want to apply the most recent stash when you run git stash apply) You can specify a particular stash-id to apply git stash drop stash@{stash-id} Delete a particular stash git stash clear Clear out all stahses Time Travelinggit checkoutcheckout command can be used to create branches, switch to new branches, restore files, and undo history.Note: HEAD usually refers to a branch NOT a specific commit When we checkout a particular commit, HEAD points at that commit rather than at the branch pointer (DETACHED HEAD). Then you have a couple options: Stay in detached HEAD to examine the contents of the old commit. Leave and go back to wherever you were before: reattach the HEAD Create a new branch and switch to it. You can now make and save changes, since HEAD is no longer detached. git checkout supports a slightly odd syntax for referencing previous commits relative to a particular commit. HEAD~1 refers to the commit before HEAD (parent) HEAD~2 refers to 2 commits before HEAD (grandparent) … CMD Note git checkout &amp;lt;commit-hash&amp;gt; View a previous commit git switch &amp;lt;branch-name&amp;gt; Re-attach HEAD: simply switch back to whatever branch you were on before git switch - Take you back to where you left off git checkout HEAD &amp;lt;file&amp;gt; or git checkout -- &amp;lt;file&amp;gt; Revert the file back to whatever it looked like in last commit (to the HEAD), discard the chanegs git restoregit restore is a brand new Git command that helps with undoing operations.Because it is so new, most of the existing Git tutorials and books do not mention it, but it is worth knowing!Recall that git checkout does a million different things, which many git users find very confusing. git restore was introduced alongside git switch as alternatives to some of the uses for checkout. CMD Note git restore &amp;lt;file&amp;gt; Restore the file to the contents in the HEAD git restore --source &amp;lt;commit-hash&amp;gt; &amp;lt;file&amp;gt; Restore the file to the contents in a particular commit git restore --staged &amp;lt;file&amp;gt; Remove unwanted file(s) in current staged file(s) git resetSuppose you’ve just made a couple of commits on the master branch, but you actually meant to make them on a separate branch instead. To undo those commits, you can use git reset. CMD Note git reset &amp;lt;commit-hash&amp;gt; Reset the repo back to a specific commit. The commits are gone. git reset --hard &amp;lt;commit-hash&amp;gt; Undo both the commits AND the actual changes in your files git revertgit revert is similar to git reset in that they both “undo” changes, but they accomplish it in different ways.git reset actually moves the branch pointer backwards, eliminating commits.git revert instead creates a brand new commit which reverses/undos the changes from a commit. Because it results in a new commit, you will be prompted to enter a commit message.Both git reset and git revert help us reverse changes, but there is a significant difference when it comes to collaboration: If you want to reverse some commits that other people already have on their machines, you should use revert. If you want to reverse commits that you haven’t shared with others, use reset and no one will ever know!Remote Tracking Branchesmaster: A regular branch reference. I can move this around myself.origin/master: This is a “Remote Tracking Branch”. It’s a reference to the state of the master branch on the remote. I can’t move this myself. It’s like a bookmark pointing to the last known commit on the master branch on origin. At the time you last communicated with this remote repository, here is where x branch was pointing.They follow this pattern &amp;lt;remote&amp;gt;/&amp;lt;branch&amp;gt;. origin/master references the state of the master branch on the remote repo named origin. upstream/logoRedesign references the state of the logoRedesign branch on the remote named upstream (a common remote name) CMD Note git branch -r View the remote branches our local repository knows about git checkout origin/master Checkout these remote branch pointers git switch &amp;lt;remote-branch-name&amp;gt;or git checkout --track origin/remote-branch-name Create a new local branch from the remote branch of the same name FetchingFetching allows us to download changes from a remote repository, BUT those changes will not be automatically integrated into our working files.It lets you see what others have been working on, without having to merge those changes into your local repo.Think of it as “please go and get the latest information from Github, but don’t screw up my working directory.” CMD Note git fetch &amp;lt;remote&amp;gt; &amp;lt;branch&amp;gt; Fetch branches and history from a specific remote repository. It only updates remote tracking branches git fetch origin Fetch all changes from the origin remote repository Pullinggit pull is another command we can use to retrieve changes from a remote repository. Unlike fetch, pull actually updates our HEAD branch with whatever changes are retrieved from the remote.“go and download data from Github AND immediately update my local repo with those changes” CMD Note git pull &amp;lt;remote&amp;gt; &amp;lt;branch&amp;gt; git fetch + git merge RebasingThere are two main ways to use the git rebase command: as an alternative to merging as a cleanup toolMerge vs. RebaseThe feature branch has a bunch of merge commits. If the master branch is very active, my feature branch’s history is muddiedWe can instead rebase the feature branch onto the master branch. This moves the entire feature branch so that it BEGINS at the tip of the master branch. All of the work is still there, but we have re-written history.Instead of using a merge commit, rebasing rewrites history by creating new commits for each of the original feature branch commits.We can also wait until we are done with a feature and then rebase the feature branch onto the master branch.git switch featuregit rebase masterInteractive RebaseSometimes we want to rewrite, delete, rename, or even reorder commits (before sharing them). We can do this using interactive git rebase.Running git rebase with the -i option will enter the interactive mode, which allows us to edit commits, add files, drop commits, etc. Note that we need to specify how far back we want to rewrite commits. Also, notice that we are not rebasing onto another branch. Instead, we are rebasing a series of commits onto the HEAD they currently are based on.Functions: pick - use the commit reword - use the commit, but edit the commit message edit - use commit, but stop for amending fixup - use commit contents but meld it into previous commit and discard the commit message drop - remove commit CMD Note git rebase -i HEAD~n Interactive rebase, with n commits before the HEAD Interact with remote repo CMD Note git remote -v View any existing remotes for you repository git remote add &amp;lt;name&amp;gt; &amp;lt;url&amp;gt; Add a new remote. Name is typically origin git remote rename &amp;lt;old&amp;gt; &amp;lt;new&amp;gt; Rename a remote git remote remove &amp;lt;name&amp;gt; Remove a remote git push &amp;lt;remote&amp;gt; &amp;lt;local-branch&amp;gt; Make a push to a local branch up to a remote branch of the same name git push &amp;lt;remote&amp;gt; &amp;lt;local-branch&amp;gt;:&amp;lt;remote-branch&amp;gt; Make a push to a local branch up to a remote branch of the different name git push -u The -u option allows us to set the upstream of the branch we’re pushingRunning git push -u origin master sets the upsteam of the local master branch so that it tracks the master branch on the origin repo, then next time we only need to do git push TagsTags are pointers that refer to particular points in Git history. We can mark a particular moment in time with a tag. Tags are most often used to mark version releases in projects (v4.1.0, v4.1.1, etc.)Think of tags as branch references that do NOT CHANGE. Once a tag is created, it always refers to the same commit. It’s just a label for a commit.There are two types of Git tags we can use: lightweight tags: just a name/label that points to a particular commit annotated tags: store extra meta data including the author’s name and email, the date, and a tagging message (like a commit message) Semantic Versioning: The semantic versioning spec outlines a standardized versioning system for software releases. It provides a consistent way for developers to give meaning to their software releases (how big of a change is this release ??) Versions consist of three numbers separated by periods. Patch releases: normally do not contain new features or significant changes. They typically signify bug fixes and other changes that do not impact how the code is used Minor releases: signify that new features or functionality have been added, but the project is still backwards compatible. No breaking changes. The new functionality is optional and should not force users to rewrite their own is code. Major releases: signify significant changes that is no longer backwards compatible. Features may be removed or changed substantially. CMD Note git tag Print a list of all the tags in the current repository git tag -l &quot;*beta*&quot; Search for tags that match a particular wildcard pattern. For example, The one on the left will print a list of tags that include “beta” in their name. git checkout &amp;lt;tag&amp;gt; View the state of a repo at a particular tag git diff &amp;lt;tag1&amp;gt; &amp;lt;tag2&amp;gt; Check out the difference between two tages/versions git tag &amp;lt;tagname&amp;gt; To create a lightweight tag, By default, Git will create the tag referring to the commit that HEAD is referencing. git tag -a &amp;lt;tagname&amp;gt; To create a annotated tagSimilar to git commit, we can also use the -m option to pass a message directly and forgo the opening of the text editor git show &amp;lt;tagname&amp;gt; See more information about a particular tag git tag &amp;lt;tagname&amp;gt; &amp;lt;commit&amp;gt; Tag an older commit by providing the cimmit hash git tag -f &amp;lt;tagname&amp;gt; Force to change the exisiting &amp;lt;tagname&amp;gt; to a new commit git tag -d &amp;lt;tagname&amp;gt; Delete a existing tag git push --tags Transfer all of your tags to the remote server that are not already there.By default, the git push command doesnt transfer tags to remote servers. git push &amp;lt;tagname&amp;gt; To push one tag What is in .git?Note: There’s more, but this is the juicy stuffconfigThe config file is for configuration. We’ve seen how to configure global settings like our name and email across all Git repos, but we can also configure things on a per-repo basis.Reference link: git-configrefs Folder Inside of refs, you’ll find a heads directory. refs/heads contains one file per branch in a repository. Each file is named after a branch and contains the hash of the commit at the tip of the branch. E.g. refs /heads/master contains the commit hash of the last commit on the master branch. Refs also contains a refs/tags folder which contains one file for each tag in the repo. Refs also contains a refs/remotes folder which contains different remotes have been set up.HEAD FileHEAD is just a text file that keeps track of where HEAD points. If it contains refs/heads/master, this means that HEAD is pointing to the master branch. In detached HEAD, the HEAD file contains a commit hash instead of a branch reference.objects FolderThe objects directory contains all the repo files. This is where Git stores the backups of files, the commits in a repo, and more. The files are all compressed and encrypted, so they won’t look like much! Hashing Functions Hashing functions are functions that map input data of some arbitrary size to fixed-size output values. Cryptographic Hash Function One-way function which is infeasible to invert Small change in input yields large change in the output Deterministic: same input yields same output Unlikely to find 2 outputs with same value Git is a key-value data store. We can insert any kind of content into a Git repository, and cit will hand us back a unique key we can later use to retrieve that content. These keys that we get back are SHA-1 checksums.Git uses a hashing function called SHA-1 (though this is set to change eventually).Git uses SHA-1 to hash our files, directories, and commits. SHA-1 always generates 40-digit hexadecimal numbers. The commit hashes we’ve seen a million times are the output of SHA-1Four types of Git objects: Git blobs (binary large object) are the object type Git uses to store the contents of files in a given repository. Blobs don’t even include the filenames of each file or any other data. They just store the contents of a file! CMD Note git hash-object &amp;lt;file&amp;gt; Takes some data, stores in in our .git/objects directory and gives us back the unique SHA-1 hash that refers to that data object.In the simplest form (show on the left), Git simply takes some content and returns the unique key that WOULD be used to store our object. But it does not actually store anything. echo &quot;hello&quot; \\| git hash-object --stdin The --stdin option tells git hash-object to use the content from stdin rather than a file. In the example, it will hash the word “hello” echo &quot;hello&quot; \\| git hash-object --stdin -w Rather than simply outputting the key that git would store our object under, we can use the -w option to tell git to actually write the object to the database. After running this command, check out the contents of .git/objects git cat-file -p &amp;lt;object-hash&amp;gt; Retrieve the stored data in .git/objects database. The -p option tells Git to pretty print the contents of the object based on its type git cat-file -t &amp;lt;object-hash&amp;gt; Tell the type of this hash object Trees are Git objects used to store the contents of al directory. Each tree contains pointers that can refer to blobs and to other trees.Each entry in a tree contains the SHA-1 hash of a blob or tree, as well as the mode, type, and filename. CMD Note git cat-file -p master^{tree} Print out the tree object that is pointed to by the tip of our master branch Commit objects combine a tree object along with information about the context that led to the current tree. Commits store a reference to parent commit(s), the author, the commiter and of course the commit message! Annotated tag " }, { "title": "基于深度学习的游客人脸表情的识别", "url": "/posts/facial-expression-detection-using-deep-learning/", "categories": "Deep Learning", "tags": "deep learning, face detection, emotion detection, cnn", "date": "2020-06-30 06:00:00 +0200", "snippet": " 摘要：面对面的进行沟通交流，在我们日常人与人之间的交往中扮演着很重要的角色，从交谈者的面部我们可以获取许多我们在文字交流中难以获取的信息，例如一个人的心理状态、情绪、意向等。的确，一个人的面部蕴含着许多潜在有价值的信息，对其加以有效地收集和分析，就可以创造出一些有利用价值的数据。近些年，对于面部表情识别的研究，被广泛地应用于教育、心理、医学以及商业等多个领域，人脸表情识别也是当前计算机视觉、模式识别等多个人工智能领域的热门研究课题，它是实现智能人机交互中重要的一个环节。 深度学习，是一个近十年来飞速发展的领域，它的出现改进了许多现有的机器学习算法。在特征提取和数学建模上，深度学习都有着明显的优势，由于拥有良好的泛化能力，困扰过去人工智能领域发展的一些难以解决的问题被较好的克服了，并且随着近些年GPU数据处理能力的飞速提升，深度学习也被广泛的应用于目标检测、计算机视觉、自然语言处理等多个领域并取得了不错的成效。 本课题主要针对旅游景区这一特定应用场景，实现利用多任务卷积神经网络（MTCNN）模型实现对游客人脸的实时检测，再利用一个简化的AlexNet卷积神经网络模型实现对所检测到的人脸进行表情分类。经过多次的模型训练和改良，最终在CPU环境下可以实现到一个不错的实时检测效果。 Abstract: Face-to-face communication plays an important role in our daily interpersonal interactions. From the face of the interlocutor, we can obtain a lot of information that we cannot obtain in text communication, such as a person’s mental state, emotions, and intentions, etc. Indeed, faces contain a lot of potentially valuable information. An effective collection and analysis of them can create some valuable data. In recent years, research on facial expression recognition has been widely used in education, psychology, medicine, and business. Facial expression recognition is also popular research in the community of artificial intelligence such as computer vision and pattern recognition. It is a critical component in the realization of intelligent human-computer interaction. Deep learning is a rapid development field over the past decade, and its emergence has improved many existing machine learning algorithms. In feature extraction and mathematical modelling, deep learning has obvious advantages. Due to its good generalization ability, some intractable problems that have troubled the development of artificial intelligence in the past have been better overcome. In recent years, with the rapid development of GPU data processing capabilities, deep learning has also been widely used in target detection, computer vision, natural language processing and other fields and has achieved good results. This subject is mainly aimed at the specific application scenario of tourist scenic spots, using the multi-task convolutional neural network (MTCNN) model to realize the real-time detection of tourist’s faces, and then a simplified AlexNet convolutional neural network model is used to realize the classification of the detected faces. After many times of model training and improvement, a good real-time detection result can be achieved in the CPU environment.1. 绪论本章节重点介绍基于深度学习的景区游客笑脸识别与实现这一课题的研究背景和意义，国内外目前针对人脸检测、表情识别等研究方向的研究成果以及遇到的一些难题。本章节还将对实现本课题的具体步骤初步进行简要概述，并对本文的结构框架作出阐述。1.1 课题的背景和意义心理学研究表明，人脸表情是人类表达情感的重要载体之一，其中蕴涵着丰富的人体行为信息，通过一个人的表情，就可以预测此时此刻这个人内心的情感、动机、个性等众多信息。由此可见，对表情的理解是智能的体现，如果计算机可以智能地对这些人脸表情信息进行收集并加以分辨，将从根本上改变人机交互的方式，使计算机更好的服务于人类。随着近些年数据量的爆炸式增长，大数据技术也在迅猛地发展，在商业、经济以及其他领域中，决策将日益趋向于由数据和分析而作出，而并非基于传统习惯上的经验和直觉。大数据能够为企业改进运营模式提供强有力参考，也可为用户带来更为丰富的服务体验，而如今在高效的算法和成熟计算能力的硬件的加持下，深度学习被应用在了我们生活的方方面面。而人脸作为一个可以传递思想感情的一种重要方式，近些年对人脸的各种研究也是不断地被开展并且研究成果被广泛的应用于各种领域。然而目前在国内，机器学习技术在旅游业的应用仍然处于发展的初级阶段，国内大部分智慧旅游及旅游大数据企业仍然以提供信息化基础建设及旅游大数据的可视化为主，缺乏真正的机器学习以及人工智能的技术实现。目前，国内仅有少数为旅游者和旅游管理部门提供智能化服务的企业，这些企业率先应用了机器学习技术，从而进一步提升了行业管理水平，优化游客旅游服务体验。在旅游管理方面，对于机器学习技术的应用可以促进旅游产业监管检测以及强化景区内部管理制度，从而提升旅游业管理人员的决策能力以及管理效率。经过多年研究的发展，围绕人脸表情识别的研究已经数不胜数，来自全世界不同地方的研究者们也先后提出了很多优秀的方法。近些年，由于深度学习具有无监督特征学习能力突出的优点，所以对之的应用也是越来越多。本课题就针对旅游业这一特定行业，利用一些现有的技术手段，实现对旅游景区游客人脸的检测以及表情识别，从而可以协助景区管理者更及时的调整运营模式，为游客提供更好的服务和出游体验。1.2 国内外目前对于人脸检测、人脸表情的研究现状从19世纪起，人类就开始了对于人脸表情的研究，但是提及人脸表情，就不可避免的会涉及到人脸检测的问题，因为对于计算机而言，需要先从大维度的图像数据中完成对人脸位置的定位和裁切，才能进一步实现对人脸表情的分类。所以，本节将针对人脸检测、人脸表情现如今国内外的主要研究现状作出概括并总结研究中遇到的一些主要问题。1.2.1 人脸检测人脸检测是计算机视觉中一个被广泛研究的课题，如今大多数人脸检测器都可以轻松的实现正面人脸的检测任务，对于该领域的最新研究更多地集中在不受控制的面部检测问题上1，例如姿势变化，夸张的表情和极端照明等许多可能导致面部外观出现较大的视觉变化的因素，这些因素可能会严重地降低面部检测器的耐用性。人脸检测任务的困难主要源自两个方面：1）在杂乱的背景中，人脸的视觉变化会很大； 2）在大范围内搜索各种各样的人脸尺寸和姿势。前一种要求人脸检测器能准确地解决二分类问题，而后一种则进一步要求检测器需要提高检测效率。最开始Viola-Jones是使用Haar特征来实现面部检测器的2，使用这个特征可以对相对正向的面部完成迅速评估并识别。然而，由于Haar特征的简单特质，在不受控的环境中使用它进行人脸检测相对较弱，例如各种各样的人脸的姿势或不理想的光照条件。在过去的十年中，研究者们提出了很多基于Viola-Jones的人脸检测器的改进3，这些改进大部分是基于更先进的特征的级联式框架，得力于更优秀的特征，这些改进可以实现准确度更高的二分类任务，而且总体上对于计算量的需求并没有提高。但近些年随着硬件条件的不断进步，卷积神经网络在图像分类任务中取得了质的飞跃，也很快被应用于人脸检测的任务中，其性能和精度都超过了之前基于特征的检测方法。在第二章中，将对一些基于深度学习方法的人脸检测算法进行阐述和总结。1.2.2 人脸表情人通过变化自身的脸部肌肉以及眼和口部的肌肉，从而形成各种各样的表情，这些表情可以被用来传达不同的情绪状态，与声音、语言和身体语言等组成了人的交流系统中沟通系统。人脸面部表情识别是一个横跨众多科研领域的课题，例如神经学、人工智能领域等，当然对于该项课题的研究成果也被应用于广泛的领域，例如对病人进行心理测评、潜在用户购买行为的预测等4。对于人脸表情的研究的先锋者是Darwin5，之后Ekman和Friesen提出了面部运动编码系统（FACS）6，FACS是一种基于人类观察者的系统，旨在促进客观测量由面部肌肉收缩引起的面部外观的细微变化。通过对44个面部动作单元的监控，FACS可以对所有明显可辨别的表达方式进行语言描述。这个系统还定义了六种普遍公认的基本的人脸表情，分别是：开心、伤心、惊喜、害怕、生气和厌恶。尽管这六种表情是否是通用的人脸表情受到了很多人的质疑，但是之后大多数基于视觉的面部研究都是依赖于Ekman的定义的表情分类开展的，所以说FACS系统的提出是具有里程碑意义的。使用计算机对人脸表情信息进行特征提取，并且按照人类的思维方式对其加以理解和归类，再使用人类对表情已有的认知使计算机对检测到人脸表情加以联想，就是我们所说的人脸表情自动识别系统。人脸表情自动识别系统可以被应用于人机交互、压力监测、人类行为分析等众多领域，所以对于开发一套完整的人脸表情识别系统的研究吸引了来自不同领域的研究者。总体来说，对于人脸表情识别的方法可以被粗略的分为两种：基于几何特征的方法和基于外观的方法7。基于几何特征的方法依赖于人脸特征的几何分布，例如眉毛、眼睛、眼角、鼻子、嘴巴等脸部元素的位置和形状，但是试验结果表明，由于图像的质量、照明的情况等一些不可控的因素，用基于人脸几何特征分布的方法预测人脸表情并不是很可靠；至于第二种方法，基于外观的方法，是采用光流法或某些特定的滤波器对整个人脸或人脸的局部进行分析。人脸表情识别大致可以分为以下几个部分：图像的采集，图像数据的预处理，人脸检测，人脸特征提取，最终获得表情所属的分类。图1给出来表情识别的具体过程。图1 表情识别过程1.3 本课题的内容阐述本课题主要模拟旅游景区这一特定应用场景，先后分别训练出一套可用的人脸检测神经网络模型和一套负责完成表情分类的网络模型，将两个模型进行融合。之后完成对包含人脸的图像以及视频数据的采集，完成对采集的图像以及视频数据进行预处理后，输送进训练好并融合在一起的神经网络，使用模型实现对数据中人脸的定位以及预测表情的所属分类。实现本课题具体流程如下：(1) 选取训练数据集实现训练本课题所用卷积神经网络模型，一共要用到三套数据集，分别用来训练人脸检测模型、人脸关键点回归以及边框回归、和表情分类的模型。(2) 神经网络的搭建如上述所示，一共需要两个不同的网络模型。用于实现人脸检测的模型我采用了多任务卷积神经网络(MTCNN)，该网络可以同时实现人脸的检测与对齐，同时完成人脸关键点定位。用于人脸表情分类的网络，我采用了经典的AlexNet结构8，但将其特征维度进行了调整，从原始的千分类网络降成7分类网络。(3) 网络训练使用的深度学习框架Caffe，完成对神经网络的搭建和训练，Caffe框架对于简单模型的建立和训练，对于刚刚接触深度学习的人更加友好且更加便捷。并且Caffe拥有python和matlab的接口，可以方便的搭配利用opencv对图像进行处理。(4) 测试以及模型完善在网络训练结束后，就可以用建立好的网络模型，在实际的应用场景下进行测试，检测理论是否符合实际，及时发现问题并尽量完成对网络模型的修正。1.4 本文的组织结构第一章 绪论。主要对开展本课题的研究背景进行介绍，说明研究此课题的目的和意义，简单总结当前国内外针对人脸检测和人脸表情检测研究的主攻方向和方法，并简单介绍对于本课题的主要内容和实现步骤以及使用方法，概括本文的章节安排。第二章 深度学习的基本原理。本文重点使用的方法都是基于卷积神经网络来完成对数据特征的提取的，从而可以进一步实现分类任务。本章将简要对卷积神经网络的原理和结构进行阐述。为了能够准确且实时的从给定图像或视频数据中完成对人脸的检测和表情的识别，人脸检测的任务显得尤为重要，所以一个好的人脸检测模型是不可或缺地。本章进而将重点介绍多任务卷积神经网络理论，以及在具体实现人脸检测任务时涉及到的方法或算法。第三章 人脸检测及人脸表情识别网络结构。本章主要对人脸检测、人脸表情识别的卷积神经网络进行讨论，将详细阐述所用网络架构和训练方法。第四章 实验与测试。根据研究的课题，实验步骤可被分为四步：（1）训练数据集的选取；（2）神经网络模型的搭建及训练；（3）实际任务下的检测和对模型的修正。本章主要记录用卷积神经网络实现人脸表情识别系统的过程并实际测试使用效果。2. 深度学习的基本原理2.1 引言机器学习技术为现代社会的各个方面提供了强大的动力：从网络搜索到社交网络上的内容过滤再到电子商务网站上的推荐机制，并且它在诸如相机和智能手机之类的消费产品中越来越频繁地被利用。然而常规的机器学习技术在处理原始格式的自然数据方面受到了限制。几十年来，想要构建一个模式识别或者机器学习系统，需要非常仔细的工程设计和相当多领域的专业知识，才能设计出一个可以从输入中检测或分类其模式的子学习系统。表征学习是一组允许向机器提供原始数据并可以自动发现需要被检测或分类的表征的方法。深度学习方法是具有多层表征的表征学习方法，这些层级是通过组合简单但非线性的模块而获得的，每个模块将一个层级（从原始输入开始）的表征转换为更高且稍微更加抽象的层级的表征。有了足够多的此类转换，非常复杂的函数模型也可以被学习了。事实证明，深度学习方法非常善于发现高维数据中的复杂结构，因此在科研，商业和政府等众多领域都是适用的。深层神经网络是目前主要的深度学习形式，卷积神经网络就是其中一种经典的结构。卷积神经网络是被设计用来处理多维数组数据的，比如一张包含三个由三颜色通道组成的二维像素值的彩色图像。卷积神经网络使用4个关键的思想来利用自然信号的属性：局部连接、权值共享、池化以及多网络层的使用9。本章将简要概括卷积神经网络的结构和基本原理，并重点介绍一下本文人脸检测所用的算法 – 多任务卷积神经网络（MTCNN）。2.2 卷积神经网络一个典型的卷积神经网络（CNN）的结构可以被分为几个部分，如图2所示。最前面的部分是由卷积层和池化层两种层级结构构成。卷积层含有多个特征图（Feature Map），这些特征图则是由神经元组成，每一个单独的神经元通过卷积核与当前特征图之前的特征图的局部相连接，这里的卷积核是一组由权值组成的矩阵，而这个链接的局部可以被称为局部感受野。这里得到的局部加权和将被通过一次非线性函数的运算，例如修正线性单元（ReLU函数），之后将得到该卷积层中所有神经元的输出值。在同一特征图中，所有的神经元共享相同的卷积核，该特性也被称为权值共享，但在不同的特征图中，所用的卷积核是不同的。通过权值共享操作，可以有效的降低模型复杂度，并可以更快捷地训练网络。使用这种结构的原因有两个：第一，对于一个阵列数据，例如图像，局部的一些数据是具有高度相关性的，一个可辨别的图案是由一个区域内的所有数据形成的而并非某一点的一个数据；第二，在例如图像这种数据中，局部的数据是和其位置无关的，例如人脸可能会出现在一张图像中的任意位置。在数学意义上，这里的卷积核操作就是一个离散的卷积操作，卷积神经网络也因此而得名。在卷积层之后，紧接着的是池化层，和卷积层结构类似，由池化操作得到的每一个特征图都唯一对应其之前层的某一个局部感受野。如果说卷积层的作用是用来检测上一层中局部特征的，那么池化层扮演的角色则是融合前面检测到的相似的特征。由于组成某一图案的特征是可变的，所以为了提高模型的泛化能力和鲁棒性，对之前提取到的特征图进行压缩处理是很有必要的，这么做同时还可以降低模型的复杂度，提高训练速度。常用的池化操作包括最大池化、均值池化等，分别对其对应的局部感受野进行求最大值或均值，至于在不同应用场景下应该应用哪些池化操作，Boureau等人10做出了详尽的理论分析。经过几个卷积层和池化层的反复操作后，在CNN结构中一般还会有1个或以上的全连接层。与传统的神经网路类似，CNN中的全连接层中的单个神经元会与其前一层的所有神经元一一对应相连，通过全连接层后，之前经过卷积层和池化层后得到的区分性局部信息将被得到整合。如果想要提升整个网络结构的性能，可以在全连接层中的每个神经元后加一个非线性函数（例如ReLU函数）以提高其非线性分类能力。通过最后一层全连接层后，将得到输出值，这些值形成输出层，该层可以通过采用例如softmax逻辑回归的方法进行分类任务。最后通过计算一个损失函数，就可以开始使用反向传播算法进行特征值的迭代训练了。当然，本文只是简单对卷积神经网络的原理和结构进行了概述，很有很多细节例如特征图维度和数量的选取、损失函数的选取、正则化方法等都没有进行总结，由于其内容过于冗长，所以仅对一些必要的内容进行了阐述。图2 卷积神经网络的基本结构2.3 人脸检测算法2.3.1 人脸检测算法概述人脸检测的目标是找出输入图像中是否有人脸，如果有，需要输出所有包含人脸的对应位置，算法的输出是人脸外接矩形在图像中的坐标，可能还包括姿态如倾斜角度、五官位置等信息。在目标检测所有的子研究领域中，人脸检测是目前被研究的最充分的问题之一，它在安防监控、人机交互、社交娱乐等方面有很高的应用价值，也是整个人脸识别、表情识别等算法的第一步。虽然人脸近似于一个刚体，都是由五官构成，但由于表情、姿态、角度、光照等原因，准确的定位图像中的人脸是有难度的。一个人脸检测算法需要考虑如下几个问题：人脸会出现在图像中的任意位置；人脸在图像中的尺寸是可变的；图像中的人脸可能是任意角度和姿态的；图像中的人脸有可能被遮挡。正确检测率和误检率两个指标通常被用来评价一个人脸检测模型的好坏，正确检测率被定义为\\[\\text{正确检测率}=\\frac{\\text{检测到的人脸数量}}{\\text{图像中所有的人脸数量}}\\]误检率被定义为\\[\\text{误检率}=\\frac{\\text{误报个数}}{\\text{图像中所有非人脸扫描窗口数量}}\\]一个优秀的人脸检测算法，将会在正确检测率和误检率之间进行权衡，从而获得尽可能高的正确检测率和低的误检率。一个典型的人脸分类器模型需要用大量的人脸和非人脸图像进行训练，训练后可以得到一个能够解决二分类问题的分类器模型，该分类器模型也被称为人脸检测模版。该分类器模型可以对输入图像进行预测，从而获得图中是否为人脸，但通常该分类器仅可以对固定大小的输入进行处理，所以对一般输入（非训练时设定的尺寸）的数据，需要进行如图3所示的处理。由于人脸的尺寸可以是任意的，所以第一步需要对原始输入图像进行尺寸的放大和缩小，从而构建图像金字塔；接下来为了检测所有图像金字塔内图像的任意位置的人脸，需要对所有图像进行滑动窗口（Sliding Window）检测，简单来说就是用一个固定尺寸的窗口在所有图像内进行从上到下、从左到右的滑动检测。这个过程需要对大量不同尺度的图像进行反复扫描，从而获得预选人脸窗口，所以这个过程会消耗大量时间。图3 多尺度滑动窗口实现人脸检测如图3所示，滑动窗口在完成检测后，还会进行一步非极大值抑制（NMS, Non-maximum Suppression）操作。顾名思义，该操作就是抑制局部非极大值的预测，从而保留概率最高的局部最大值。该算法不仅适用于人脸检测，在几乎所有的目标检测任务中，都会用NMS来对所有的预测窗口进行筛选。如图4，假设当前完成滑动窗口检测后，有多个人脸的预选窗口存在，可以看出这些预选窗口是高度重合的，我们的任务是将这些高度重合的预选窗口进行排序，迭代排除掉所有概率相对较低且与概率相对较高的预选窗口高度重合的预选窗口，从而保留IOU相对较低的所有高概率预选窗口。经过NMS操作后，大量的高度重合的无用预测窗口将会被筛选掉。图4 NMS操作以上是对整个人脸检测流程的简单概述，基本覆盖了典型人脸检测算法的所有流程。不同的人脸检测算法，大多是基于该流程基础上，对人脸分类器进行改良，例如级联CNN算法1（Cascade CNN），该算法的人脸分类器采用的是级联式的框架。在构建完图像金字塔后，在第一层网络上（12-net）将对这些图像进行滑动窗口检测并过滤掉超过90%的预选窗口，剩下的窗口将被送入一级校准网络（12-calibration-net）进行窗口校准，并采用NMS合并高度重叠的检测窗口，在后面更高层级的网络中，将进行人脸检测框位置进一步的矫正和进一步的筛选，详细网络结构可以参考文献[1]。该算法在一定程度上降低了传统算法在开放环境中对于光照、位置、角度等的敏感度，相较于之前基于传统卷积神经网络的一些算法是有很大突破的，但是由于框架的第一级网络仍然是采用滑动窗口的形式进人脸位置的初步检测，所以在对效率要求较高的应用场景下，该算法还是被限制了，而且级联CNN模型对于小目标人脸检测仍不是很好。仍然采用类似于级联CNN算法的级联式框架，Zhang等人11在2016年提出另一种级联式的人脸检测模型–多任务卷积神经网络，本文后续在程序实现时也是采用的这种人脸检测算法，在下一小节将对这种算法进行详细阐述。2.3.2 多任务卷积神经网络MTCNN（Multi-task Convolutional Neural Network）顾名思义是一种集人脸分类、人脸区域回归和人脸关键点回归三个任务于一身的多任务同时执行的算法，结构采用的是类似于级联CNN算法的级联式结构，不同于级联CNN算法的是在初级的检测中没有使用滑动窗口的方法来对输入图像进行初步人脸框预测，这种算法同时兼顾了计算效率和准确率，对上述两种评测指标进行了良好的平衡。在开始介绍MTCNN的网络结构和工作原理之前，需要先了解边框回归（Bounding-Box regression）的工作原理，这也是在MTCNN网络中在执行人脸边框预测、人脸关键点回归任务时至关重要的一个步骤。之所以要执行边框回归的操作，是为了使预测的人脸边框位置或者对于人脸关键点的定位更加准确，如图5(a)所示，在输入图像经过网络后得到的预测人脸窗口为红色框覆盖区域，而准确的人脸位置应该是绿色框所覆盖的区域，这时红色框的定位并不准确（例如 $IOU&amp;lt;0.7$)，所以我们需要对预测窗口进行修正。对于窗口，一般使用四维向量 $(x,y,w,h)$ 分别表示窗口的中心点坐标、高度、和宽度，如图5(b)绿色框 $G$ 代表准确位置，红色框 $P$ 代表原始的预测位置，而蓝色框 $G’$ 则代表一个经过校准后更加接近于真实窗口 $G$ 的窗口区域。所以边框回归的目的就是当给定 $(P_x,P_y,P_w,P_h)$ 时，寻求一种映射 $f$ ，从而使得\\[f(P_x,P_y,P_w,P_h)=(G&#39;_x,G&#39;_y,G&#39;_w,G&#39;_h)\\tag{2.1}\\]并且\\[(G&#39;_x,G&#39;_y,G&#39;_w,G&#39;_h) \\approx (G_x,G_y,G_w,G_h)\\tag{2.2}\\]至于如何修改损失函数从而使窗口 $P$ 校正成更接近准确窗口的窗口 $G’$ ，本文不做详细赘述，可以参考文献12。图5 边框回归原理MTCNN采用了三个级联卷积神经网络级联的结构，如图6所示，分别为P-Net，R-Net，O-Net，三个网络结构从简单到复杂，采取候选框以及分类器的思想，从而可以非常高效的进行人脸检测任务。图6 MTCNN整体结构图像金字塔中图像首先会被送入P-Net（Proposal Network），顾名思义这是一个人脸区域建议网络。该网络是一个全卷积网络13，相较于上文提到的级联CNN网络第一级的12-net，P-Net使用的全卷积网络的优势在于由于在网络中没有全连接层（当然有些网络在有全连接层时也可以接受任意尺寸的图像输入，例如空间金字塔池化法14），所以它可以接受任意尺寸的图像输入，且相较于滑动窗口的方法，卷积运算可以节省大量的计算时间。P-Net是一个相对较浅的网络，而且输入特征仅为$12\\times12\\times3$大小，所以该网络可以快速地初步对人脸框位置以及人脸关键点进行回归。由于这个网络仅是由三个卷积层和一个最大池化层构成的神经网络，而且每层结构并不复杂，所以特征经过这一级网络后得到的预测仅仅是有一定可信度的，即使之后使用NMS和边框回归算法分别对候选框进行了筛选和校正，但是此时的预测结果仍然不准确。第二级网络为R-Net（Refine Network），顾名思义这一级网络是用来改进前一级网络对人脸位置和关键点的回归的。相较于P-Net，R-Net多了一层全连接层，且输入特征也是增大到了$24\\times24\\times3$，因此这一层网络会对输入数据进行更加仔细严格的筛选，大部分的错误预测会被该级网络排除掉，之后会有同样的NMS和边框回归算法操作。之所以R-Net可以实现更加精确的预测，这得力于R-Net最后一层128大小的全连接层，这个全连接层可以保留更多的图像特征，从而实现更高的准确性。最后一级网络被称为O-Net（Output Network），这层网络是用来进行最后一级筛选和校准并输出预测和回归结果的。O-Net是三级网络中结构最为复杂的一个，输入特征为$48\\times48\\times3$，相较于R-Net，O-Net多了一个卷积层，并且全连接层的维度扩大到了其两倍，这意味着它会保留更多的图像特征细节，实现更加精准的人脸判别、人脸区域回归、以及人脸特征定位。总结来说，MTCNN利用从简易到复杂的级联式神经网络结构，兼顾了计算效率和准确率，第一级网络使用了全卷积网络从而避免了滑动窗口采样操作带来的巨大的计算时间消耗，大幅度提升了人脸框预测和关键点回归的效率。由于其简易的网络结构和相对不错的效率和准确度，MTCNN在工业级场景中得到了广泛应用。3. 人脸检测及人脸表情识别网络结构3.1 CNN结构如上一章节所述，MTCNN共包含3个子网络（P-Net，R-Net，和O-Net），3个子网络通过级联的方式实现对人脸的检测、边框回归、以及人脸关键点的回归，3个网络的具体参数和机构如图7所示。(a) P-Ne(b) R-Net(c) O-Net图7 MTCNN3个子网络架构（Conv代表卷积层，步长为1；Max-Pool代表最大池化层，步长为2）用于人脸表情分类的网络，本文使用的是一个简化版的AlexNet，原始的AlexNet是用于1000类分类的维度庞大的网络，由于表情共分为7个不同类别，所以简化的网络即为一个七分类网络，结构如图8所示。图8 人脸表情分类网络架构（Conv代表卷积层，步长为1；Max-Pool代表最大池化层，步长为2）3.2 网络训练我们在整个实现算法中，一共用到了两个模型。在MTCNN网络中共有三个任务，分别是人脸二元分类任务、边框回归、以及人脸关键点定位，所以我们需要用到三个不同的损失函数得到熟练，分别是 人脸二元分类任务损失函数使用的是分类任务常用的交叉熵函数，对于每一个样本$x_i$，\\[L_i^{bin}=-(y_i^{bin}log(p_i)+(1-y_i^{bin})(1-log(p_i)))\\tag{3.1}\\]其中$p_i$是输入经过网络后得到是人脸的概率值，$y_i^{bin}\\in{0,1}$表示已标注的真实标签。 边框回归（Bounding Box Regression）任务的损失函数是计算预测边框与最近真实边框的欧式距离，对于每个样本$x_i$，\\[L_i^{bbr}=\\left \\|\\hat y_i^{bbr}-y_i^{bbr}\\right \\|_2^2\\tag{3.2}\\]其中$\\hat y_i^{bbr}$是回归预测得到的边框位置，$ y_i^{bbr}$是标注框的真实位置，$ y_i^{bbr}\\in\\mathbb{R}^{4}$。 人脸关键点回归任务与边框回归任务类似，同样计算预测点与真实标注点的欧式距离，对于每个样本$x_i$，\\[L_i^{lm}=\\left \\|\\hat y_i^{lm}-y_i^{lm}\\right \\|_2^2\\tag{3.3}\\]其中$\\hat y_i^{lm}$是通过网络得到的预测值，$y_i^{lm}$是真实的标注值，$ y_i^{lm}\\in\\mathbb{R}^{10}$。由于我们在MTCNN的三个不同CNN中需要完成不同的任务，所以在训练时，输入的训练数据也是不同的，因此在训练不同的CNN时计算的损失函数也需要区别对待。总的训练损失函数可以表示为\\[\\sum_{i=1}^N \\sum_{j \\in \\left\\{bin,bbr,lm \\right\\} }{\\alpha_j \\beta_i^j L_i^j}\\tag{3.4}\\]其中$N$表示训练样本总数，$\\alpha_j$表示任务权重，在P-Net和R-Net中，$\\alpha_{bin}=1,\\alpha_{bbr}=0.5,\\alpha_{lm}=0.5$，在O-Net中$\\alpha_{bin}=1,\\alpha_{bbr}=0.5,\\alpha_{lm}=1$，$\\beta_i^j\\in{0,1}$表示样本类型权重。对于人脸表情识别网络，它是一个多分类网络，所以损失函数使用的同样是交叉熵函数，\\[L_i^{facialExp}=-(y_i^{facialExp}log(p_i)+(1-y_i^{facialExp})(1-log(p_i)))\\tag{3.5}\\]其中$p_i$是输入经过网络后得到预测表情的概率值，$y_i^{facialExp}\\in{0,1}$表示已标注的真实标签。3.3 小结本章节主要介绍了实现本课题具体使用的深度神经网络模型及其训练方法，通过一些现有的深度学习框架完成对上述理论模型的搭建，一个简单的人脸检测及人脸表情识别的深度学习模型就可以被成功建立了。在下一章，将具体介绍实现本课题所用的一些实验条件和方法。4. 实验与测试4.1 数据集如上文提到的，实现人脸表情识别共需训练两个网络模型，MTCNN模型用来做人脸检测和人脸关键点回归，另一个AlexNet模型用来做人脸表情分类预测。对于训练MTCNN，需要使用两个数据集来进行模型训练，Wider Face数据集用来训练人脸二分类，CelebA数据集用来训练人脸边框回归（Bounding-Box Regression）以及人脸关键点回归。对于训练AlexNet，用于训练表情分类的数据集是Kaggle论坛上发布的fer2013人脸表情数据集。WIDER FACE数据集是一个人脸面部检测基准数据集，共包含32,203张图像，其中共标记了393,703张人脸，这些人脸在尺度，姿态，遮挡方面都有很大的变化范围。Wider Face数据集的制作者来自于香港中文大学，其图像主要源自于公开数据集WIDER。CelebA数据集是一个大规模人脸属性数据集，其中包含超过二十万个名人图像，每个图像都有40个属性注释，其中的人脸五点标记和人脸边框标记需要被利用到。此数据集中的图像涵盖了较大的姿势变化和杂乱背景。CelebA具有样式众多，数量众多且注释丰富的特点。CelebA数据集的制作者也来自于香港中文大学。Fer2013数据集发布于Kaggle平台，是目前相对较大的人脸表情识别公开数据集。共包含35886张人脸表情图片，测试集图像有28708张，公共验证集图像和私有验证集图像各3589张，所有图片都是由固定尺寸为48×48的灰度图像，共包含7种表情（与上文所述相符，包含了中性），并且所属类别已被标注。官方发布的数据集的格式为csv格式，所以需先对其进行格式转换(此实验中使用的python中pandas库对csv文件进行的处理)，转换成为可用的图像格式。4.2 网络搭建及训练4.2.1 实验环境 系统：Ubuntu 16.04LTS； GPU：TITAN X； 语言环境：Python2.7； 深度学习框架：Caffe； 所用依赖库包括：pycaffe、opencv、numpy、pandas、cPickle等。4.2.2 网络搭建及训练利用深度学习框架Caffe完成对所需网络的搭建和训练，搭建上述网络，搭建后caffe深度学习网络具体参数可参见附录，使用Caffe的原始是因为其对于简单网络的搭建很方便，且运算效率也很高。对于训练集和测试集，在MTCNN网络中，人脸边框回归任务对于训练集和验证集分别可以达到92.3%以及85%的回归率，对于人脸关键点定位达到91.3%和79%的准确率。然而对于人脸表情分类模块，对于验证集的准确率仅仅达到了63.5%识别率。对表情识别的识别率过低的原因进行分析，作者本人认为是因为人类表情过于复杂，对于一些微微的肌肉形变，计算机很难对其察觉，人类自己有时也很难对表情作出正确判断。在向网络输入图像时（此处不单独讨论视频任务，视频只是将多张图片组合而成的图像流），需要对输入图像进行预处理，包括图像归一化处理以保证模型收敛的更快、图像尺寸处理（最长边限制在1000像素以内）以确保计算效率、。4.3 算法测试如图8和图9所示，分别对应该网络模型分别对静态图像和视频流任务的测试结果。对于具有相对完整人脸的图像（如图8上面两个测试结果），该模型可以很好的对其进行识别，但是对于图像中包含不完整人脸、肤色较深、姿势过偏的，该模型还是不能很好的对其进行检测，这也应证了第一章所说的在不受控的条件下在人脸检测任务中的难点所在。在GPU环境下，对于视频文件（如图9为240p,30fps视频文件）的测试，可以达到15fps左右，可以看出本算法还不完善，还不能达到实时的效果，在MTCNN原作论文中，作者的人脸检测和关键点回归模型可以达到99fps。图9 对于图像的测试结果图10 对于视频的测试结果4.4 小结本章节介绍了对于本文所用人脸检测以及人脸表情识别深度神经网络模型的实现和测试结果，不难看出，对于人脸表情识别网络，虽然在训练时对于验证集测试的准确率仅仅达到63.5%，但是在实际测试时，对于较明显人脸表情测试的准确率还是不错的。本模型目前较大的问题仍在运算效率上，仍不能满足工业级实时性的要求。参考文献 Li H , Lin Z , Shen X , et al. A convolutional neural network cascade for face detection[C]// 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2015. &amp;#8617; &amp;#8617;2 Viola P A , Jones M J . Rapid Object Detection using a Boosted Cascade of Simple Features[C]// IEEE Computer Society Conference on Computer Vision &amp;amp; Pattern Recognition. IEEE, 2001. &amp;#8617; C. Zhang and Z. Zhang. A survey of recent advances in face detection. Technical Report MSR-TR-2010-66, 2010. &amp;#8617; 徐琳琳, 张树美, 赵俊莉. 基于图像的面部表情识别方法综述[J]. 计算机应用, 2017(12):171-178+208. &amp;#8617; D, C, P,等. The Expression of Emotions in Man and Animals[J]. The American Journal of Psychology, 1981.用,2017,37(12):3509-3516+3546. &amp;#8617; Ekman P , Friesen W V , Ellsworth P C . Emotion in the Human Face[M]. Cambridge University Press ;, 1982. &amp;#8617; Y. l. Tian, T. Kanade, and J. F. Cohn, “Evaluation of Gabor-Wavelet-Based Facial Action Unit Recognition in Image Sequences of Increasing Complexity,” in Proceedings of the Fifth IEEE International Conference on Automatic Face and Gesture Recognition, pp. 229-234, 2002. &amp;#8617; Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105. &amp;#8617; Lecun Y , Bengio Y , Hinton G . Deep learning[J]. nature, 2015, 521(7553):436. &amp;#8617; Y-Lan Boureau, Jean Ponce, Yann LeCun. A Theoretical Analysis of Feature Pooling in Visual Recognition[C]// International Conference on Machine Learning. DBLP, 2010. &amp;#8617; Zhang K , Zhang Z , Li Z , et al. Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks[J]. IEEE Signal Processing Letters, 2016, 23(10):1499-1503. &amp;#8617; Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object detection and semantic segmentation. In: CVPR (2014) &amp;#8617; Shelhamer E , Long J , Darrell T . Fully Convolutional Networks for Semantic Segmentation[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39(4):640-651. &amp;#8617; He K , Zhang X , Ren S , et al. Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition[J]. IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2014, 37(9):1904-16. &amp;#8617; " } ]
