<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Generate Digit Images using VAE" /><meta property="og:locale" content="en" /><meta name="description" content="Abstract: In the last decade, deep learning based generative models have gained more and more attention due to their tremendous progress. Among these models, there are two major families of them that deserve extra attention: Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). As a generative model comparable to GANs, VAEs combine the advantages of the Bayesian method and deep learning. It is built on the basis of elegant mathematics, easy to understand and performs outstandingly. Its ability to extract disentangled latent variables also enables it to have a broader meaning than the general generative model. A VAE can be defined as being an autoencoder whose training is regularised to avoid overfitting and ensure that the latent space has good properties that enable the generative process. In this post, VAE will be used to generate digit images based on the MNIST dataset." /><meta property="og:description" content="Abstract: In the last decade, deep learning based generative models have gained more and more attention due to their tremendous progress. Among these models, there are two major families of them that deserve extra attention: Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). As a generative model comparable to GANs, VAEs combine the advantages of the Bayesian method and deep learning. It is built on the basis of elegant mathematics, easy to understand and performs outstandingly. Its ability to extract disentangled latent variables also enables it to have a broader meaning than the general generative model. A VAE can be defined as being an autoencoder whose training is regularised to avoid overfitting and ensure that the latent space has good properties that enable the generative process. In this post, VAE will be used to generate digit images based on the MNIST dataset." /><link rel="canonical" href="https://zhuqishawn.github.io/posts/Generate-Digit-Images-using-VAE/" /><meta property="og:url" content="https://zhuqishawn.github.io/posts/Generate-Digit-Images-using-VAE/" /><meta property="og:site_name" content="Zhuqi Xiao" /><meta property="og:image" content="https://zhuqishawn.github.io/generated.png" /><meta property="og:image:height" content="400" /><meta property="og:image:width" content="1000" /><meta property="og:image:alt" content="VAE" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-04-01T06:00:00+02:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://zhuqishawn.github.io/generated.png" /><meta name="twitter:image:alt" content="VAE" /><meta property="twitter:title" content="Generate Digit Images using VAE" /><meta name="twitter:site" content="@twitter_username" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-04-01T06:00:00+02:00","datePublished":"2021-04-01T06:00:00+02:00","description":"Abstract: In the last decade, deep learning based generative models have gained more and more attention due to their tremendous progress. Among these models, there are two major families of them that deserve extra attention: Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). As a generative model comparable to GANs, VAEs combine the advantages of the Bayesian method and deep learning. It is built on the basis of elegant mathematics, easy to understand and performs outstandingly. Its ability to extract disentangled latent variables also enables it to have a broader meaning than the general generative model. A VAE can be defined as being an autoencoder whose training is regularised to avoid overfitting and ensure that the latent space has good properties that enable the generative process. In this post, VAE will be used to generate digit images based on the MNIST dataset.","headline":"Generate Digit Images using VAE","image":{"width":1000,"height":400,"alt":"VAE","url":"https://zhuqishawn.github.io/generated.png","@type":"imageObject"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://zhuqishawn.github.io/posts/Generate-Digit-Images-using-VAE/"},"url":"https://zhuqishawn.github.io/posts/Generate-Digit-Images-using-VAE/"}</script><title>Generate Digit Images using VAE | Zhuqi Xiao</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Zhuqi Xiao"><meta name="application-name" content="Zhuqi Xiao"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src=" https://raw.githubusercontent.com/ZhuqiShawn/ZhuqiShawn.github.io/main/assets/img/profile.jpg " alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Zhuqi Xiao</a></div><div class="site-subtitle font-italic">I write code, and have fun</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/ZhuqiShawn" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['zqxiao.shawn','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="https://www.linkedin.com/in/zhuqi-xiao-6607231b9/" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://space.bilibili.com/181492373" aria-label="bilibili" target="_blank" rel="noopener"> <i class="fas fa-video"></i> </a> <a href="" aria-label="" target="_blank" rel="noopener"> <i class=""></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Generate Digit Images using VAE</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Generate Digit Images using VAE</h1><div class="post-meta text-muted"><div> By <em> <a href="https://zhuqishawn.github.io/about/">Zhuqi Xiao</a> </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" data-ts="1617249600" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2021-04-01 </em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1628 words"> <em>9 min</em> read</span></div></div></div><div class="post-content"><blockquote class="prompt-short"><div><p><strong>Abstract</strong>: In the last decade, deep learning based generative models have gained more and more attention due to their tremendous progress. Among these models, there are two major families of them that deserve extra attention: Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). As a generative model comparable to GANs, VAEs combine the advantages of the Bayesian method and deep learning. It is built on the basis of elegant mathematics, easy to understand and performs outstandingly. Its ability to extract disentangled latent variables also enables it to have a broader meaning than the general generative model. A VAE can be defined as being an autoencoder whose training is regularised to avoid overfitting and ensure that the latent space has good properties that enable the generative process. In this post, VAE will be used to generate digit images based on the MNIST dataset.</p></div></blockquote><h2 id="method"><span class="mr-2">Method</span><a href="#method" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>In short, as one of state-of-art generative models, a VAE is an autoencoder whose encodings distribution is regularised during the training process to ensure that its latent space has good performance allowing it to generate new data that is not included in the training dataset. It is worth noting that the term “variational” comes from the close relationship between regularization in statistics and variational inference methods. Just like a standard autoencoder, a variational autoencoder is composed of both an encoder and a decoder, as shown in Figure 1<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, and that is trained to minimize the reconstruction error between the decoded output and the initial data. However, in order to introduce some regularisation of the latent space, the encoding-decoding process is slightly modified by encoding the input as a distribution on the latent space instead of encoding it as a single point. In practice, the encoded distributions are chosen to be normal so that the encoder can be trained to return the mean and the variance matrix that describes these Gaussians. The reason for encoding the input as a distribution with a certain variance instead of a single point is that it can express latent spatial regularization very naturally: the distribution returned by the encoder is forced to be close to the standard normal distribution. <img data-src="https://raw.githubusercontent.com/ZhuqiShawn/ZhuqiShawn.github.io/main/assets/img/vae/VAE-Model.png" alt="vae" class="shadow" style="max-width: 60%" data-proofer-ignore> <em><strong>Figure 1</strong>. Overview of VAE architecture</em></p><p>Therefore, we can intuitively infer that the loss function of a VAE model consists of two parts: a “reconstruction term” that tends to make the encoding-decoding scheme as good as possible and a “regularisation term” that tends to regularise the organisation of the latent space by making the distributions returned by the encoder close to a standard normal distribution, which can be written as</p>\[L = ||x-\hat{x}||^2+\mathbb{KL}(\mathbf{N}(\mu_x, \sigma_x),\mathbf{N}(0, 1))\]<p>This function is also called <em>evidence lower bound</em> (ELBO) loss, which will be derived in Theoretical Part. As you can see, the regularisation term is expressed as the <em>Kulback-Leibler divergence</em> (KL) between the returned distribution and normal distribution. We can notice that the KL divergence between two Gaussian distributions has a closed form that can be directly expressed in terms of the means and the covariance matrices of the the two distributions. In this report, the mean-squared error (MSE) is applied to calculate the reconstruction loss, which is defined as</p>\[\text{MSE} = \frac{1}{n}\sum_{i=1}^n||x-\hat{x}||^2\]<p>where $n$ is the number of training data, and</p>\[\text{KL Loss} = \sum_{i=1}^n(1+\log(\sigma_i^2)-\mu_i^2-\sigma_i^2)\]<p>is applied for calculating the KL divergence.</p><p>Next, we will discuss the encoder and the decoder. The output of an encoder is a compressed representation which is called latent variable and the decoder takes it as input and tries to recreate the original input data. In practice, these two parts can be implemented using two neural networks. In this report, two convolutional neural networks (CNN) are used with details shown in Table 1 and 2. It is worth noting that the reason why the dimension of the fully connected layer of encoder used here is 40 is that half of it is used as mean, and the other half is used as variance so that the length of latent space is 20. Moreover, transposed convolution layers are applied in decoder to up-sample the latent variables.</p><div class="table-wrapper"><table><thead><tr><th><strong>Layer</strong><th><strong>Input Size</strong><th><strong>Filter Size</strong><th><strong>Stride</strong><th><strong>Output Size</strong><tbody><tr><td>conv1<td>$28^2\times1$<td>$3^2\times32$<td>2<td>$14^2\times32$<tr><td>conv2<td>$14^2\times32$<td>$3^2\times64$<td>2<td>$7^2\times64$<tr><td>fc<td>$7^2\times64$<td>-<td>-<td>$40$</table></div><center>Table 1. Encoder neural network</center><div class="table-wrapper"><table><thead><tr><th><strong>Layer</strong><th><strong>Input Size</strong><th><strong>Filter Size</strong><th><strong>Stride</strong><th><strong>Output Size</strong><tbody><tr><td>tconv1<td>$1^2\times20$<td>$7^2\times64$<td>7<td>$7^2\times64$<tr><td>tconv2<td>$7^2\times64$<td>$3^2\times64$<td>2<td>$14^2\times64$<tr><td>tconv3<td>$14^2\times64$<td>$3^2\times32$<td>2<td>$28^2\times32$<tr><td>tconv4<td>$28^2\times1$<td>$3^2\times1$<td>2<td>$28^2\times1$</table></div><center>Table 2. Decoder neural network</center><p>As we discussed before, before minimizing the loss, we need to generate sampled data, which includes the mean and the variance vectors to create the final encoding to be passed to the decoder network. However, we need to use back-propagation later to train our network later so that we cannot do sampling in a random manner. In this case, the trick of <em>reparameterization</em> can be adopted to substitute for random sampling. As shown in Figure 2, it is an example of our model with the length of latent variable equal to three and you can see that the the latent space</p>\[z_i=\mu_i+\exp{(\sigma_i)}\times e_i\]<p>where $e_i\sim\mathbf{N}(0,1)$. The general idea is that sampling from $\mathbf{N}(\mu_i,\sigma^2)$ is same with sampling from $\mu_i+\exp{(\sigma_i)}\times e_i$.</p><p><img data-src="https://raw.githubusercontent.com/ZhuqiShawn/ZhuqiShawn.github.io/main/assets/img/vae/VAE_train.png" alt="vae-train" class="shadow" style="max-width: 60%" data-proofer-ignore> <em><strong>Figure 2</strong>. VAE with reparameterization</em></p><h2 id="experiment-and-evaluation"><span class="mr-2">Experiment and Evaluation</span><a href="#experiment-and-evaluation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>Now that we have defined everything we need, it is time to get it trained. The training parameters are shown in Table 3 and momentum is used in this report. The training progress plot is shown in Figure 3, the loss stays at around $18.5\%$ after 50 epochs of training.</p><div class="table-wrapper"><table><thead><tr><th><strong>Epochs</strong><th><strong>Learning Rate</strong><th><strong>Gradient Decay Factor</strong><tbody><tr><td>50<td>0.001<td>0.9</table></div><center>Table 3. VAE with reparameterization</center><p><img data-src="https://raw.githubusercontent.com/ZhuqiShawn/ZhuqiShawn.github.io/main/assets/img/vae/Training.png" alt="training" class="shadow" style="max-width: 60%" data-proofer-ignore> <em><strong>Figure 3</strong>.Training progress plot of VAE</em>&lt;/center&gt;_</p><p>Now, let’s evaluate our trained VAE. First of all, randomly select ten images with labels of 0-9 from the test set and pass them through the encoder, and then use the decoder to reconstruct the images using the obtained representation. Shown in Figure 4, in most cases, after the encoding-decoding process, the reconstruction result is quite good, but for some input images whose writing is not in standard style, errors will occur. In addition, by observing these reconstructed images, we can find that the images generated by VAE are usually blurry.</p><p><img data-src="https://raw.githubusercontent.com/ZhuqiShawn/ZhuqiShawn.github.io/main/assets/img/vae/reconstruction.png" alt="reconstruction" class="shadow" style="max-width: 50%" data-proofer-ignore> <em><strong>Figure 4</strong>. Reconstruction images of each digit</em></p><p>Now, let’s generate some new images. Randomly generate a number of encodings from normal distribution with the length same with the latent space and pass them through the decoder to reconstruct the images, the results are shown in Figure 5. In addition to the phenomenon mentioned before that the generated image will get blurred, it is not difficult to see that some generated images are meaningless, such as the image in the fourth row and fifth column, since it is hard to determine whether it’s a 4 or 9.</p><p><img data-src="https://raw.githubusercontent.com/ZhuqiShawn/ZhuqiShawn.github.io/main/assets/img/vae/generated.png" alt="generated" class="shadow" style="max-width: 50%" data-proofer-ignore> <em><strong>Figure 5</strong>. Randomly generated samples of digits</em></p><p>In fact, compared with some models I tried before, the test result of the current model is one of the most satisfactory ones. Previously, I tried to add pooling layers and use shorter hidden variables, but the results were very bad. Some examples are shown in Figure 6. Apparently, using pooling layer to down-sample the image is not a good choice here for encoder and the length of latent variable should not be too short.</p><p><img data-src="https://raw.githubusercontent.com/ZhuqiShawn/ZhuqiShawn.github.io/main/assets/img/vae/failure.png" alt="failure" class="shadow" style="max-width: 50%" data-proofer-ignore> <em>Figure 6. Image generated by the model in case of failure</em></p><p>In the experiment, the classifier network architecture is shown in Table 4 below and the softmax function is applied on the final output of the network to obtain the classification result. After training on the entire training set of MNIST, the accuracy for testing on the test set can reach 99.21%.</p><div class="table-wrapper"><table><thead><tr><th><strong>Layer</strong><th><strong>Input Size</strong><th><strong>Filter Size</strong><th><strong>Stride</strong><th><strong>Output Size</strong><tbody><tr><td>conv1<td>$28^2\times1$<td>$3^2\times8$<td>1<td>$28^2\times8$<tr><td>BN<td>$28^2\times8$<td>-<td>-<td>$28^2\times8$<tr><td>mp1<td>$28^2\times8$<td>$2^2$<td>2<td>$14^2\times8$<tr><td>conv2<td>$14^2\times8$<td>$3^2\times16$<td>1<td>$14^2\times16$<tr><td>BN<td>$14^2\times16$<td>-<td>-<td>$14^2\times16$<tr><td>mp2<td>$14^2\times16$<td>$2^2$<td>2<td>$7^2\times16$<tr><td>conv3<td>$7^2\times16$<td>$3^2\times32$<td>1<td>$7^2\times32$<tr><td>BN<td>$7^2\times32$<td>-<td>-<td>$7^2\times32$<tr><td>fc<td>$7^2\times32$<td>-<td>-<td>$10$</table></div><center>Table 4. Convolutional neural network (CNN) architecture of classiﬁer. BN is an abbreviation for batch normalization layer and there is always a ReLU activation layer follows it, and mp is for max pooling.</center><p>Following the instruction, using half of the MNIST training set to train the VAE model defined in last section and use it to generate new images. The process of generating new images is as follows: re-input the training set used for training VAE to the encoder and get its encoding, add some Gaussian Noise to a portion of the encoding and then pass it through the decoder to regenerate new images. The generated image can refer to Figure 5, whose result is generally consistent. Use a certain proportion of unused training data in MNIST together with the same amount of newly generated data to train the classifier defined in Table 4, and use the test set in MNIST to test the trained classifier. The accuracy of the tests is shown in Table 5. As you can see, when the amount of training data increases, the accuracy of the test also increases. This is not difficult to explain, but what is interesting is that these test results are not as good as the classifier trained with the entire MNIST training set. I think the reason may be because the MNIST dataset is so easy that adding some interference does not improve the generalization ability of the classifier. However, I think for some more difficult datasets, doing this will reduce the overfitting and improve the classification ability.</p><div class="table-wrapper"><table><thead><tr><th><strong>Percentage (%)</strong><th><strong>20</strong><th><strong>50</strong><th><strong>100</strong><tbody><tr><td>Accuracy (%)<td>97.50<td>98.34<td>98.70</table></div><center>Table 5. The accuracy of the classiﬁer trained with different proportions of the images in the other half of the training set on the MNIST test set together with a same amount of newly generated images.</center><h2 id="reference"><span class="mr-2">Reference</span><a href="#reference" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><div class="footnotes" role="doc-endnotes"><ol><li id="fn:1" role="doc-endnote"><p>Joseph Rocca: Understanding Variational Autoen-coders (VAEs). <a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73">Link</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p></ol></div></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/deep-learning/'>Deep Learning</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/gnn/" class="post-tag no-text-decoration" >gnn</a> <a href="/tags/vae/" class="post-tag no-text-decoration" >vae</a> <a href="/tags/cnn/" class="post-tag no-text-decoration" >cnn</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://www.facebook.com/sharer/sharer.php?title=Generate Digit Images using VAE - Zhuqi Xiao&amp;u=https://zhuqishawn.github.io/posts/Generate-Digit-Images-using-VAE/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://zhuqishawn.github.io/posts/Generate-Digit-Images-using-VAE/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <a href="http://service.weibo.com/share/share.php?title=Generate Digit Images using VAE - Zhuqi Xiao&amp;url=https://zhuqishawn.github.io/posts/Generate-Digit-Images-using-VAE/" data-toggle="tooltip" data-placement="top" title="Weibo" target="_blank" rel="noopener" aria-label="Weibo"> <i class="fa-fw fab fa-weibo"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/cnn/">cnn</a> <a class="post-tag" href="/tags/deep-learning/">deep learning</a> <a class="post-tag" href="/tags/algorithm/">algorithm</a> <a class="post-tag" href="/tags/big-data/">big data</a> <a class="post-tag" href="/tags/emotion-detection/">emotion detection</a> <a class="post-tag" href="/tags/face-detection/">face detection</a> <a class="post-tag" href="/tags/git/">git</a> <a class="post-tag" href="/tags/github/">github</a> <a class="post-tag" href="/tags/gnn/">gnn</a> <a class="post-tag" href="/tags/object-detection/">object detection</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/facial-expression-detection-using-deep-learning/"><div class="card-body"> <em class="timeago small" data-ts="1593489600" > 2020-06-30 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>基于深度学习的游客人脸表情的识别</h3><div class="text-muted small"><p> 摘要：面对面的进行沟通交流，在我们日常人与人之间的交往中扮演着很重要的角色，从交谈者的面部我们可以获取许多我们在文字交流中难以获取的信息，例如一个人的心理状态、情绪、意向等。的确，一个人的面部蕴含着许多潜在有价值的信息，对其加以有效地收集和分析，就可以创造出一些有利用价值的数据。近些年，对于面部表情识别的研究，被广泛地应用于教育、心理、医学以及商业等多个领域，人脸表情识别也是当前计...</p></div></div></a></div><div class="card"> <a href="/posts/Animal-Detection-Networks/"><div class="card-body"> <em class="timeago small" data-ts="1633147200" > 2021-10-02 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Animal Detection Networks</h3><div class="text-muted small"><p> Abstract: The project aims to provide a demo animal detection model with the long-term goal of developing a phone application. Two different state-of-art algorithms are tested out, one is YOLOv5...</p></div></div></a></div><div class="card"> <a href="/posts/Count-Min-Sketch-%E7%AE%97%E6%B3%95/"><div class="card-body"> <em class="timeago small" data-ts="1621396800" > 2021-05-19 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Count-min Sketch 算法</h3><div class="text-muted small"><p> 简介 Count-min Sketch算法是一个可以用来计数的算法，在数据大小非常大时，一种高效的计数算法，通过牺牲准确性提高的效率。 是一个概率数据机制 算法效率高 提供计数上限 其中，重要参数包括 Hash 哈希函数数量： k 计数表格列的数量： m 内存中用空间： $k \times m \times \text{size of counter}$ ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Note-for-learning-Git/" class="btn btn-outline-primary" prompt="Older"><p>Note for Learning Git</p></a> <a href="/posts/Count-Min-Sketch-%E7%AE%97%E6%B3%95/" class="btn btn-outline-primary" prompt="Newer"><p>Count-min Sketch 算法</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://zhuqishawn.github.io/about/">Zhuqi Xiao</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/cnn/">cnn</a> <a class="post-tag" href="/tags/deep-learning/">deep learning</a> <a class="post-tag" href="/tags/algorithm/">algorithm</a> <a class="post-tag" href="/tags/big-data/">big data</a> <a class="post-tag" href="/tags/emotion-detection/">emotion detection</a> <a class="post-tag" href="/tags/face-detection/">face detection</a> <a class="post-tag" href="/tags/git/">git</a> <a class="post-tag" href="/tags/github/">github</a> <a class="post-tag" href="/tags/gnn/">gnn</a> <a class="post-tag" href="/tags/object-detection/">object detection</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { function updateMermaid(event) { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { const mode = event.data.message; if (typeof mermaid === "undefined") { return; } let expectedTheme = (mode === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* Re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } let initTheme = "default"; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); window.addEventListener("message", updateMermaid); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
